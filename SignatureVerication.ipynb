{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72e36c51",
   "metadata": {},
   "source": [
    "<h1>Project Title:</h1>\n",
    "<p>Signature Verification using Machine Learning. This project demonstrates the application of statistical analysis and predictive modeling using various machine learning algorithms.</p>\n",
    "\n",
    "<h2>Objective:</h2>\n",
    "<p>Develop a machine learning model that can verify a person's signature.</p>\n",
    "\n",
    "<h2>Data Collection:</h2>\n",
    "<p>You have been provided with a dataset consisting of 80 signature images from each participant. These images will serve as the basis for training and testing your machine learning models.</p>\n",
    "\n",
    "<h2>Data Preprocessing:</h2>\n",
    "<ol>\n",
    "  <li>Ensure all images are in a JPG format.</li>\n",
    "  <li>Resize images to a standard size of 100x100 pixels.</li>\n",
    "  <li>Crop the images to include only the signature, excluding any black frame around the signature.</li>\n",
    "  <li>Convert images to grayscale to simplify the data.</li>\n",
    "  <li>(Optional) Apply image processing techniques, such as edge detection or thresholding, to highlight the signature.</li>\n",
    "</ol>\n",
    "\n",
    "<h2>Feature Engineering:</h2>\n",
    "<ol>\n",
    "  <li>Focus on pixel-based features that can be used to detect the signature.</li>\n",
    "  <li>Consider using dimensionality reduction techniques, such as PCA, to reduce the number of features while retaining the essential information.</li>\n",
    "  <li>Standardize the feature values to ensure consistency.</li>\n",
    "</ol>\n",
    "\n",
    "<h2>Model Development:</h2>\n",
    "<ol>\n",
    "  <li>Split the dataset into training (80%) and testing (20%) sets.</li>\n",
    "  <li>Apply data augmentation techniques to produce more images and increase the dataset size.</li>\n",
    "  <li>You are allowed to use any machine learning models that you have learned about in class or any other algorithm that you wish to try. This includes, but is not limited to, classic machine learning algorithms such as logistic regression, decision trees, and support vector machines, as well as any other models you find suitable for signature verification.</li>\n",
    "  <li>Train the model on the training set and evaluate its performance on the testing set.</li>\n",
    "</ol>\n",
    "\n",
    "<h2>Model Evaluation:</h2>\n",
    "<ol>\n",
    "  <li>Evaluate the model's performance using metrics such as accuracy, precision, recall, and F1 score.</li>\n",
    "  <li>Analyze the results and identify areas for improvement.</li>\n",
    "</ol>\n",
    "\n",
    "<h2>Final Model Selection and Deployment:</h2>\n",
    "<ol>\n",
    "  <li>After evaluating all the models, choose the best model based on your criteria.</li>\n",
    "  <li>Train this final model on the entire dataset.</li>\n",
    "  <li>Save the trained model using the pickle package in Python.</li>\n",
    "</ol>\n",
    "\n",
    "<h2>Deliverables:</h2>\n",
    "<ol>\n",
    "  <li>A well-documented codebase containing the preprocessing, model development, evaluation, deployment, and testing steps.</li>\n",
    "  <li>A report detailing the methodology, results, and conclusion of your project.</li>\n",
    "  <li>A presentation summarizing the project and its findings.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8ec38e",
   "metadata": {},
   "source": [
    "## Project Report: Signature Classification\n",
    "\n",
    "<p><strong>Objective:</strong><br>\n",
    "The primary goal of this project is to develop a signature classification system capable of categorizing signatures into 9 distinct classes, labeled Class 1 through Class 9. Each class corresponds to a specific type of signature, with the unique signatures for each class stored in separate PDF files. In total, there are 80 different signature variants per class.</p>\n",
    "\n",
    "<p><strong>Data Extraction and Preparation:</strong><br>\n",
    "The initial task involves extracting signature images from each PDF file. These images are then sorted into nine separate folders, named Folder 1 to Folder 9, corresponding to their respective classes (Class 1 to Class 9). An important part of the data preparation process is resizing each image to a uniform dimension of 100x100 pixels to ensure consistency in input data for the model training.</p>\n",
    "\n",
    "<p><strong>Training Process:</strong><br>\n",
    "The training of the classification model will involve reading and processing the images from each designated folder. This approach ensures that the model learns the distinct features and patterns of each signature class.</p>\n",
    "\n",
    "<p><strong>Expected Outcome:</strong><br>\n",
    "Upon completion of the training, the system will be tested with unseen signature data. The effectiveness of the system will be evaluated based on its ability to accurately classify these unseen signatures into one of the nine predefined classes.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0eff83",
   "metadata": {},
   "source": [
    "## Data Preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2214dddd",
   "metadata": {},
   "source": [
    "<p>In our signature verification project, we have employed a thorough and methodical approach to data preprocessing and augmentation, essential for the model's accuracy and reliability. Here's an elaboration of our process, suitable for inclusion in a project report:</p>\n",
    "\n",
    "<h3>Initial Dataset and Automated Cropping:</h3>\n",
    "<ul>\n",
    "  <li>We were initially provided with approximately 640 signature images. We developed an automated process to extract and crop signature images directly from PDF files. This automation was crucial in efficiently processing a large volume of documents, ensuring that only the signature area was included in each image.</li>\n",
    "  <li>We increased the dataset to 6,700 signature images by applying augmentation techniques and later expanded it to a  17,000 images to create a more robust and diverse training set.</li>\n",
    "</ul>\n",
    "\n",
    "<h3>Standardization and Preprocessing of Images:</h3>\n",
    "<ul>\n",
    "  <li>All images were converted to the JPG format for consistency.</li>\n",
    "  <li>We resized each image to a uniform size of 100x100 pixels, which is essential for standardized input to the model.</li>\n",
    "  <li>The images were then converted to grayscale, focusing the model's learning on structural features and reducing computational complexity.</li>\n",
    "  <li>To highlight the signatures, we enhanced the contrast between the signature using the CONTOUR method.</li>\n",
    "</ul>\n",
    "\n",
    "<h3>Data Augmentation for an Expanded Dataset:</h3>\n",
    "<ul>\n",
    "  <li>To augment our dataset and introduce more variability, we applied several image manipulation techniques:</li>\n",
    "  <li>Rotating the images by various degrees: 10, 15, 20, 25, 30, 90, 270. This rotation introduces various orientations, simulating different signing positions and styles.</li>\n",
    "  <li>Adjusting the brightness of the images, which helps the model learn from signatures under different lighting conditions.</li>\n",
    "  <li>Flipping the images horizontally and vertically, further diversifying the dataset by mirroring the signatures.</li>\n",
    "  <li>Applying a zoom-in feature on signatures to create close-up views, enabling the model to recognize finer details.</li>\n",
    "</ul>\n",
    "\n",
    "<p>These steps not only enhanced the quality and uniformity of our dataset but also significantly increased its size and diversity. By meticulously preparing and augmenting the data, we established a robust foundation for training our signature verification model, thereby improving its ability to accurately authenticate signatures in a wide range of real-world conditions.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09e9fee",
   "metadata": {},
   "source": [
    "#### Automation: Cropping from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859470e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from PIL import Image, ImageOps, ImageFilter\n",
    "import os\n",
    "\n",
    "# PDF file path. Below is the example for 1 directory. Need to change for rest of the directory.\n",
    "pdf_file = 'Images/1/1.pdf' ## YOUR_PDF_FILE_LOCATION\n",
    "\n",
    "dpi = 300  # Increased the dpi \n",
    "\n",
    "\n",
    "square_coordinates_cm = [\n",
    "    (2.5, 3, 8, 7),\n",
    "    (11.5, 3, 18, 7),\n",
    "    (2.5, 8.5, 8, 13),\n",
    "    (11.5, 8.5, 18, 13),\n",
    "    (2.5, 14.5, 8, 19.2),\n",
    "    (11.5, 14.5, 18, 19.2),\n",
    "    (2.5, 20.5, 8, 25),\n",
    "    (11.5, 20.5, 18, 25)\n",
    "]\n",
    "\n",
    "\n",
    "pdf_document = fitz.open(pdf_file)\n",
    "\n",
    "\n",
    "pdf_folder = os.path.dirname(pdf_file)\n",
    "\n",
    "\n",
    "output_folder = os.path.join(pdf_folder, \"extracted_images_clear\")\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "for page_number, page in enumerate(pdf_document):\n",
    "    \n",
    "    for i, (x1_cm, y1_cm, x2_cm, y2_cm) in enumerate(square_coordinates_cm):\n",
    "        \n",
    "        x1_pixels = int(x1_cm * dpi / 10.58)\n",
    "        y1_pixels = int(y1_cm * dpi / 10.58)\n",
    "        x2_pixels = int(x2_cm * dpi / 10.58)\n",
    "        y2_pixels = int(y2_cm * dpi / 10.58)\n",
    "\n",
    "        \n",
    "        img = page.get_pixmap(matrix=fitz.Matrix(1, 1), clip=(x1_pixels, y1_pixels, x2_pixels, y2_pixels))\n",
    "\n",
    "        \n",
    "        pil_image = Image.frombytes(\"RGB\", [img.width, img.height], img.samples)\n",
    "\n",
    "        \n",
    "        pil_image = pil_image.resize((100, 100))\n",
    "\n",
    "        # This converts the image to grayscale\n",
    "        pil_image = ImageOps.grayscale(pil_image)\n",
    "\n",
    "        # Image processing techniques thresholding to highlight the signature\n",
    "        pil_image = pil_image.filter(ImageFilter.CONTOUR)\n",
    "\n",
    "        \n",
    "        image_filename = os.path.join(output_folder, f\"page_{page_number + 1}_square_{i}.jpg\")\n",
    "        pil_image.save(image_filename, \"JPEG\")\n",
    "\n",
    "\n",
    "pdf_document.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982c59f2",
   "metadata": {},
   "source": [
    "### Data augmentation techniques to produce more images and increase the dataset size\n",
    "\n",
    "1. The augmentations include rotating the images by 10, 15, 20, 25, 30, 90, 180, and 270 degrees\n",
    "\n",
    "2. Adjusting the brightness\n",
    "3. Flipping the images horizontally and vertically.\n",
    "\n",
    "4. Zoom In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b174501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "image_directory = './Images/1/extracted_images_clear/'\n",
    "output_directory = './Train/1/'\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "rotation_degrees = [10, 15, 20, 25, 30]\n",
    "\n",
    "image_files = [f for f in os.listdir(image_directory) if f.lower().endswith(('.jpg', '.jpeg'))]\n",
    "print(f\"Processing {len(image_files)} files from {image_directory}\")\n",
    "\n",
    "for image_file in image_files:\n",
    "    with Image.open(os.path.join(image_directory, image_file)) as img:\n",
    "        file_name, file_ext = os.path.splitext(image_file)\n",
    "\n",
    "        for degrees in rotation_degrees:\n",
    "            rotated = img.rotate(degrees, expand=True, fillcolor='white')\n",
    "            h_flipped = rotated.transpose(Image.Transpose.FLIP_LEFT_RIGHT)\n",
    "            v_flipped = rotated.transpose(Image.Transpose.FLIP_TOP_BOTTOM)\n",
    "\n",
    "            rotated.save(os.path.join(output_directory, f'rotated_{degrees}_{file_name}_new.jpg'), 'JPEG')\n",
    "            h_flipped.save(os.path.join(output_directory, f'rotated_{degrees}_hflip_{file_name}_new.jpg'), 'JPEG')\n",
    "            v_flipped.save(os.path.join(output_directory, f'rotated_{degrees}_vflip_{file_name}_new.jpg'), 'JPEG')\n",
    "\n",
    "            print(f\"Saved augmented images for {image_file}\")\n",
    "\n",
    "print(\"Image augmentation completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792e5e80",
   "metadata": {},
   "source": [
    "#### Augmentation- Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "43c8093c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-190-49a1a1946731>:22: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  img = img.resize(target_size, Image.ANTIALIAS)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resized 11 images to 100x100 pixels.\n",
      "Image augmentation completed. Copying files to Train/1 directory.\n",
      "Files copied to Train/1 directory.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "image_directory = 'Images/1/extracted_images_clear/'\n",
    "output_directory = 'Images/1/Augmented'\n",
    "train_directory = 'Train/9'\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "os.makedirs(train_directory, exist_ok=True)\n",
    "\n",
    "rotation_degrees = [10, 15, 20, 25, 30,90,180,270]\n",
    "target_size = (100, 100)\n",
    "\n",
    "# Check and resize images if they are not 100x100\n",
    "resized_count = 0\n",
    "image_files = os.listdir(image_directory)\n",
    "\n",
    "for image_file in image_files:\n",
    "    if image_file.lower().endswith(('.jpg', '.jpeg')):\n",
    "        with Image.open(os.path.join(image_directory, image_file)) as img:\n",
    "            if img.size != target_size:\n",
    "                img = img.resize(target_size, Image.ANTIALIAS)\n",
    "                img.save(os.path.join(image_directory, image_file), 'JPEG')\n",
    "                resized_count += 1\n",
    "\n",
    "print(f\"Resized {resized_count} images to 100x100 pixels.\")\n",
    "\n",
    "# Process the images\n",
    "for image_file in image_files:\n",
    "    if image_file.lower().endswith(('.jpg', '.jpeg')):\n",
    "        with Image.open(os.path.join(image_directory, image_file)) as img:\n",
    "            file_name, file_ext = os.path.splitext(image_file)\n",
    "            file_name = file_name.lower()  # Convert filename to lowercase for consistency\n",
    "\n",
    "            for degrees in rotation_degrees:\n",
    "                rotated = img.rotate(degrees, expand=True, fillcolor='white')\n",
    "                rotated.save(os.path.join(output_directory, f'rotated_{degrees}_{file_name}_new.jpg'), 'JPEG')\n",
    "\n",
    "                h_flipped = rotated.transpose(Image.Transpose.FLIP_LEFT_RIGHT)\n",
    "                h_flipped.save(os.path.join(output_directory, f'rotated_{degrees}_hflip_{file_name}_new.jpg'), 'JPEG')\n",
    "\n",
    "                v_flipped = rotated.transpose(Image.Transpose.FLIP_TOP_BOTTOM)\n",
    "                v_flipped.save(os.path.join(output_directory, f'rotated_{degrees}_vflip_{file_name}_new.jpg'), 'JPEG')\n",
    "\n",
    "print(\"Image augmentation completed. Copying files to Train/1 directory.\")\n",
    "\n",
    "# Copy files from output_directory to train_directory\n",
    "for file in os.listdir(output_directory):\n",
    "    shutil.copy(os.path.join(output_directory, file), os.path.join(train_directory, file))\n",
    "\n",
    "print(\"Files copied to Train/1 directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413e9eae",
   "metadata": {},
   "source": [
    "#### Augmentation- Zoom in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90b9bb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zoom-in augmentation and resizing to 100x100 pixels completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-1e557b2a7cad>:26: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  zoomed_img = cropped_img.resize((width, height), Image.ANTIALIAS)\n",
      "<ipython-input-2-1e557b2a7cad>:27: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  final_img = zoomed_img.resize(target_size, Image.ANTIALIAS)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "image_directory = './Test'\n",
    "output_directory = './out'\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "zoom_factor = 4 #  zoomed in by 40%.\n",
    "target_size = (100, 100)\n",
    "\n",
    "# Updated to include uppercase extensions\n",
    "image_files = [f for f in os.listdir(image_directory) if f.lower().endswith(('.jpg', '.jpeg'))]\n",
    "# print(f\"Found {len(image_files)} image files.\")\n",
    "\n",
    "for image_file in image_files:\n",
    "    with Image.open(os.path.join(image_directory, image_file)) as img:\n",
    "        width, height = img.size\n",
    "        new_width = width / zoom_factor\n",
    "        new_height = height / zoom_factor\n",
    "        left = (width - new_width) / 2\n",
    "        top = (height - new_height) / 2\n",
    "        right = (width + new_width) / 2\n",
    "        bottom = (height + new_height) / 2\n",
    "\n",
    "        cropped_img = img.crop((left, top, right, bottom))\n",
    "        zoomed_img = cropped_img.resize((width, height), Image.ANTIALIAS)\n",
    "        final_img = zoomed_img.resize(target_size, Image.ANTIALIAS)\n",
    "\n",
    "        output_path = os.path.join(output_directory, f'zoomed_100x100_{image_file}')\n",
    "        final_img.save(output_path, 'JPEG')\n",
    "#         print(f\"Saved: {output_path}\")\n",
    "\n",
    "print(\"Zoom-in augmentation and resizing to 100x100 pixels completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c60ea8",
   "metadata": {},
   "source": [
    "## Feature Engineering:\n",
    "<p>We implemented a systematic approach to dimensionality reduction and feature scaling as part of the preprocessing steps for a signature verification model.</p>\n",
    "<ol>\n",
    "    <li><strong>PCA for Dimensionality Reduction:</strong> To address the challenge of having 10,000 features for a 100x100-pixel image, we must consider dimensionality reduction techniques. We started by applying Principal Component Analysis (PCA) to reduce the high-dimensional data into a more manageable form while retaining most of the variance in the data. We experimented with retaining different levels of variance 95%, 85%, 80%, and 70% and determined that retaining 80% of the variance with PCA offered a good balance between reducing dimensionality and preserving information necessary for accurate model performance.</li>\n",
    "    <li><strong>Truncated SVD:</strong> After comparing PCA with Truncated Singular Value Decomposition (SVD), We opted for Truncated SVD to reduce the feature space to 400 features. Truncated SVD is similar to PCA but is more suitable for sparse datasets, often found in text data or image pixels. Unlike PCA, it does not center the data before computing the singular value decomposition, which makes it more efficient in terms of computation, especially with large datasets. We decided to use Truncated SVD because of its faster processing speed, which would be beneficial in the operational phase of the project.</li>\n",
    "    <li><strong>Feature Scaling with StandardScaler:</strong> We then used StandardScaler to standardize the features of our dataset. StandardScaler transforms each feature to have a mean of zero and a standard deviation of one. By standardizing the features, We ensure that each feature contributes equally to the distance computations, which can improve the performance of these algorithms.</li>\n",
    "    <li><strong>Saving Transformation Objects:</strong> Finally, we saved the trained Truncated SVD and StandardScaler objects as 'tsvd.pkl' and 'scaler.pkl', respectively. This step is crucial because We need to apply the same transformations to new data during the inference phase to maintain consistency. By saving the trained objects, We ensure that new data is scaled and reduced in dimensionality in the exact same way as the training data, which is necessary for the model to make accurate predictions on new signatures.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b53592",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8f1ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Function to load and process images from a folder\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        with Image.open(img_path) as img:\n",
    "            img = img.convert('L')  # Convert to grayscale\n",
    "            img = img.resize((100, 100))  # Resize to 100x100\n",
    "            images.append(np.array(img).flatten())  # Flatten the image\n",
    "    return images\n",
    "\n",
    "# Specify the data directory\n",
    "data_dir = './Train/'\n",
    "\n",
    "# Load images and labels\n",
    "class_images = {}  # Dictionary to store images for each class\n",
    "for i in range(1, 10):  # Classes 1 to 9\n",
    "    folder_name = os.path.join(data_dir, str(i))\n",
    "    class_images[i] = load_images_from_folder(folder_name)\n",
    "\n",
    "# Combine images into a single array and create labels\n",
    "all_images = []\n",
    "labels = []\n",
    "for class_id, images in class_images.items():\n",
    "    all_images.extend(images)\n",
    "    labels.extend([class_id] * len(images))\n",
    "\n",
    "X = np.array(all_images)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Save the StandardScaler object\n",
    "with open('scaler.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler, file)\n",
    "\n",
    "# Apply PCA to reduce dimensionality\n",
    "pca = PCA(n_components=0.80) \n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Save the PCA object\n",
    "with open('pca80.pkl', 'wb') as file:\n",
    "    pickle.dump(pca, file)\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print the total time taken\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total time taken: {total_time} seconds\")\n",
    "\n",
    "# X_pca and y are ready for training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7690d2",
   "metadata": {},
   "source": [
    "###  Truncated SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "236217d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 107.30074739456177 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from sklearn.decomposition import TruncatedSVD  # Import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Function to load and process images from a folder\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        with Image.open(img_path) as img:\n",
    "            img = img.convert('L')  # Convert to grayscale\n",
    "            img = img.resize((100, 100))  # Resize to 100x100\n",
    "            images.append(np.array(img).flatten())  # Flatten the image\n",
    "    return images\n",
    "\n",
    "# Specify the data directory\n",
    "data_dir = './Train/'\n",
    "\n",
    "# Load images and labels\n",
    "class_images = {}  # Dictionary to store images for each class\n",
    "for i in range(1, 10):  # Classes 1 to 9\n",
    "    folder_name = os.path.join(data_dir, str(i))\n",
    "    class_images[i] = load_images_from_folder(folder_name)\n",
    "\n",
    "# Combine images into a single array and create labels\n",
    "all_images = []\n",
    "labels = []\n",
    "for class_id, images in class_images.items():\n",
    "    all_images.extend(images)\n",
    "    labels.extend([class_id] * len(images))\n",
    "\n",
    "X = np.array(all_images)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Save the StandardScaler object\n",
    "with open('scaler_truncate.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler, file)\n",
    "\n",
    "# Apply Truncated SVD to reduce dimensionality\n",
    "n_components = 400  # Adjust the number of components based on your requirements\n",
    "tsvd = TruncatedSVD(n_components=n_components)  # Create TruncatedSVD object\n",
    "X_pca = tsvd.fit_transform(X_scaled)  # Fit and transform with TruncatedSVD\n",
    "\n",
    "# Save the TruncatedSVD object\n",
    "with open('tsvd.pkl', 'wb') as file:\n",
    "    pickle.dump(tsvd, file)\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print the total time taken\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total time taken: {total_time} seconds\")\n",
    "\n",
    "# X_pca and y are ready for training the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa96b5b1",
   "metadata": {},
   "source": [
    "##### Check the number of features before applying 80% PCA and TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4567962c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features before PCA: 10000\n",
      "Number of features after PCA: 705\n"
     ]
    }
   ],
   "source": [
    "# X_scaled is our data after standardization and before PCA\n",
    "num_features_before_pca = X_scaled.shape[1]\n",
    "\n",
    "# X_pca is our data after applying PCA\n",
    "num_features_after_pca = X_pca.shape[1]\n",
    "\n",
    "print(\"Number of features before PCA:\", num_features_before_pca)\n",
    "print(\"Number of features after PCA:\", num_features_after_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6daffcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features before TruncatedSVD: 10000\n",
      "Number of features after TruncatedSVD: 400\n"
     ]
    }
   ],
   "source": [
    "# X_scaled is our data after standardization and before TruncatedSVD\n",
    "num_features_before_pca = X_scaled.shape[1]\n",
    "\n",
    "# X_pca is our data after applying PCA\n",
    "num_features_after_pca = X_pca.shape[1]\n",
    "\n",
    "print(\"Number of features before TruncatedSVD:\", num_features_before_pca)\n",
    "print(\"Number of features after TruncatedSVD:\", num_features_after_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef56a71",
   "metadata": {},
   "source": [
    "### Model Development:\n",
    "<p>In our signature verification project, <strong>precision</strong> is a critical metric because it indicates the reliability of the model in correctly identifying authentic signatures. <strong>High precision</strong> means that when the model predicts a signature as authentic, it is likely to be correct. This is crucial in scenarios where the consequences of false positives — incorrectly identifying a forgery as genuine — can be severe, such as in legal documents.</p>\n",
    "\n",
    "<p>However, relying solely on <strong>precision</strong> might result in a model that is overly cautious, potentially leading to many false negatives, where genuine signatures are incorrectly marked as forgeries. To address this, we also consider the F1 score, which balances <strong>precision</strong> with recall — the model's ability to identify all genuine signatures. The F1 score ensures that while we maintain <strong>high precision</strong>, we do not do so at the expense of failing to detect a substantial number of authentic signatures.</p>\n",
    "\n",
    "<p>The dataset was initially partitioned into an 80% training set and a 20% testing set, and all the model we use 'stratify=y' ensures that our training and testing datasets are representative of the overall distribution of the data. We utilized a sample size of 6,700 signatures for all the models evaluation. Upon identifying the most promising models, we expanded the dataset to 17,000 signatures, enabling us to conduct a focused validation on the best performing models. Starting with a <strong>Logistic regression model</strong> optimized with <strong>GridSearchCV</strong>. We progressed through various models, including <strong>SVM</strong> with <strong>Random</strong> and <strong>Grid search</strong> methods and <strong>a refined Grid search</strong>. We explored <strong>Decision trees</strong>, pinpointing the best hyperparameters to enhance model performance.</p>\n",
    "\n",
    "<p>\n",
    "After delving into ensemble methods, we applied boosting algorithms like <strong>XGBoost</strong> and <strong>CatBoost</strong>, alongside <strong>Bagging techniques</strong>, to harness the strengths of various learning algorithms. Among all the models evaluated, <strong>XGBoost</strong> and <strong>CatBoost</strong> stood out, with precision rates of <strong>89%</strong> and <strong>90%</strong> respectively. Ultimately, <strong>CatBoost</strong>, with its slightly superior precision, was selected as the best-performing model for our signature verification project.\n",
    "</p>\n",
    "\n",
    "\n",
    "<p>To ensure that our model was robust and not just memorizing the training data, we closely examined the learning curves for signs of overfitting. This vigilance allowed us to make informed adjustments to our models, ensuring they generalize well to new, unseen data.</p>\n",
    "\n",
    "<p>In the following sections, each of these methods is detailed comprehensively, providing insight into our strategic modeling choices and the rationale behind our focus on <strong>precision</strong> and the F1 score.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3ed698",
   "metadata": {},
   "source": [
    "### <u>Logistic Regression</u>\n",
    "\n",
    "<p>Logistic regression was initially performed with Principal Component Analysis (PCA) retaining 95% of the variance to preserve most of the data's characteristics. However, as we iterated to optimize the model, we experimented with reducing the PCA to 80% to assess the impact on performance and computational efficiency. We halted further adjustments once the total number of iterations reached its limit.</p>\n",
    "\n",
    "<p><strong>Part 1: Coarse Grid Search</strong><br>\n",
    "<strong>Objective:</strong> The aim of the coarse grid search is to explore a broad range of hyperparameters to quickly identify promising regions within the hyperparameter space. This step is not about finding the best hyperparameters but rather about narrowing down the possibilities for a more detailed search.<br>\n",
    "<strong>Process:</strong>\n",
    "<ul>\n",
    "  <li>Define a wide range of hyperparameter values (coarse grid).</li>\n",
    "  <li>Perform a grid search over this range using cross-validation.</li>\n",
    "  <li>Identify the best-performing hyperparameters within this coarse grid.</li>\n",
    "</ul>\n",
    "<strong>Output:</strong>\n",
    "<ul>\n",
    "  <li>Best parameters from this broader search.</li>\n",
    "  <li>A general idea of which hyperparameters and their ranges are promising.</li>\n",
    "</ul>\n",
    "</p>\n",
    "\n",
    "<p><strong>Part 2: Fine Grid Search</strong><br>\n",
    "<strong>Objective:</strong> Based on the results of the coarse grid search, the fine grid search aims to fine-tune the hyperparameters by exploring a more narrow and specific range around the best values found in Part 1.<br>\n",
    "<strong>Process:</strong>\n",
    "<ul>\n",
    "  <li>Adjust the hyperparameter grid based on the results of Part 1, focusing on a narrower range around the best values found.</li>\n",
    "  <li>Perform another grid search over this refined grid using cross-validation.</li>\n",
    "  <li>Identify the best-performing hyperparameters within this fine grid.</li>\n",
    "</ul>\n",
    "<strong>Output:</strong>\n",
    "<ul>\n",
    "  <li>More precisely tuned hyperparameters.</li>\n",
    "  <li>A higher level of confidence in the chosen hyperparameters' effectiveness.</li>\n",
    "</ul>\n",
    "</p>\n",
    "\n",
    "<p><strong>Part 3: Model Training and Evaluation</strong><br>\n",
    "<strong>Objective:</strong> With the best hyperparameters identified from Part 2, the final step is to train the logistic regression model using these parameters and evaluate its performance on a separate test dataset.<br>\n",
    "<strong>Process:</strong>\n",
    "<ul>\n",
    "  <li>Train the logistic regression model using the best hyperparameters identified in Part 2.</li>\n",
    "  <li>Evaluate the model's performance on the test set using metrics such as accuracy, precision, recall, and F1-score.</li>\n",
    "</ul>\n",
    "<strong>Output:</strong>\n",
    "<ul>\n",
    "  <li>A trained and tuned logistic regression model.</li>\n",
    "  <li>Performance metrics that give insights into how well the model is likely to perform on unseen data.</li>\n",
    "</ul>\n",
    "</p>\n",
    "\n",
    "<p><strong>Hyper Parameters used in this model</strong><br>\n",
    "    \n",
    "<strong>LogisticRegression(max_iter=2000):</strong> This parameter sets the maximum number of iterations for the logistic regression algorithm to converge.<br>\n",
    "    \n",
    "<strong>cv=3:</strong> This parameter determines the number of cross-validation folds. In this case, it's set to 3-fold cross-validation, meaning the dataset will be divided into 3 subsets, and the grid search will be performed three times, each time using a different subset as the validation set and the others as the training set.<br>\n",
    "    \n",
    "<strong>'C': np.logspace(-3, 3, 5):</strong> 'C' represents the regularization parameter in logistic regression. np.logspace(-3, 3, 5) generates a range of values for 'C' that spans from 0.001 to 1000, which are evenly spaced on a logarithmic scale. Regularization is a technique used to prevent overfitting in machine learning models. A smaller 'C' value imposes stronger regularization, and a larger 'C' value reduces the impact of regularization. By searching a broad range of 'C' values, you are exploring different levels of regularization to find the one that best suits your problem. The grid search will identify the optimal 'C' value that results in the best model performance during cross-validation.<br>\n",
    "    \n",
    "<strong>'multi_class': ['multinomial']:</strong> 'multi_class' specifies the type of multi-class classification to perform.<br>\n",
    "    \n",
    "<strong>'solver': ['lbfgs']:</strong> 'solver' specifies the optimization algorithm used to fit the logistic regression model. 'lbfgs' stands for Limited-memory Broyden–Fletcher–Goldfarb–Shanno, which is an optimization algorithm designed for smooth and multivariate functions. It is a popular choice for training logistic regression models for multi-class classification problems when the The LBFGS solver is efficient and well-suited for optimizing the logistic regression loss function.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fda072",
   "metadata": {},
   "source": [
    "#### Part 1: Coarse Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5915fc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "Coarse Grid Search Best Parameters: {'C': 0.03162277660168379, 'multi_class': 'multinomial', 'solver': 'lbfgs'}\n",
      "Coarse Grid Search Best Score: 0.5532552066303064\n",
      "Total time taken to run the coe: 376.6345109939575 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Split the dataset into training (80%) and testing (20%) sets.\n",
    "# stratify=y ensures that the split is done in a way that the proportion of classes in both training and testing sets is similar\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.20, random_state=42, stratify=y)\n",
    "\n",
    "# Coarse grid search\n",
    "\n",
    "# The 'C' parameter in logistic regression controls the strength of the regularization. \n",
    "# It is the inverse of regularization strength, meaning a smaller value of 'C' specifies stronger regularization (more penalty)\n",
    "# and a larger value of 'C' specifies weaker regularization (less penalty).\n",
    "\n",
    "# np.logspace(-3, 3,5): This function creates logarithmically spaced numbers between 10^start and 10^stop. \n",
    "# Here, it generates numbers between 10^-3 (0.001) and 10^3 (1000).\n",
    "# np.logspace(-3, 3, 5) will produce 5 numbers logarithmically spaced between 0.001 and 1000.\n",
    "\n",
    "# multinomial: Our dataset has 9 different classes. Logistic regression is naturally a binary classifier, \n",
    "# but setting multi_class to 'multinomial' adapts it for multi-class classification problems.\n",
    "\n",
    "# The 'lbfgs' solver in logistic regression is an optimization algorithm used to find the parameters (weights) of the model that minimize the cost function\n",
    "# We choose 'lbfgs' as our dataset is midsize and L2 regularization is commonly employed with lbfgs.\n",
    "# L2 regularization is a good for our signature recognition model, \n",
    "# considering the high number of features. \n",
    "# It will help in controlling model complexity and managing overfitting without eliminating potentially useful information spread across the features.\n",
    "\n",
    "coarse_param_grid = {\n",
    "    'C': np.logspace(-3, 3, 5),  # Broad range for C\n",
    "    'multi_class': ['multinomial'],\n",
    "    'solver': ['lbfgs'] \n",
    "}\n",
    "\n",
    "# max_iter=2000 as our feature is large 1771\n",
    "coarse_grid_search = GridSearchCV(LogisticRegression(max_iter=2000), coarse_param_grid, cv=3, scoring='accuracy', verbose=1, n_jobs=1)\n",
    "coarse_grid_search.fit(X_train, y_train)\n",
    " \n",
    "# Print the best parameters and score from the coarse grid search\n",
    "print(\"Coarse Grid Search Best Parameters:\", coarse_grid_search.best_params_)\n",
    "print(\"Coarse Grid Search Best Score:\", coarse_grid_search.best_score_)\n",
    "\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total time taken to run the coe: {total_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec48980",
   "metadata": {},
   "source": [
    "<p><strong>Output Analysis</strong></p>\n",
    "<ul>\n",
    "    <li><strong>Best Parameters:</strong>\n",
    "        <ul>\n",
    "            <li><code>'C': 0.03162277660168379, 'multi_class': 'multinomial', 'solver': 'lbfgs'</code>: This indicates that the best performance was achieved when the regularization strength 'C' was approximately 0.0316</li>\n",
    "            <li>In Part 2, the fine grid search, we will focus on a narrower range of values around 'C': 0.03162277660168379. The goal is to find a more precise 'C' value that might offer even better performance.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>Best Score:</strong>\n",
    "        <ul>\n",
    "            <li><code>0.5532552066303064</code>: This is the highest accuracy score achieved during the coarse grid search. It signifies that, on average, the model was about 55.3% accurate in predicting the correct class on the training data using the best parameters found.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46c2108",
   "metadata": {},
   "source": [
    "#### Part 2: Fine Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba99e6fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "Fine Grid Search Best Parameters: {'C': 0.01, 'multi_class': 'multinomial', 'solver': 'lbfgs'}\n",
      "Fine Grid Search Best Score: 0.5558960519565826\n",
      "Total time taken to run the coe: 340.2522437572479 seconds\n"
     ]
    }
   ],
   "source": [
    "# Fine grid search based on results of coarse grid search\n",
    "# Adjust the range/values based on the output of the coarse grid search\n",
    "\n",
    "start_time = time.time()\n",
    "fine_param_grid = {\n",
    "    # Narrower range for 'C' around the best value found in the coarse search\n",
    "    'C': np.linspace(0.01, 0.05, 5),  # Adjust this range as needed\n",
    "    'multi_class': ['multinomial'],\n",
    "    'solver': ['lbfgs']  # Keeping the solver consistent\n",
    "}\n",
    "\n",
    "fine_grid_search = GridSearchCV(LogisticRegression(max_iter=2000), fine_param_grid, cv=3, scoring='accuracy', verbose=1, n_jobs=1)\n",
    "fine_grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Fine Grid Search Best Parameters:\", fine_grid_search.best_params_)\n",
    "print(\"Fine Grid Search Best Score:\", fine_grid_search.best_score_)\n",
    "\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total time taken to run the coe: {total_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1fb5f6",
   "metadata": {},
   "source": [
    "<p><strong>Output Analysis</strong></p>\n",
    "<ul>\n",
    "    <li><strong>Best Parameters:</strong>\n",
    "        <ul>\n",
    "            <li><code>'C': 0.01, 'multi_class': 'multinomial', 'solver': 'lbfgs'</code>: The fine grid search determined that the best performance is achieved when the regularization strength 'C' is set to 0.01. This is a lower value compared to what was found in the coarse grid search, indicating stronger regularization. These parameters are now used to instantiate the logistic regression model in Part 3. This ensures that the model is configured with the most optimal settings as determined by the fine-tuning process.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dba9105",
   "metadata": {},
   "source": [
    "#### Part 3: Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5d5da0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5915492957746479\n",
      "Logistic regression Classification Report on Test data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.56      0.59      0.58       144\n",
      "           2       0.70      0.71      0.70        72\n",
      "           3       0.59      0.63      0.61       144\n",
      "           4       0.54      0.59      0.56       128\n",
      "           5       0.57      0.60      0.59       144\n",
      "           6       0.49      0.45      0.47       144\n",
      "           7       0.68      0.61      0.64        72\n",
      "           8       0.65      0.68      0.66       144\n",
      "           9       0.63      0.53      0.58       144\n",
      "\n",
      "    accuracy                           0.59      1136\n",
      "   macro avg       0.60      0.60      0.60      1136\n",
      "weighted avg       0.59      0.59      0.59      1136\n",
      "\n",
      "Total time taken to run the coe: 41.03973460197449 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Follwoing line creates a new logistic regression model instance using the best parameters from the fine grid search. \n",
    "# The **best_params syntax unpacks the parameter dictionary directly into the model's constructor.\n",
    "best_params = {'C': 0.01, 'multi_class': 'multinomial', 'solver': 'lbfgs'}\n",
    "best_model = LogisticRegression(**best_params, max_iter=2000)\n",
    "best_model.fit(X_train, y_train) # The model is trained (fitted) on the entire training dataset\n",
    "\n",
    "# Evaluate on the test set\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "# Print the test accuracy and classification report\n",
    "test_accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Logistic regression Classification Report on Test data:\")\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total time taken to run the coe: {total_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd17c729",
   "metadata": {},
   "source": [
    "<p><strong>Model Performance on Logistic Regression</strong></p>\n",
    "<ul>\n",
    "    <li><strong>Test Accuracy:</strong>\n",
    "        <ul>\n",
    "            <li>The model achieved an overall accuracy of approximately 59.15%. This means that out of all the test samples, about 59.15% of the time the model correctly predicted the class of the signature.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>Class-wise Performance:</strong>\n",
    "        <ul>\n",
    "            <li><strong>Precision (Positive Predictive Value):</strong> It indicates the proportion of positive identifications (in this case, correctly identified signatures for each class) that were actually correct. For example, This means that when the model predicts a signature to be of Class 2, it is correct 70% of the time. In the context of signature classification, precision indicates how well the model identifies actual signatures among the predicted positive cases. High precision means that when the model predicts a signature, it is likely to be correct, which is crucial for applications like fraud detection or document verification.</li>\n",
    "            <li><strong>Recall (True Positive Rate):</strong> This measures the proportion of actual positives (true signatures of each class) that were identified correctly. For example, when there are actual Class 2 signatures in the documents, the model successfully detects and classifies them correctly as Class 2 about 71% of the time.</li>\n",
    "            <li><strong>F1-Score:</strong> This is the harmonic mean of precision and recall, providing a single score that balances both the concerns of precision and recall. Higher F1-scores are generally better.</li>\n",
    "            <li><strong>Support:</strong> This indicates the number of actual occurrences of the class in the specified dataset. For example, Class 1 had 144 instances in our test set.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>Macro and Weighted Averages:</strong>\n",
    "        <ul>\n",
    "            <li><strong>Macro Average:</strong> This calculates the metric independently for each class and then takes the average (treating all classes equally, regardless of how many samples are in each class.). The macro average for precision, recall, and F1-score is around 0.60, indicating average performance across all classes without considering the frequency of each class.</li>\n",
    "            <li><strong>Weighted Average:</strong> This accounts for class imbalance by weighting the average of each class by the number of true instances. The weighted average for precision, recall, and F1-score is also around 0.59, similar to the overall accuracy, reflecting the performance across all classes considering their frequency.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>Overall Model Performance:</strong> The accuracy of 59.15% suggests moderate performance. It indicates the model's general ability to correctly classify signatures into their respective classes.</li>\n",
    "    <li><strong>Individual Class Performance:</strong> The varying scores across different classes suggest that the model performs differently for each class. For instance, it seems more effective in predicting Classes 2, 7, and 8 compared to others like Class 6.</li>\n",
    "</ul>\n",
    "<p><strong>Conclusion</strong><br>\n",
    "The logistic regression model doesn't show decent level of effectiveness in classifying signatures into 9 different classes with certain variability in performance across classes. This analysis should guide further efforts to improve classification accuracy, especially for classes where the model currently underperforms.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eece99",
   "metadata": {},
   "source": [
    "### <u>Support Vector Machine (SVM)</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082b950a",
   "metadata": {},
   "source": [
    "<p><strong>SVM</strong> is a powerful supervised machine learning algorithm used for both classification and regression tasks. It's well-suited for complex classification problems with distinct margins of separation in the data. Hyperparameter tuning is a critical aspect of building machine learning models.</li>\n",
    "</ul>\n",
    "<p>\n",
    "   In our quest to enhance our model's efficiency, we explored into various SVM methodologies, incorporating PCA at both 95% and 80% thresholds. it's noteworthy that the PCA at 95% involved processing 1771 features, whereas at 80% PCA, the feature count was reduced to 540 features. Our initial strategy involved the application of <b>Randomized Search SVM</b>, which demonstrated an improved performance with a more streamlined structure. With PCA at 95%, we achieved an F1 score of 66%, requiring a duration of 40 minutes. In contrast, reducing PCA to 80% led to a notable increase in the F1 score, reaching 79%, and significantly decreased the runtime to just 12 minutes.\n",
    "</p>\n",
    "<p>\n",
    "    Recognizing the potential from Randomized Search, we proceeded with <b>Grid Search SVM</b> for more precise hyperparameter tuning. Despite achieving a similar 79% F1 score, this approach had a higher computational cost.\n",
    "</p>\n",
    "<p>\n",
    "    To refine our approach further, we conducted what we termed \"<b>Refined Grid Search SVM</b>\", using the best hyperparameters identified in the previous Grid Search. This was executed for both PCA settings.\n",
    "</p>\n",
    "<p>\n",
    "    Comparing <b>Randomized Search SVM</b>, <b>Tuned Grid Search SVM</b>, and <b>Refined Grid Search SVM</b>, we found notable differences. The Grid Search SVM at 95% PCA achieved the highest F1 score of 67%. However, both PCA 80% Grid Search SVM and Randomized Search SVM achieved a 79% F1 score, with a precision of 81%.\n",
    "\n",
    "In summary, the <b>Refined Grid Search SVM with PCA at 80%</b> showed the best performance. It achieved an F1-Score of 79% with a high average precision of 0.81 while also being computationally efficient (271 seconds). This is a notable improvement over the PCA with 95%, which had a larger feature set (1771 features compared to 540 for PCA 80%). <b>This suggests that reducing the complexity of the feature space can lead to more efficient and equally effective models.<b>\n",
    "</p>\n",
    "<p>\n",
    "    Below is a comparison table summarizing the performances:\n",
    "</p>\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<th>PCA Setting</th>\n",
    "<th>Model Description</th>\n",
    "<th>Average Precision</th>\n",
    "<th>Average Recall</th>\n",
    "<th>Average F1-Score</th>\n",
    "<th>Time Taken to Run (seconds)</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>95%</td>\n",
    "<td>Randomized Search SVM</td>\n",
    "<td>0.70</td>\n",
    "<td>0.65</td>\n",
    "<td>0.66</td>\n",
    "<td>2208</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>95%</td>\n",
    "<td>Grid Search SVM</td>\n",
    "<td>0.68</td>\n",
    "<td>0.66</td>\n",
    "<td>0.67</td>\n",
    "<td>11402</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>95%</td>\n",
    "<td>Refined Grid Search SVM</td>\n",
    "<td>0.68</td>\n",
    "<td>0.66</td>\n",
    "<td>0.66</td>\n",
    "<td>859</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>80%</td>\n",
    "<td>Randomized Search SVM</td>\n",
    "<td>0.79</td>\n",
    "<td>0.79</td>\n",
    "<td>0.79</td>\n",
    "<td>571</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>80%</td>\n",
    "<td>Grid Search SVM</td>\n",
    "<td>0.81</td>\n",
    "<td>0.79</td>\n",
    "<td>0.79</td>\n",
    "<td>2943</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>80%</td>\n",
    "<td>Refined Grid Search SVM</td>\n",
    "<td>0.81</td>\n",
    "<td>0.79</td>\n",
    "<td>0.79</td>\n",
    "<td>271</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53711388",
   "metadata": {},
   "source": [
    "### Randomized Search SVM with PCA 95%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf38b2c7",
   "metadata": {},
   "source": [
    "<p>\n",
    "       Here, the choices of hyperparameter parameters and their ranges are carefully crafted to ensure a thorough yet efficient exploration of\n",
    "    the model's hyperparameter space, balancing the breadth of exploration with computational practicality.\n",
    "    \n",
    "<strong>C Parameter (Regularization Strength):</strong> \n",
    "    The 'C' parameter in SVM determines the strength of regularization. It is set to vary logarithmically, \n",
    "    using np.logspace, from \\(10^{-4}\\) to \\(10^4\\). This range is chosen to cover a wide spectrum of values, \n",
    "    from very weak to very strong regularization. By using only 10 values within this range, the complexity \n",
    "    is managed, allowing for a balanced exploration without overwhelming the model with too many variations.\n",
    "</p>\n",
    "<p>\n",
    "    <strong>Kernel Parameter:</strong> \n",
    "    The 'kernel' parameter defines the type of hyperplane used to separate the data. Only two kernel types \n",
    "    are chosen: 'linear' and 'rbf' (Radial Basis Function). 'Linear' kernel is suitable for linearly separable \n",
    "    data, while 'rbf' is effective for non-linear separations. This selection narrows the focus to two primary \n",
    "    and distinct kernel behaviors, simplifying the tuning process.\n",
    "</p>\n",
    "<p>\n",
    "    <strong>Gamma Parameter (Kernel Coefficient):</strong> \n",
    "    'Gamma' influences the decision boundary in kernel-based classifiers like SVM. It is set using np.logspace \n",
    "    to vary logarithmically from \\(10^{-4}\\) to \\(10^1\\). This range provides a balanced exploration from low \n",
    "    to high values, affecting how far the influence of a single training example reaches. Limiting to 10 values \n",
    "    allows for a thorough yet manageable assessment of its impact, avoiding unnecessary complexity\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7e82f11e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Test Accuracy: 0.653169014084507\n",
      "Randomized Search Classification Report on Test data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.72      0.61      0.66       144\n",
      "           2       0.84      0.75      0.79        72\n",
      "           3       0.72      0.62      0.67       144\n",
      "           4       0.64      0.58      0.61       128\n",
      "           5       0.80      0.68      0.73       144\n",
      "           6       0.52      0.42      0.46       144\n",
      "           7       0.85      0.56      0.67        72\n",
      "           8       0.71      0.77      0.74       144\n",
      "           9       0.48      0.89      0.62       144\n",
      "\n",
      "    accuracy                           0.65      1136\n",
      "   macro avg       0.70      0.65      0.66      1136\n",
      "weighted avg       0.68      0.65      0.65      1136\n",
      "\n",
      "Total time taken: 2208.237100839615 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.20, random_state=42, stratify=y)\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "svm = SVC()\n",
    "\n",
    "# Simplified hyperparameter space\n",
    "param_dist = {\n",
    "    'C': np.logspace(-4, 4, 10),  # Fewer values for regularization parameter\n",
    "    'kernel': ['linear', 'rbf'],  # Reduced kernel options\n",
    "    'gamma': np.logspace(-4, 1, 10)  # Fewer values for kernel coefficient\n",
    "}\n",
    "\n",
    "# Randomized search with fewer iterations and cross-validation folds\n",
    "# Set the n_jobs parameter to -1 to use all available cores.\n",
    "random_search = RandomizedSearchCV(svm, param_dist, n_iter=50, cv=3, scoring='accuracy', random_state=42, verbose=1, n_jobs=-1)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Select the best model\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "# Print the test accuracy and classification report\n",
    "test_accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Randomized Search Classification Report on Test data:\")\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "print(f\"Total time taken: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28d3d3f",
   "metadata": {},
   "source": [
    "### Randomized Search SVM with PCA 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c55277d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Test Accuracy: 0.7825704225352113\n",
      "Randomized Search Classification Report on Test data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.81      0.84       144\n",
      "           1       0.81      0.89      0.85        72\n",
      "           2       0.80      0.75      0.77       144\n",
      "           3       0.72      0.77      0.74       128\n",
      "           4       0.90      0.79      0.84       144\n",
      "           5       0.68      0.59      0.63       144\n",
      "           6       0.88      0.72      0.79        72\n",
      "           7       0.82      0.85      0.84       144\n",
      "           8       0.67      0.89      0.77       144\n",
      "\n",
      "    accuracy                           0.78      1136\n",
      "   macro avg       0.79      0.79      0.79      1136\n",
      "weighted avg       0.79      0.78      0.78      1136\n",
      "\n",
      "Total time taken: 571.133519411087 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.20, random_state=42, stratify=y)\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "svm = SVC()\n",
    "\n",
    "# Simplified hyperparameter space\n",
    "param_dist = {\n",
    "    'C': np.logspace(-4, 4, 10),  # Fewer values for regularization parameter\n",
    "    'kernel': ['linear', 'rbf'],  # Reduced kernel options\n",
    "    'gamma': np.logspace(-4, 1, 10)  # Fewer values for kernel coefficient\n",
    "}\n",
    "\n",
    "# Randomized search with fewer iterations and cross-validation folds\n",
    "# Set the n_jobs parameter to -1 to use all available cores.\n",
    "random_search = RandomizedSearchCV(svm, param_dist, n_iter=50, cv=3, scoring='accuracy', random_state=42, verbose=1, n_jobs=-1)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Select the best model\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "# Print the test accuracy and classification report\n",
    "test_accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Randomized Search Classification Report on Test data:\")\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "print(f\"Total time taken: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471be29a",
   "metadata": {},
   "source": [
    "After the randomized search successfully pinpointed promising areas in the hyperparameter space, we are now in a position to utilize Grid Search SV. This method will allow for more precise and detailed adjustments within those identified regions.\n",
    "### Grid Search SVM with PCA 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dde3cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Refined Grid Search Best Hyperparameters:  {'C': 100.0, 'gamma': 0.00019999999999999998}\n",
      "Grid Search Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.74      0.62      0.68       144\n",
      "           2       0.81      0.75      0.78        72\n",
      "           3       0.68      0.65      0.66       144\n",
      "           4       0.59      0.57      0.58       128\n",
      "           5       0.72      0.67      0.69       144\n",
      "           6       0.52      0.47      0.49       144\n",
      "           7       0.79      0.68      0.73        72\n",
      "           8       0.68      0.76      0.72       144\n",
      "           9       0.57      0.80      0.67       144\n",
      "\n",
      "    accuracy                           0.66      1136\n",
      "   macro avg       0.68      0.66      0.67      1136\n",
      "weighted avg       0.66      0.66      0.66      1136\n",
      "\n",
      "Total time taken: 11402.485391378403 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# X_pca and y are already defined and preprocessed\n",
    "\n",
    "# Initialize the SVM classifier with RBF kernel\n",
    "svm = SVC(kernel='rbf')\n",
    "\n",
    "# Define a more focused hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': np.linspace(100, 200, 10),  # Values around 166.81\n",
    "    'gamma': np.linspace(0.0001, 0.001, 10)  # Values around 0.000359\n",
    "}\n",
    "\n",
    "\n",
    "# Conduct grid search\n",
    "\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters from the grid search\n",
    "print(\"Refined Grid Search Best Hyperparameters: \", grid_search.best_params_)\n",
    "\n",
    "\n",
    "# Evaluate the best model from grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "# Print the test accuracy and classification report\n",
    "test_accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Grid Search Classification Report on Test data:\")\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "print(f\"Total time taken: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec56faec",
   "metadata": {},
   "source": [
    "### Grid Search SVM with PCA 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7d58a5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Refined Grid Search Best Hyperparameters:  {'C': 100.0, 'gamma': 0.0005}\n",
      "Test Accuracy: 0.7878521126760564\n",
      "Grid Search Classification Report on Test data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.83      0.86       144\n",
      "           1       0.85      0.92      0.88        72\n",
      "           2       0.81      0.72      0.76       144\n",
      "           3       0.76      0.78      0.77       128\n",
      "           4       0.91      0.82      0.86       144\n",
      "           5       0.68      0.57      0.62       144\n",
      "           6       0.91      0.72      0.81        72\n",
      "           7       0.85      0.88      0.86       144\n",
      "           8       0.61      0.90      0.73       144\n",
      "\n",
      "    accuracy                           0.79      1136\n",
      "   macro avg       0.81      0.79      0.79      1136\n",
      "weighted avg       0.80      0.79      0.79      1136\n",
      "\n",
      "Total time taken: 2943.3644654750824 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# X_pca and y are already defined and preprocessed\n",
    "\n",
    "# Initialize the SVM classifier with RBF kernel\n",
    "svm = SVC(kernel='rbf')\n",
    "\n",
    "# Define a more focused hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': np.linspace(100, 200, 10),  # Values around 166.81\n",
    "    'gamma': np.linspace(0.0001, 0.001, 10)  # Values around 0.000359\n",
    "}\n",
    "\n",
    "\n",
    "# Conduct grid search\n",
    "\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters from the grid search\n",
    "print(\"Refined Grid Search Best Hyperparameters: \", grid_search.best_params_)\n",
    "\n",
    "\n",
    "# Evaluate the best model from grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "# Print the test accuracy and classification report\n",
    "test_accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Grid Search Classification Report on Test data:\")\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "print(f\"Total time taken: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb90b4ce",
   "metadata": {},
   "source": [
    "### We will do further Tuning from Grid search:\n",
    "\n",
    "basd on best Hyperparameters that came from above grid searchSV:  {'Kernel'='rbf', 'C': 100.0, 'gamma': 0.00019999999999999998}\n",
    "### Refined Grid Search SVM with PCA 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1def63cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "Refined Grid Search - Best Hyperparameters: {'C': 95, 'gamma': 0.00022}\n",
      "Test Accuracy: 0.6558098591549296\n",
      "Refined Grid Search Classification Report on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.61      0.67       144\n",
      "           2       0.81      0.75      0.78        72\n",
      "           3       0.70      0.65      0.67       144\n",
      "           4       0.58      0.56      0.57       128\n",
      "           5       0.71      0.67      0.69       144\n",
      "           6       0.52      0.45      0.48       144\n",
      "           7       0.81      0.67      0.73        72\n",
      "           8       0.68      0.76      0.72       144\n",
      "           9       0.56      0.83      0.67       144\n",
      "\n",
      "    accuracy                           0.66      1136\n",
      "   macro avg       0.68      0.66      0.66      1136\n",
      "weighted avg       0.66      0.66      0.65      1136\n",
      "\n",
      "Total time taken: 859.2540204524994 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# X_pca and y are already defined and preprocessed\n",
    "\n",
    "# Initialize the SVM classifier with RBF kernel\n",
    "svm_refined = SVC(kernel='rbf')\n",
    "\n",
    "# Define a more focused hyperparameter grid based on previous results\n",
    "param_grid_refined = {\n",
    "    'C': [95, 100, 105],  # Specific values around 100\n",
    "    'gamma': [0.00018, 0.0002, 0.00022]  # Specific values around gamma = 0.0002\n",
    "}\n",
    "\n",
    "# Perform a refined grid search using the training set\n",
    "# Perform a refined grid search\n",
    "# n_jobs=-1 means that you want to use all available cores on your machine. \n",
    "# This can significantly speed up operations that can be parallelized, \n",
    "# such as cross-validation during grid searches or random searches.\n",
    "\n",
    "grid_search_refined = GridSearchCV(svm_refined, param_grid_refined, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_search_refined.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters from the refined grid search\n",
    "print(\"Refined Grid Search - Best Hyperparameters:\", grid_search_refined.best_params_)\n",
    "\n",
    "# Evaluate the best model from the refined grid search on the test set\n",
    "best_model_refined = grid_search_refined.best_estimator_\n",
    "predictions = best_model_refined.predict(X_test)\n",
    "\n",
    "# Print the test accuracy and classification report\n",
    "test_accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Refined Grid Search Classification Report on Test Set:\")\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "print(f\"Total time taken: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f95971b",
   "metadata": {},
   "source": [
    "### Refined Grid Search SVM with PCA 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "95bf98fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "Refined Grid Search - Best Hyperparameters: {'C': 95, 'gamma': 0.0005}\n",
      "Test Accuracy: 0.7878521126760564\n",
      "Refined Grid Search Classification Report on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.83      0.86       144\n",
      "           1       0.85      0.92      0.88        72\n",
      "           2       0.81      0.72      0.76       144\n",
      "           3       0.76      0.78      0.77       128\n",
      "           4       0.91      0.82      0.86       144\n",
      "           5       0.68      0.57      0.62       144\n",
      "           6       0.91      0.72      0.81        72\n",
      "           7       0.85      0.88      0.86       144\n",
      "           8       0.61      0.90      0.73       144\n",
      "\n",
      "    accuracy                           0.79      1136\n",
      "   macro avg       0.81      0.79      0.79      1136\n",
      "weighted avg       0.80      0.79      0.79      1136\n",
      "\n",
      "Total time taken: 271.0175988674164 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# X_pca and y are already defined and preprocessed\n",
    "\n",
    "# Initialize the SVM classifier with RBF kernel\n",
    "svm_refined = SVC(kernel='rbf')\n",
    "\n",
    "# Define a more focused hyperparameter grid based on previous results\n",
    "param_grid_refined = {\n",
    "    'C': [95, 100, 105],  # Specific values around 100\n",
    "    'gamma': [0.0003, 0.0005, 0.0007]  # Specific values around gamma = 0.0002\n",
    "}\n",
    "\n",
    "# Perform a refined grid search using the training set\n",
    "# Perform a refined grid search\n",
    "# n_jobs=-1 means that you want to use all available cores on your machine. \n",
    "# This can significantly speed up operations that can be parallelized, \n",
    "# such as cross-validation during grid searches or random searches.\n",
    "\n",
    "grid_search_refined = GridSearchCV(svm_refined, param_grid_refined, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_search_refined.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters from the refined grid search\n",
    "print(\"Refined Grid Search - Best Hyperparameters:\", grid_search_refined.best_params_)\n",
    "\n",
    "# Evaluate the best model from the refined grid search on the test set\n",
    "best_model_refined = grid_search_refined.best_estimator_\n",
    "predictions = best_model_refined.predict(X_test)\n",
    "\n",
    "# Print the test accuracy and classification report\n",
    "test_accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Refined Grid Search Classification Report on Test Set:\")\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "print(f\"Total time taken: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91bd0e8",
   "metadata": {},
   "source": [
    "#### Comparing result among Randomized Search SVM, Tunned Grid Search SV and Refined Grid Search SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648e5d7f",
   "metadata": {},
   "source": [
    "### Result Table when used PCA 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2149a2df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model Description  Average Precision  Average Recall  Average F1-Score\n",
      "  Randomized Search SVM               0.70            0.65              0.66\n",
      "        Grid Search SVM               0.68            0.66              0.67\n",
      "Refined Grid Search SVM               0.68            0.66              0.66\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set the display width option to a larger value\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Accuracy: Average Macro  of precision, recall, and F1-score from our classification reports\n",
    "# Macro averaging is useful as we want to understand the model's performance across all classes equally, \n",
    "# without giving more importance to one class over another\n",
    "\n",
    "# Randomized Search SVM\n",
    "avg_precision_random = 0.70\n",
    "avg_recall_random = 0.65\n",
    "avg_f1_random = 0.66\n",
    "\n",
    "# Grid Search SVM\n",
    "avg_precision_grid = 0.68\n",
    "avg_recall_grid = 0.66\n",
    "avg_f1_grid = 0.67\n",
    "\n",
    "# Refined Grid Search SVM\n",
    "avg_precision_refined_grid = 0.68\n",
    "avg_recall_refined_grid = 0.66\n",
    "avg_f1_refined_grid = 0.66\n",
    "\n",
    "# Create a DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    'Model Description': ['Randomized Search SVM', 'Grid Search SVM', 'Refined Grid Search SVM'],\n",
    "    'Average Precision': [avg_precision_random, avg_precision_grid, avg_precision_refined_grid],\n",
    "    'Average Recall': [avg_recall_random, avg_recall_grid, avg_recall_refined_grid],\n",
    "    'Average F1-Score': [avg_f1_random, avg_f1_grid, avg_f1_refined_grid]\n",
    "})\n",
    "\n",
    "# Display the DataFrame without the index\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4244d405",
   "metadata": {},
   "source": [
    "### Result Table when used PCA 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "54bdda9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model Description  Average Precision  Average Recall  Average F1-Score\n",
      "  Randomized Search SVM               0.79            0.79              0.79\n",
      "        Grid Search SVM               0.81            0.79              0.79\n",
      "Refined Grid Search SVM               0.81            0.79              0.79\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set the display width option to a larger value\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Accuracy: Average Macro  of precision, recall, and F1-score from our classification reports\n",
    "# Macro averaging is useful as we want to understand the model's performance across all classes equally, \n",
    "# without giving more importance to one class over another\n",
    "\n",
    "# Randomized Search SVM\n",
    "avg_precision_random = 0.79\n",
    "avg_recall_random = 0.79\n",
    "avg_f1_random = 0.79\n",
    "\n",
    "# Grid Search SVM\n",
    "avg_precision_grid = 0.81\n",
    "avg_recall_grid = 0.79\n",
    "avg_f1_grid = 0.79\n",
    "\n",
    "# Refined Grid Search SVM\n",
    "avg_precision_refined_grid = 0.81\n",
    "avg_recall_refined_grid = 0.79\n",
    "avg_f1_refined_grid = 0.79\n",
    "\n",
    "# Create a DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    'Model Description': ['Randomized Search SVM', 'Grid Search SVM', 'Refined Grid Search SVM'],\n",
    "    'Average Precision': [avg_precision_random, avg_precision_grid, avg_precision_refined_grid],\n",
    "    'Average Recall': [avg_recall_random, avg_recall_grid, avg_recall_refined_grid],\n",
    "    'Average F1-Score': [avg_f1_random, avg_f1_grid, avg_f1_refined_grid]\n",
    "})\n",
    "\n",
    "# Display the DataFrame without the index\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21340773",
   "metadata": {},
   "source": [
    "## <u>Decision Tree<u>\n",
    "<p>\n",
    "    A PCA with 95% resulted in an F1 score and precision of 56%, whereas a reduction to 80% yielded a slightly higher F1 score of 58%. This outcome suggests that a simpler structure tends to be more effective. While the 95% PCA incorporated 1771 features, reducing it to 80% utilized only 540 features. Despite the improvements with a simpler model, the overall results still did not reach a promising level.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598ca082",
   "metadata": {},
   "source": [
    "<p>We will proceed with careful consideration in selecting the hyperparameters for the decision tree.</p>\n",
    "<ol>\n",
    "    <li><strong>Criterion:</strong>\n",
    "        <ul>\n",
    "            <li>The 'criterion' parameter determines the function used to measure the quality of a split in the decision tree.</li>\n",
    "            <li>The options are 'gini' and 'entropy':</li>\n",
    "            <li>'Gini' measures the frequency at which any element of the dataset will be wrongly labeled when it is randomly chosen.</li>\n",
    "            <li>'Entropy' is a measure of information that indicates the disorder or uncertainty of the data.</li>\n",
    "            <li>Choice between 'gini' and 'entropy' affects how decision tree nodes decide to split data.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>Max Depth:</strong>\n",
    "        <ul>\n",
    "            <li>The 'max_depth' parameter specifies the maximum depth of the tree.</li>\n",
    "            <li>The values provided are [10, 20, 30, None]:</li>\n",
    "            <li>Specific numerical values (10, 20, 30) limit the depth of the tree.</li>\n",
    "            <li>A deeper tree can capture more information about the data, but risks overfitting.</li>\n",
    "            <li>'None' means that nodes are expanded until all leaves are pure or until all leaves contain less than the minimum samples required to split a node.</li>\n",
    "            <li>Setting 'max_depth' helps in preventing the tree from becoming overly complex and overfitting the training data.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>Min Samples Split:</strong>\n",
    "        <ul>\n",
    "            <li>The 'min_samples_split' parameter defines the minimum number of samples required to split an internal node.</li>\n",
    "            <li>The options [2, 5, 10] represent the minimum number of samples that a node must have before it can be split:</li>\n",
    "            <li>Lower values allow the tree to split even with a small number of samples, leading to potentially deeper trees.</li>\n",
    "            <li>Higher values prevent the model from learning overly specific patterns, thus controlling overfitting.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>Min Samples Leaf:</strong>\n",
    "        <ul>\n",
    "            <li>The 'min_samples_leaf' parameter sets the minimum number of samples required to be at a leaf node.</li>\n",
    "            <li>The choices [1, 2, 4] dictate the smallest size of leaves:</li>\n",
    "            <li>Smaller leaf sizes allow the tree to capture more information about the data, which can be beneficial for prediction but may lead to overfitting.</li>\n",
    "            <li>Larger leaf sizes increase the generalization ability of the model and help prevent overfitting.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ebadd6",
   "metadata": {},
   "source": [
    "### Decision Tree with PCA 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b31662e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#Split your dataset into training and testing sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.20, random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05f54434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for the DecisionTreeClassifier\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b0faec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Total time taken: 1384.7651715278625 seconds\n",
      "Best hyperparameters found:\n",
      "{'criterion': 'gini', 'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "\n",
      "Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.68      0.66      0.67       144\n",
      "           2       0.61      0.58      0.60        72\n",
      "           3       0.60      0.55      0.57       144\n",
      "           4       0.45      0.48      0.46       128\n",
      "           5       0.61      0.62      0.62       144\n",
      "           6       0.45      0.53      0.49       144\n",
      "           7       0.43      0.44      0.44        72\n",
      "           8       0.69      0.62      0.65       144\n",
      "           9       0.57      0.54      0.55       144\n",
      "\n",
      "    accuracy                           0.57      1136\n",
      "   macro avg       0.56      0.56      0.56      1136\n",
      "weighted avg       0.57      0.57      0.57      1136\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the DecisionTreeClassifier and conduct a grid search:\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(estimator=dtree, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "print(f\"Total time taken: {end_time - start_time} seconds\")\n",
    "\n",
    "# Print the best hyperparameters and their corresponding performance\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(grid_search.best_params_)\n",
    "print(\"\\nClassification Report on Test Data:\")\n",
    "y_pred = grid_search.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f903a764",
   "metadata": {},
   "source": [
    "### Decision Tree with PCA 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "87a6d08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#Split your dataset into training and testing sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.20, random_state=42,stratify=y)\n",
    "# Define the parameter grid for the DecisionTreeClassifier\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f9878f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Total time taken: 1251.4177117347717 seconds\n",
      "Best hyperparameters found:\n",
      "{'criterion': 'gini', 'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "\n",
      "Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.63      0.66       144\n",
      "           1       0.66      0.56      0.60        72\n",
      "           2       0.58      0.59      0.59       144\n",
      "           3       0.49      0.52      0.51       128\n",
      "           4       0.59      0.62      0.60       144\n",
      "           5       0.45      0.48      0.46       144\n",
      "           6       0.54      0.51      0.53        72\n",
      "           7       0.66      0.64      0.65       144\n",
      "           8       0.56      0.59      0.58       144\n",
      "\n",
      "    accuracy                           0.58      1136\n",
      "   macro avg       0.58      0.57      0.58      1136\n",
      "weighted avg       0.58      0.58      0.58      1136\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the DecisionTreeClassifier and conduct a grid search:\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(estimator=dtree, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "print(f\"Total time taken: {end_time - start_time} seconds\")\n",
    "\n",
    "# Print the best hyperparameters and their corresponding performance\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(grid_search.best_params_)\n",
    "print(\"\\nClassification Report on Test Data:\")\n",
    "y_pred = grid_search.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c118112",
   "metadata": {},
   "source": [
    "<h3>Ensemble Technique XGBoost, CatBoost</h3>\n",
    "\n",
    "<p>\n",
    "    In machine learning, an \"ensemble technique\" refers to a method where multiple models, often called \"weak learners,\" are combined to form a stronger predictive model. The idea is to leverage the strengths of each individual model to improve overall accuracy, reduce overfitting, or enhance the model's ability to generalize to new data. We will use <b>XGBoost (eXtreme Gradient Boosting)</b> and <b>CatBoost (Category Boosting)</b> in our next approach.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "    In the scenario of signature verification involving large datasets and multiple classes, ensemble techniques like XGBoost and CatBoost provide significant advantages over simpler approaches such as single decision trees, even those optimized with grid search:\n",
    "    <ul>\n",
    "        <li><strong>Handling Large and Complex Data:</strong><br>\n",
    "            XGBoost and CatBoost are highly efficient with large datasets, capable of effectively managing thousands to millions of data points. This makes them particularly suitable for complex datasets in signature verification.\n",
    "        </li>\n",
    "        <li><strong>Improved Accuracy and Robustness:</strong><br>\n",
    "            As ensemble methods, XGBoost and CatBoost generally yield more accurate models than a single decision tree. They achieve this by reducing the risk of overfitting and enhancing generalization through the aggregation of multiple models' predictions.\n",
    "        </li>\n",
    "        <li><strong>Handling Imbalanced Classes:</strong><br>\n",
    "            In scenarios with multi-class problems like signature verification, class imbalance can be a significant issue. XGBoost and CatBoost are equipped with mechanisms to effectively handle this imbalance, which is crucial for accurate classification.\n",
    "        </li>\n",
    "        <li><strong>Feature Importance and Selection:</strong><br>\n",
    "            Both XGBoost and CatBoost excel in feature selection and understanding feature importance. This capability is critical in signature verification for pinpointing unique signature characteristics that are vital for accurate classification.\n",
    "        </li>\n",
    "        <li><strong>Flexibility and Scalability:</strong><br>\n",
    "            These algorithms offer considerable flexibility in optimization objectives and are scalable to different data sizes and shapes. This adaptability is crucial for handling varying complexities in datasets, typical in signature verification tasks.\n",
    "        </li>\n",
    "        <li><strong>Handling Noise and Variability:</strong><br>\n",
    "            Signature data can often be variable and noisy. Ensemble methods like XGBoost and CatBoost are better suited for such data compared to a single decision tree, as they are more adept at distinguishing signal from noise.\n",
    "        </li>\n",
    "        <li><strong>Speed and Efficiency:</strong><br>\n",
    "            Known for their computational efficiency, XGBoost and CatBoost utilize advanced techniques like gradient boosting and support for categorical features, which accelerate the training and prediction processes.\n",
    "        </li>\n",
    "    </ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41da020a",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e041191",
   "metadata": {},
   "source": [
    "<p>We will carry out a series of experiments using XGBoost with PCA, retaining 80% of the variance. We also uses Truncated SVD for dimenstion reduction to 400 features. This decision is based on our observation that reducing dimensionality simplifies models, reduces overfitting, and improves predictions. These experiments will involve a Coarse Tuning phase.</p>\n",
    "<p>In Coarse Tuning, we initiate the hyperparameter optimization process with broader ranges or fewer values for each hyperparameter. This approach is designed to efficiently identify promising regions within the hyperparameter space, avoiding excessive computational costs on areas less likely to yield optimal results.</p>\n",
    "\n",
    "<p>The insights gained from the coarse tuning phase will guide us in refining our hyperparameter search in a more focused and detailed manner, known as fine-tuning. Fine-tuning involves conducting additional iterations with narrower hyperparameter ranges, leveraging the knowledge acquired during coarse tuning to further enhance model performance. We start with folloiwng hyperparameter.</p>\n",
    "<ol>\n",
    "    <li>\n",
    "        <strong>max_depth:</strong>\n",
    "        <ul>\n",
    "            <li>This parameter sets the maximum depth of each tree in the XGBoost model.</li>\n",
    "            <li>Depths of 3, 5, and 7 are chosen as starting points to see how complex each tree should be. Deeper trees can capture more complex patterns but may lead to overfitting, while shallower trees may not capture enough nuances in the data.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "        <strong>learning_rate:</strong>\n",
    "        <ul>\n",
    "            <li>The learning rate is a crucial parameter in gradient boosting models like XGBoost. It determines the step size at each iteration while moving toward a minimum of a loss function.</li>\n",
    "            <li>A learning rate of 0.1 or 0.3 is typical for starting. A smaller learning rate makes the model more robust to overfitting but requires more trees (n_estimators) to converge.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "        <strong>n_estimators:</strong>\n",
    "        <ul>\n",
    "            <li>This parameter specifies the number of trees in the forest of the model.</li>\n",
    "            <li>Choosing 50 and 100 trees provides a balance between computational efficiency and having enough trees to capture the patterns in the data effectively.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "        <strong>subsample:</strong>\n",
    "        <ul>\n",
    "            <li>Subsample refers to the fraction of samples used for fitting each tree.</li>\n",
    "            <li>A value of 0.8 means using 80% of the data for each tree, which can help prevent overfitting. A value of 1 means using all the data, which can provide more information to each tree but might increase the risk of overfitting.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "        <strong>colsample_bytree:</strong>\n",
    "        <ul>\n",
    "            <li>This parameter sets the fraction of features (columns) to be randomly sampled for each tree.</li>\n",
    "            <li>Like subsample, values of 0.8 and 1 are chosen to see the effect of using a subset of features versus using all features for each tree. </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d258da5",
   "metadata": {},
   "source": [
    "### XBoost with PCA 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e7d1043c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best Parameters from Coarse Search:  {'subsample': 1, 'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.3, 'colsample_bytree': 1}\n",
      "Total time taken: 1990.9670014381409 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Adjust labels to start from 0 instead of 1\n",
    "# XGBoost is expecting the class labels to start from 0 and go up to num_class - 1. Since you have class labels starting from 1 to 9, \n",
    "# We need to subtract 1 from each label to match the expected format.\n",
    "\n",
    "y = np.array(labels) - 1  # This ensures class labels start at 0\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.20, random_state=42,stratify=y)\n",
    "\n",
    "# Stage 1: Coarse Hyperparameter Grid\n",
    "coarse_param_grid = {\n",
    "    'max_depth': [3, 5, 7],  # Few depths to start with\n",
    "    'learning_rate': [0.1, 0.3],  # Common starting learning rates\n",
    "    'n_estimators': [50, 100],  # Not too many trees\n",
    "    'subsample': [0.8, 1],  # Just a couple of values for subsampling\n",
    "    'colsample_bytree': [0.8, 1],  # Same for feature subsampling\n",
    "}\n",
    "\n",
    "xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "# RandomizedSearchCV to explore the hyperparameter space\n",
    "coarse_search = RandomizedSearchCV(xgb_clf, param_distributions=coarse_param_grid, \n",
    "                                   n_iter=10, scoring='accuracy', cv=3, \n",
    "                                   n_jobs=-1, random_state=42, verbose=1)\n",
    "\n",
    "coarse_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters from coarse search\n",
    "print(\"Best Parameters from Coarse Search: \", coarse_search.best_params_)\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "print(f\"Total time taken: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda1fc24",
   "metadata": {},
   "source": [
    "<p>In our coarse search with XGBoost, we are extending our hyperparameter exploration to include three additional parameters: 'gamma,' 'reg_alpha,' and 'reg_lambda.' These parameters have been introduced to further enhance the model's performance:</p>\n",
    "\n",
    "<p><strong>'gamma':</strong> This parameter controls the minimum loss reduction required to make a further partition on a leaf node. By exploring values such as 0, 0.1, and 0.2, we aim to understand how different thresholds for loss reduction impact the model's ability to capture meaningful patterns in the data.</p>\n",
    "\n",
    "<p><strong>'reg_alpha':</strong> 'reg_alpha' adds L1 regularization to the objective function, which helps prevent overfitting by adding a penalty to the absolute value of the coefficients. We are considering values of 0, 0.1, and 0.2 to evaluate the effect of different levels of L1 regularization on the model's robustness.</p>\n",
    "\n",
    "<p><strong>'reg_lambda':</strong> 'reg_lambda' adds L2 regularization to the objective function, which similarly prevents overfitting by adding a penalty to the square of the coefficients. We will explore values of 1, 1.1, and 1.2 to gauge the impact of different levels of L2 regularization on the model's performance.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7a3e25d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best Parameters from Coarse Search:  {'reg_lambda': 1, 'reg_alpha': 0, 'gamma': 0}\n",
      "Total time taken: 3716.3269517421722 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Adjust labels to start from 0 instead of 1\n",
    "# XGBoost is expecting the class labels to start from 0 and go up to num_class - 1. Since you have class labels starting from 1 to 9, \n",
    "# We need to subtract 1 from each label to match the expected format.\n",
    "\n",
    "y = np.array(labels) - 1  # This ensures class labels start at 0\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.20, random_state=42,stratify=y)\n",
    "\n",
    "# Stage 1: Coarse Hyperparameter Grid\n",
    "coarse_param_grid = {\n",
    "    'gamma': [0, 0.1, 0.2],  # New parameter to explore\n",
    "    'reg_alpha': [0, 0.1, 0.2],  # New parameter to explore\n",
    "    'reg_lambda': [1, 1.1, 1.2]  # New parameter to explore\n",
    "}\n",
    "\n",
    "xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "# RandomizedSearchCV to explore the hyperparameter space\n",
    "coarse_search = RandomizedSearchCV(xgb_clf, param_distributions=coarse_param_grid, \n",
    "                                   n_iter=10, scoring='accuracy', cv=3, \n",
    "                                   n_jobs=-1, random_state=42, verbose=1)\n",
    "\n",
    "coarse_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters from coarse search\n",
    "print(\"Best Parameters from Coarse Search: \", coarse_search.best_params_)\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "print(f\"Total time taken: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9494778",
   "metadata": {},
   "source": [
    "<p>Following the two rounds of coarse hyperparameter searches, we have identified the most promising parameter settings. These optimal configurations will be utilized in constructing the XGBoost classification model for generating the classification report:</p>\n",
    "<p><strong>Best Parameters from the First Coarse Search:</strong></p>\n",
    "<ul>\n",
    "    <li>'subsample': 1</li>\n",
    "    <li>'n_estimators': 100</li>\n",
    "    <li>'max_depth': 5</li>\n",
    "    <li>'learning_rate': 0.3</li>\n",
    "    <li>'colsample_bytree': 1</li>\n",
    "</ul>\n",
    "<p><strong>Best Parameters from the Second Coarse Search:</strong></p>\n",
    "<ul>\n",
    "    <li>'reg_lambda': 1</li>\n",
    "    <li>'reg_alpha': 0</li>\n",
    "    <li>'gamma': 0</li>\n",
    "</ul>\n",
    "<p>In addition, we have specified the following settings:</p>\n",
    "<ul>\n",
    "    <li><strong>eval_metric='mlogloss':</strong> This parameter specifies the evaluation metric that XGBoost uses during training and validation. In this case, eval_metric is set to 'mlogloss,' which stands for Multi-Class Logarithmic Loss.</li>\n",
    "    <li><strong>use_label_encoder=False:</strong> When use_label_encoder=False, XGBoost expects the categorical labels to be already encoded as integers or floats, and it won't perform any additional encoding. This can be advantageous when you want to manage the label encoding process externally or if your data already contains encoded categorical labels.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5a36a197",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.90      0.95      0.92       144\n",
      "           2       0.88      0.92      0.90        72\n",
      "           3       0.83      0.87      0.85       144\n",
      "           4       0.79      0.75      0.77       128\n",
      "           5       0.92      0.92      0.92       144\n",
      "           6       0.90      0.73      0.80       144\n",
      "           7       0.83      0.72      0.77        72\n",
      "           8       0.88      0.92      0.90       144\n",
      "           9       0.84      0.94      0.89       144\n",
      "\n",
      "    accuracy                           0.86      1136\n",
      "   macro avg       0.86      0.86      0.86      1136\n",
      "weighted avg       0.86      0.86      0.86      1136\n",
      "\n",
      "Total time taken: 189.15815544128418 seconds\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize XGBoost classifier with the best parameters from coarse search\n",
    "xgb_best = XGBClassifier(\n",
    "    subsample=1,  # best 'subsample' value from coarse search\n",
    "    n_estimators=100,  # best 'n_estimators' value from coarse search\n",
    "    max_depth=5,  # best 'max_depth' value from coarse search\n",
    "    learning_rate=0.3,  # best 'learning_rate' value from coarse search\n",
    "    colsample_bytree=1,  # best 'colsample_bytree' value from coarse search\n",
    "    reg_lambda= 1,\n",
    "    reg_alpha= 0, \n",
    "    gamma= 0,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss' #for multiclass \n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "xgb_best.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = xgb_best.predict(X_test)\n",
    "\n",
    "# Adjust predictions to match original class labels (1 to 9)\n",
    "adjusted_predictions = predictions + 1  # Adding 1 to each prediction\n",
    "\n",
    "# Define the class labels starting from 1\n",
    "class_labels = [1, 2, 3, 4, 5, 6, 7, 8, 9]  # Modify this list according to your class labels\n",
    "\n",
    "# Print the classification report with class labels starting from 1\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test + 1, adjusted_predictions, labels=class_labels))  # Adding 1 to y_test to match labels\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "print(f\"Total time taken: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94db2ed3",
   "metadata": {},
   "source": [
    "<p>In our ongoing pursuit of improved precision and F1 scores, we will further fine-tune the hyperparameters used in the previous rounds. This time, we will conduct an additional round of exploration with the following hyperparameters:</p>\n",
    "<ul>\n",
    "    <li>'max_depth': [4, 5, 6] - We will consider three values around the previously identified best value.</li>\n",
    "    <li>'n_estimators': [90, 100, 110] - Three values in proximity to the best value found earlier.</li>\n",
    "    <li>'learning_rate': [0.25, 0.3, 0.35] - We will explore three values around the previously determined optimal learning rate.</li>\n",
    "</ul>\n",
    "<p>This meticulous fine-tuning process aims to push the model's performance to its highest potential, enhancing precision and achieving a more favorable F1 score.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "659834ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best Parameters from Coarse Search:  {'n_estimators': 110, 'max_depth': 4, 'learning_rate': 0.3}\n",
      "Total time taken: 2906.6502029895782 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Adjust labels to start from 0 instead of 1\n",
    "# XGBoost is expecting the class labels to start from 0 and go up to num_class - 1. Since you have class labels starting from 1 to 9, \n",
    "# We need to subtract 1 from each label to match the expected format.\n",
    "\n",
    "y = np.array(labels) - 1  # This ensures class labels start at 0\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.20, random_state=42,stratify=y)\n",
    "\n",
    "# Coarse Hyperparameter Grid\n",
    "coarse_param_grid = {\n",
    "    'max_depth': [4, 5, 6],  # Three values around the best value\n",
    "    'n_estimators': [90, 100, 110],  # Three values around the best value\n",
    "    'learning_rate': [0.25, 0.3, 0.35]  # Three values around the best value\n",
    "}\n",
    "\n",
    "xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "# RandomizedSearchCV to explore the hyperparameter space\n",
    "coarse_search = RandomizedSearchCV(xgb_clf, param_distributions=coarse_param_grid, \n",
    "                                   n_iter=10, scoring='accuracy', cv=3, \n",
    "                                   n_jobs=-1, random_state=42, verbose=1)\n",
    "\n",
    "coarse_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters from coarse search\n",
    "print(\"Best Parameters from Coarse Search: \", coarse_search.best_params_)\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "print(f\"Total time taken: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fe6c65",
   "metadata": {},
   "source": [
    "<p>With our optimized hyperparameters in place, we are now ready to make predictions using our XGBoost classification model. These best hyperparameters, derived from the coarse hyperparameter search, are as follows:</p>\n",
    "<ul>\n",
    "    <li><strong>'n_estimators':</strong> 110</li>\n",
    "    <li><strong>'max_depth':</strong> 4</li>\n",
    "    <li><strong>'learning_rate':</strong> 0.3</li>\n",
    "</ul>\n",
    "<p> And we incorporated the following settings that we obtained from earlier steps:</p>\n",
    "<ul>\n",
    "    <li><strong>'subsample':</strong> 1 (best 'subsample' value from the coarse search)</li>\n",
    "    <li><strong>'colsample_bytree':</strong> 1 (best 'colsample_bytree' value from the coarse search)</li>\n",
    "    <li><strong>'use_label_encoder':</strong> False</li>\n",
    "    <li><strong>'eval_metric':</strong> 'mlogloss'</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "72306bac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.92      0.92      0.92       396\n",
      "           2       0.90      0.83      0.87       252\n",
      "           3       0.84      0.84      0.84       402\n",
      "           4       0.78      0.82      0.80       422\n",
      "           5       0.93      0.90      0.91       456\n",
      "           6       0.83      0.89      0.86       429\n",
      "           7       0.90      0.88      0.89       291\n",
      "           8       0.91      0.91      0.91       456\n",
      "           9       0.94      0.93      0.94       417\n",
      "\n",
      "    accuracy                           0.88      3521\n",
      "   macro avg       0.88      0.88      0.88      3521\n",
      "weighted avg       0.88      0.88      0.88      3521\n",
      "\n",
      "Total time taken: 677.0040559768677 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Adjust labels to start from 0 instead of 1\n",
    "# XGBoost is expecting the class labels to start from 0 and go up to num_class - 1. Since you have class labels starting from 1 to 9, \n",
    "# We need to subtract 1 from each label to match the expected format.\n",
    "\n",
    "y = np.array(labels) - 1  # This ensures class labels start at 0\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.20, random_state=42,stratify=y)\n",
    "\n",
    "# Initialize XGBoost classifier with the best parameters from coarse search\n",
    "xgb_best = XGBClassifier(\n",
    "    subsample=1,  # best 'subsample' value from coarse search\n",
    "    n_estimators=110,  # best 'n_estimators' value from coarse search\n",
    "    max_depth=4,  # best 'max_depth' value from coarse search\n",
    "    learning_rate=0.3,  # best 'learning_rate' value from coarse search\n",
    "    colsample_bytree=1,  # best 'colsample_bytree' value from coarse search\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss'\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "xgb_best.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = xgb_best.predict(X_test)\n",
    "\n",
    "# Adjust predictions to match original class labels (1 to 9)\n",
    "adjusted_predictions = predictions + 1  # Adding 1 to each prediction\n",
    "\n",
    "# Define the class labels starting from 1\n",
    "class_labels = [1, 2, 3, 4, 5, 6, 7, 8, 9]  # Modify this list according to your class labels\n",
    "\n",
    "# Print the classification report with class labels starting from 1\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test + 1, adjusted_predictions, labels=class_labels))  # Adding 1 to y_test to match labels\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "print(f\"Total time taken: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d32363",
   "metadata": {},
   "source": [
    "<p>We observed an improvement in both precision and F1 score, increasing from 86% to 89%.</p>\n",
    "<p>The classification report provides a detailed evaluation of the model's performance for each class. Each row in the report corresponds to a specific class (in this case, classes 1 to 9). Here's an explanation of the key metrics:</p>\n",
    "<ul>\n",
    "    <li><strong>Precision:</strong> Precision measures the percentage of correctly predicted positive instances out of all instances predicted as positive. For example, in class 1, the precision is 0.92, which means that 92% of the instances predicted as class 1 were correct.</li>\n",
    "    <li><strong>Recall:</strong> Recall (also known as sensitivity or true positive rate) measures the percentage of correctly predicted positive instances out of all actual positive instances. For instance, in class 2, the recall is 0.83, indicating that 83% of the actual class 2 instances were correctly predicted.</li>\n",
    "    <li><strong>F1-Score:</strong> The F1-score is the harmonic mean of precision and recall, providing a balanced measure of a model's accuracy. It considers both false positives and false negatives. In class 3, the F1-score is 0.84, which reflects the model's ability to balance precision and recall for that class.</li>\n",
    "    <p>It's important to mention that we expanded the dataset, resulting in an increased support count for each class. This expansion allowed us to gather more data for training and testing, which contributed to the improved precision and F1 score. Furthermore, it's worth noting that the model exhibited consistent and reliable performance when tested on a dataset containing 6,700 instances, indicating its robustness and generalizability.</p>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68adf9a5",
   "metadata": {},
   "source": [
    "### XGBoost with Truncated SVD\n",
    "\n",
    "\n",
    "<p>We are now going to conduct tests using Truncated SVD for dimension reduction to assess its impact on accuracy. In this approach, we have reduced the dimensionality to 400 features. The choice of 400 features is based on our observation that lower-dimensional features tend to yield simpler models. In comparison, when we employed PCA, we utilized 540 features.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a09c8ac6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.92      0.92      0.92       396\n",
      "           2       0.91      0.84      0.87       252\n",
      "           3       0.85      0.84      0.85       402\n",
      "           4       0.81      0.83      0.82       422\n",
      "           5       0.94      0.92      0.93       456\n",
      "           6       0.84      0.89      0.86       429\n",
      "           7       0.89      0.87      0.88       291\n",
      "           8       0.92      0.93      0.92       456\n",
      "           9       0.93      0.94      0.94       417\n",
      "\n",
      "    accuracy                           0.89      3521\n",
      "   macro avg       0.89      0.89      0.89      3521\n",
      "weighted avg       0.89      0.89      0.89      3521\n",
      "\n",
      "Total time taken: 437.9006760120392 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Adjust labels to start from 0 instead of 1\n",
    "# XGBoost is expecting the class labels to start from 0 and go up to num_class - 1. Since you have class labels starting from 1 to 9, \n",
    "# We need to subtract 1 from each label to match the expected format.\n",
    "\n",
    "y = np.array(labels) - 1  # This ensures class labels start at 0\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.20, random_state=42,stratify=y)\n",
    "\n",
    "# Initialize XGBoost classifier with the best parameters from coarse search\n",
    "xgb_best = XGBClassifier(\n",
    "    subsample=1,  # best 'subsample' value from coarse search\n",
    "    n_estimators=110,  # best 'n_estimators' value from coarse search\n",
    "    max_depth=4,  # best 'max_depth' value from coarse search\n",
    "    learning_rate=0.3,  # best 'learning_rate' value from coarse search\n",
    "    colsample_bytree=1,  # best 'colsample_bytree' value from coarse search\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss'\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "xgb_best.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = xgb_best.predict(X_test)\n",
    "\n",
    "# Adjust predictions to match original class labels (1 to 9)\n",
    "adjusted_predictions = predictions + 1  # Adding 1 to each prediction\n",
    "\n",
    "# Define the class labels starting from 1\n",
    "class_labels = [1, 2, 3, 4, 5, 6, 7, 8, 9]  # Modify this list according to your class labels\n",
    "\n",
    "# Print the classification report with class labels starting from 1\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test + 1, adjusted_predictions, labels=class_labels))  # Adding 1 to y_test to match labels\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "print(f\"Total time taken: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35a0ba3",
   "metadata": {},
   "source": [
    "<p><strong>Important Observation:</strong> The results indicate that employing Truncated SVD with 400 features led to a 1% increase in accuracy compared to using PCA with 80% variance retention while maintaining the same set of hyperparameters.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd167d5",
   "metadata": {},
   "source": [
    "### XGbost best Model:\n",
    "From the analysis above, it is evident that the best-performing model is <u>XGBoost with Truncated SVD<u>, achieving an  precision of 89%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7fbb55",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f353457",
   "metadata": {},
   "source": [
    "<p>Initially, we used the dimensionality reduction technique PCA with 80% variance for CatBoost. We trained two prediction models with specific hyperparameters: one with 1000 iterations and a learning rate of 0.1, and the other with 500 iterations and a learning rate of 0.3. Additionally, we kept other hyperparameters such as l2_leaf_reg and depth consistent between the two models.</p>\n",
    "<p>The results indicated that the model with 1000 iterations and a learning rate of 0.1 achieved a higher level of accuracy compared to the alternative configuration with 500 iterations and a learning rate of 0.3.</p>\n",
    "<p>Here's a brief explanation of the hyperparameters used:</p>\n",
    "<ol>\n",
    "    <li><strong>iterations=1000:</strong> This parameter specifies the maximum number of trees (boosting iterations) to be built. It controls the number of rounds in the gradient boosting process.</li>\n",
    "    <li><strong>learning_rate=0.1 or 0.3:</strong> The learning rate determines the step size at each iteration while moving towards a minimum of the loss function. A smaller learning rate makes the model more robust to overfitting but requires more iterations to converge.</li>\n",
    "    <li><strong>depth=6:</strong> It sets the maximum depth of each decision tree in the CatBoost ensemble. Deeper trees can capture more complex patterns but may lead to overfitting, while shallower trees are less likely to overfit but may not capture complex relationships in the data.</li>\n",
    "    <li><strong>l2_leaf_reg=3:</strong> This parameter controls L2 regularization for leaf values. It adds a penalty term to the sum of squared leaf values during training, which helps prevent overfitting. The value of 3 represents the strength of regularization, with higher values leading to stronger regularization.</li>\n",
    "    <li><strong>loss_function='MultiClass':</strong> This parameter specifies the loss function to be used for multi-class classification tasks. 'MultiClass' indicates that CatBoost should use a loss function suitable for multi-class problems. It's important to choose the appropriate loss function based on the problem type.</li>\n",
    "</ol>\n",
    "<p><strong>Learning Rate Comparison:</strong></p>\n",
    "<ul>\n",
    "    <li><strong>Learning Rate of 0.3:</strong>\n",
    "        <ul>\n",
    "            <li><strong>Higher Learning Rate:</strong> A learning rate of 0.3 is relatively higher. This means that during each iteration of training, the model makes larger adjustments to its parameters based on the gradient of the loss function.</li>\n",
    "            <li><strong>Faster Initial Training:</strong> A higher learning rate can result in faster convergence during the initial phase of training. The model may reach a reasonably good solution quickly.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>Learning Rate of 0.1:</strong>\n",
    "        <ul>\n",
    "            <li><strong>Lower Learning Rate:</strong> A learning rate of 0.1 is lower than 0.3. This means that parameter updates during training are smaller.</li>\n",
    "            <li><strong>Slower Convergence:</strong> With a lower learning rate, the model tends to converge more slowly. It takes smaller steps towards the minimum of the loss function.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a7d082",
   "metadata": {},
   "source": [
    "### CatBoost with PCA 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "771faacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pip install xgboost catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9380f132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.94      0.92       144\n",
      "           1       0.92      0.97      0.95        72\n",
      "           2       0.85      0.90      0.87       144\n",
      "           3       0.86      0.81      0.84       128\n",
      "           4       0.92      0.93      0.93       144\n",
      "           5       0.95      0.80      0.87       144\n",
      "           6       0.85      0.76      0.80        72\n",
      "           7       0.91      0.92      0.91       144\n",
      "           8       0.86      0.95      0.90       144\n",
      "\n",
      "    accuracy                           0.89      1136\n",
      "   macro avg       0.89      0.89      0.89      1136\n",
      "weighted avg       0.89      0.89      0.89      1136\n",
      "\n",
      "Total time taken: 774.9832634925842 seconds\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.20, random_state=42,stratify=y)\n",
    "\n",
    "# Initialize CatBoost classifier\n",
    "catboost_model = CatBoostClassifier(\n",
    "    iterations=1000, \n",
    "    learning_rate=0.1, \n",
    "    depth=6,\n",
    "    l2_leaf_reg=3,\n",
    "    loss_function='MultiClass',  # Specifying the loss function for multi-class classification\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "catboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = catboost_model.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"CatBoost Classification Report:\")\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "print(f\"Total time taken: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a63508",
   "metadata": {},
   "source": [
    "<p>Next, we conducted experiments with fewer iterations and a higher learning rate. However, the results showed a decrease in accuracy.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "acceacf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.94      0.91       144\n",
      "           1       0.93      0.93      0.93        72\n",
      "           2       0.83      0.85      0.84       144\n",
      "           3       0.83      0.80      0.81       128\n",
      "           4       0.94      0.90      0.92       144\n",
      "           5       0.86      0.78      0.82       144\n",
      "           6       0.86      0.75      0.80        72\n",
      "           7       0.88      0.90      0.89       144\n",
      "           8       0.84      0.93      0.88       144\n",
      "\n",
      "    accuracy                           0.87      1136\n",
      "   macro avg       0.87      0.86      0.87      1136\n",
      "weighted avg       0.87      0.87      0.87      1136\n",
      "\n",
      "Total time taken: 418.1586995124817 seconds\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.20, random_state=42,stratify=y)\n",
    "\n",
    "# Initialize CatBoost classifier\n",
    "catboost_model = CatBoostClassifier(\n",
    "    iterations=500,  ## less iternations\n",
    "    learning_rate=0.3, ## small learning rate\n",
    "    depth=6,\n",
    "    l2_leaf_reg=3,\n",
    "    loss_function='MultiClass',  # Specifying the loss function for multi-class classification\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "catboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = catboost_model.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"CatBoost Classification Report:\")\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "print(f\"Total time taken: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bf4e06",
   "metadata": {},
   "source": [
    "<p><strong>Observation:</strong></p>\n",
    "<p>We can now confidently conclude that the <strong>CatBoost</strong> model with the following hyperparameters:</p>\n",
    "<ul>\n",
    "    <li>Iterations: 1000</li>\n",
    "    <li>Learning Rate: 0.1</li>\n",
    "    <li>Depth: 6</li>\n",
    "    <li>L2 Leaf Regularization: 3</li>\n",
    "</ul>\n",
    "<p>This configuration consistently delivers the best performance, achieving an impressive F1 Score of 89% on <strong>CatBoost</strong>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3adadd5",
   "metadata": {},
   "source": [
    "### Impact of Additional Training Data with PCA 80%\n",
    "\n",
    "<p>In this phase, we have opted to augment the training dataset by increasing its size. The dataset was expanded from 6,700 instances to 17,000 instances using the augmentation techniques described at the beginning of this report. However, we maintained the use of PCA with an 80% variance retention.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d73fec8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.96      0.95      0.96       396\n",
      "           2       0.93      0.87      0.90       252\n",
      "           3       0.88      0.90      0.89       402\n",
      "           4       0.87      0.87      0.87       422\n",
      "           5       0.96      0.94      0.95       456\n",
      "           6       0.90      0.95      0.92       429\n",
      "           7       0.93      0.90      0.92       291\n",
      "           8       0.94      0.96      0.95       456\n",
      "           9       0.96      0.96      0.96       417\n",
      "\n",
      "    accuracy                           0.93      3521\n",
      "   macro avg       0.93      0.92      0.92      3521\n",
      "weighted avg       0.93      0.93      0.93      3521\n",
      "\n",
      "Total time taken: 784.1964163780212 seconds\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.20, random_state=42,stratify=y)\n",
    "\n",
    "# Initialize CatBoost classifier\n",
    "catboost_model = CatBoostClassifier(\n",
    "    iterations=1000, \n",
    "    learning_rate=0.1, \n",
    "    depth=6,\n",
    "    l2_leaf_reg=3,\n",
    "    loss_function='MultiClass',  # Specifying the loss function for multi-class classification\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "catboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = catboost_model.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"CatBoost Classification Report:\")\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "print(f\"Total time taken: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c659b9e8",
   "metadata": {},
   "source": [
    "<p><strong>Learning Curve:</strong></p>\n",
    "With the addition of more training data, out model's output showed a surprising increase in performance, particularly in precision, which reached an impressive 93%. However, this significant improvement raised concerns about potential overfitting. To address this uncertainty, one effective approach is to examine the learning curve\n",
    "The learning curve typically exhibits three common scenarios:\n",
    "\n",
    "<strong>Ideal Fit:</strong> In an ideal situation, the training error is low, indicating that the model learns well from the data, and the validation error is also low but converging close to the training error. This indicates that the model is generalizing well without overfitting. Adding more data may or may not significantly impact performance.\n",
    "\n",
    "<strong>Overfitting:</strong> If the training error is very low, but the validation error remains high and doesn't converge with the training error, it indicates overfitting. In this scenario, the model is memorizing the training data but failing to generalize to unseen data. Adding more data might help reduce overfitting.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f427d9",
   "metadata": {},
   "source": [
    "### Learning curve: CatBoost with PCA 80%: Total Dataset 17,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ebedb257",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.20, random_state=42, stratify=y)\n",
    "\n",
    "# We'll store the train and test accuracies in these lists\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "training_sizes = []\n",
    "\n",
    "# Define the proportions of training data to use\n",
    "training_proportions = [0.2, 0.3, 0.5, 0.7, 1.0]\n",
    "\n",
    "for proportion in training_proportions:\n",
    "    # Calculate the number of training samples to use\n",
    "    num_samples = int(proportion * len(X_train))\n",
    "    training_sizes.append(num_samples)\n",
    "    \n",
    "    # Take the first 'num_samples' samples from the training data\n",
    "    X_train_subset = X_train[:num_samples]\n",
    "    y_train_subset = y_train[:num_samples]\n",
    "    \n",
    "    # Initialize CatBoost classifier with the same parameters as the original model\n",
    "    model = CatBoostClassifier(\n",
    "        iterations=1000,\n",
    "        learning_rate=0.1,\n",
    "        depth=6,\n",
    "        l2_leaf_reg=3,  # L2 regularization coefficient\n",
    "        loss_function='MultiClass',  # Loss function for multi-class classification\n",
    "        verbose=0  # Turn off output\n",
    "    )\n",
    "    \n",
    "    # Train the model on the subset of the training data\n",
    "    model.fit(X_train_subset, y_train_subset, eval_set=(X_test, y_test), verbose=False)\n",
    "    \n",
    "    # Make predictions and calculate accuracy for the train subset\n",
    "    train_predictions = model.predict(X_train_subset)\n",
    "    train_accuracy = accuracy_score(y_train_subset, train_predictions)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    # Make predictions and calculate accuracy for the test set\n",
    "    test_predictions = model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "# Plotting the learning curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(training_sizes, train_accuracies, label='Training Accuracy')\n",
    "plt.plot(training_sizes, test_accuracies, label='Test Accuracy')\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Size of Training Data')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7d7488",
   "metadata": {},
   "source": [
    "<p><strong>Observations</strong></p>\n",
    "\n",
    "<p>• <strong>Training accuracy</strong> is consistently at 100%, suggesting that the model has learned to perfectly classify all the training data.</p>\n",
    "\n",
    "<p>• The <strong>testing accuracy</strong> rising gradually to 93% is a good sign, indicating that the model is improving its ability to generalize to new data. However, as training accuracy is at 100% and does not converge with the testing accuracy, this discrepancy can still be indicative of overfitting.</p>\n",
    "\n",
    "<p>• In a <strong>well-fitting model</strong>, we would expect to see the training accuracy slightly above the testing accuracy, but both should be converging to a point. The fact that the training accuracy is perfect and the testing accuracy is lower (but still high) suggests that the model may be too complex and fitting the training data too closely.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf45411",
   "metadata": {},
   "source": [
    "### To simplify the model, we employed Truncated SVD dimension reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3b5a00",
   "metadata": {},
   "source": [
    "#### <u>CatBoost with Truancated SVD_version1</u> high learning rate=0.1 with iterations=1000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e51b1292",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.97      0.96      0.97       396\n",
      "           2       0.94      0.88      0.91       252\n",
      "           3       0.89      0.89      0.89       402\n",
      "           4       0.87      0.88      0.88       422\n",
      "           5       0.97      0.96      0.96       456\n",
      "           6       0.91      0.96      0.94       429\n",
      "           7       0.94      0.92      0.93       291\n",
      "           8       0.96      0.96      0.96       456\n",
      "           9       0.97      0.96      0.96       417\n",
      "\n",
      "    accuracy                           0.93      3521\n",
      "   macro avg       0.93      0.93      0.93      3521\n",
      "weighted avg       0.93      0.93      0.93      3521\n",
      "\n",
      "CatBoost model saved as Train_Test_catBosst_1000.pkl\n",
      "Total time taken: 787.2758128643036 seconds\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.20, random_state=42, stratify=y)\n",
    "\n",
    "# Initialize CatBoost classifier\n",
    "catboost_model = CatBoostClassifier(\n",
    "    iterations=1000, \n",
    "    learning_rate=0.1, \n",
    "    depth=6,\n",
    "    l2_leaf_reg=3,\n",
    "    loss_function='MultiClass',  # Specifying the loss function for multi-class classification\n",
    "    early_stopping_rounds=50, # if the model's performance on the validation dataset doesn't get better for a 50 number \n",
    "                              # of rounds, training is stopped early to prevent overfitting.\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "catboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = catboost_model.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "classification_rep = classification_report(y_test, predictions)\n",
    "print(\"CatBoost Classification Report:\")\n",
    "print(classification_rep)\n",
    "\n",
    "# Save the trained CatBoost model to a file\n",
    "model_filename = 'Train_Test_catBosst_1000.pkl'\n",
    "with open(model_filename, 'wb') as model_file:\n",
    "    pickle.dump(catboost_model, model_file)\n",
    "\n",
    "print(f\"CatBoost model saved as {model_filename}\")\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "print(f\"Total time taken: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f763a89c",
   "metadata": {},
   "source": [
    "<p><strong>Observations:</strong> Despite implementing Truncated SVD for dimension reduction, the accuracy of the model remains at 93%. This consistent high accuracy raises concerns about potential overfitting, as the model may still be fitting the training data too closely. Furthermore, it's noteworthy that when reducing dimensions on a dataset containing 17,000 instances, PCA with 80% variance retention required 3,000 seconds for processing, whereas Truncated SVD completed the task in only 800 seconds, demonstrating its computational efficiency.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd4dcea",
   "metadata": {},
   "source": [
    "<p><strong>We will now reduce the number of iterations.</strong> . By doing so, we are effectively constraining the complexity of the final ensemble model. The model will halt the boosting process after 500 iterations, leading to a potentially simpler model as compared to when it was allowed to run for 1000 iterations.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f2df35",
   "metadata": {},
   "source": [
    "#### <u>CatBoost TruncatedSVD_version2 </u> with high learning rate=0.1, iteration=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0980ccd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.96      0.95      0.96       396\n",
      "           2       0.90      0.86      0.88       252\n",
      "           3       0.87      0.87      0.87       402\n",
      "           4       0.82      0.81      0.81       422\n",
      "           5       0.96      0.93      0.94       456\n",
      "           6       0.86      0.92      0.89       429\n",
      "           7       0.89      0.89      0.89       291\n",
      "           8       0.92      0.93      0.92       456\n",
      "           9       0.94      0.94      0.94       417\n",
      "\n",
      "    accuracy                           0.90      3521\n",
      "   macro avg       0.90      0.90      0.90      3521\n",
      "weighted avg       0.90      0.90      0.90      3521\n",
      "\n",
      "CatBoost model saved as Train_Test_catBosst_500.pkl\n",
      "Total time taken: 299.20911169052124 seconds\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.20, random_state=42, stratify=y)\n",
    "\n",
    "# Initialize CatBoost classifier\n",
    "catboost_model = CatBoostClassifier(\n",
    "    iterations=500, \n",
    "    learning_rate=0.1, \n",
    "    depth=6,\n",
    "    l2_leaf_reg=3,\n",
    "    loss_function='MultiClass',  # Specifying the loss function for multi-class classification\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "catboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = catboost_model.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "classification_rep = classification_report(y_test, predictions)\n",
    "print(\"CatBoost Classification Report:\")\n",
    "print(classification_rep)\n",
    "\n",
    "# Save the trained CatBoost model to a file\n",
    "model_filename = 'Train_Test_catBosst_500.pkl'\n",
    "with open(model_filename, 'wb') as model_file:\n",
    "    pickle.dump(catboost_model, model_file)\n",
    "\n",
    "print(f\"CatBoost model saved as {model_filename}\")\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "print(f\"Total time taken: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b29f93",
   "metadata": {},
   "source": [
    "<p><strong>Observations:</strong> Although accuracy dropped from 93% to 90%, it indicates that the model's complexity has been simplified. We will further investigate the impact of this change on the model's performance by examining the learning curve.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1f09fe",
   "metadata": {},
   "source": [
    "<p>To further simplify the model, we conducted an experiment in which we increased the <strong>learning rate to 0.5 while maintaining the number of iterations at 500</strong>- Slower convergence: with a smaller learning rate, the model may require more iterations to converge to a solution.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903697fa",
   "metadata": {},
   "source": [
    "#### <u>CatBoost with Truancated SVD_version3</u> with learning rate =0.05 and iterations =500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "67ab1572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.86      0.83       396\n",
      "           2       0.78      0.77      0.78       252\n",
      "           3       0.71      0.72      0.71       402\n",
      "           4       0.64      0.60      0.62       422\n",
      "           5       0.85      0.82      0.83       456\n",
      "           6       0.74      0.71      0.73       429\n",
      "           7       0.75      0.79      0.77       291\n",
      "           8       0.72      0.76      0.74       456\n",
      "           9       0.87      0.85      0.86       417\n",
      "\n",
      "    accuracy                           0.76      3521\n",
      "   macro avg       0.76      0.76      0.76      3521\n",
      "weighted avg       0.76      0.76      0.76      3521\n",
      "\n",
      "CatBoost model saved as Train_Test_catBoost_500.pkl\n",
      "Total time taken: 119.832839012146 seconds\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.20, random_state=42, stratify=y)\n",
    "\n",
    "# Initialize CatBoost classifier with 500 iterations\n",
    "catboost_model_500 = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    learning_rate=0.05,\n",
    "    depth=4,\n",
    "    l2_leaf_reg=5,\n",
    "    loss_function='MultiClass',\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "catboost_model_500.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = catboost_model_500.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "classification_rep = classification_report(y_test, predictions)\n",
    "print(\"CatBoost Classification Report:\")\n",
    "print(classification_rep)\n",
    "\n",
    "# Save the trained CatBoost model to a file\n",
    "model_filename = 'Train_Test_catBoost_500.pkl'\n",
    "with open(model_filename, 'wb') as model_file:\n",
    "    pickle.dump(catboost_model_500, model_file)\n",
    "\n",
    "print(f\"CatBoost model saved as {model_filename}\")\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "print(f\"Total time taken: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10ebbda",
   "metadata": {},
   "source": [
    "<p><strong>Observation:</strong> Decreasing the learning rate resulted in a drop in accuracy, but it contributed to a simpler model that helps mitigate overfitting. This inference may be supported by the patterns observed in the learning curve, which we will examine later.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6d7a97",
   "metadata": {},
   "source": [
    "### Display Learning Curve: XBoost and CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14396295",
   "metadata": {},
   "source": [
    "#### Learning curve for XGBoost TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3fd58dd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.20, random_state=42, stratify=y)\n",
    "\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "training_sizes = []\n",
    "\n",
    "training_proportions = np.arange(0.1, 1.1, 0.1)  # From 10% to 100% in 10% increments\n",
    "\n",
    "for proportion in training_proportions:\n",
    "    num_samples = int(proportion * len(X_train))\n",
    "    training_sizes.append(num_samples)\n",
    "    \n",
    "    X_train_subset = X_train[:num_samples]\n",
    "    y_train_subset = y_train[:num_samples]\n",
    "    \n",
    "    xgb_best_model = XGBClassifier(\n",
    "        subsample=1, \n",
    "        n_estimators=110, \n",
    "        max_depth=4, \n",
    "        learning_rate=0.3, \n",
    "        colsample_bytree=1,  \n",
    "        use_label_encoder=False,\n",
    "        eval_metric='mlogloss'\n",
    "    )\n",
    "    \n",
    "        \n",
    "    try:\n",
    "        xgb_best_model.fit(X_train_subset, y_train_subset, eval_set=[(X_test, y_test)], verbose=False)\n",
    "    \n",
    "        train_predictions = xgb_best_model.predict(X_train_subset)\n",
    "        train_accuracy = accuracy_score(y_train_subset, train_predictions)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "    \n",
    "        test_predictions = xgb_best_model.predict(X_test)\n",
    "        test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "    except Exception as e:\n",
    "        print(f\"Error at proportion {proportion}: {str(e)}\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(training_sizes, train_accuracies, label='Training Accuracy')\n",
    "plt.plot(training_sizes, test_accuracies, label='Test Accuracy')\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Size of Training Data')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.yticks(np.arange(0.5, 1.05, 0.05))  # Setting y-ticks from 50% to 100% with 5% increments\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba64582",
   "metadata": {},
   "source": [
    "<p><strong>Observations:</strong> The testing accuracy rising gradually to 89% is a good sign, indicating that the model is improving its ability to generalize to new data. However, as training accuracy is at 100% and does not converge with the testing accuracy, this discrepancy can still be indicative of overfitting.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066cd383",
   "metadata": {},
   "source": [
    "#### Learning curve for <u>CatBoost TruncatedSVD_version3 with low learning rate=0.05, iteration=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fd222d4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.20, random_state=42, stratify=y)\n",
    "\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "training_sizes = []\n",
    "\n",
    "training_proportions = np.arange(0.1, 1.1, 0.1)  # From 10% to 100% in 10% increments\n",
    "\n",
    "for proportion in training_proportions:\n",
    "    num_samples = int(proportion * len(X_train))\n",
    "    training_sizes.append(num_samples)\n",
    "    \n",
    "    X_train_subset = X_train[:num_samples]\n",
    "    y_train_subset = y_train[:num_samples]\n",
    "    \n",
    "    model = CatBoostClassifier(\n",
    "        iterations=500,\n",
    "        learning_rate=0.05,\n",
    "        depth=4,\n",
    "        l2_leaf_reg=5,\n",
    "        loss_function='MultiClass',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train_subset, y_train_subset, eval_set=(X_test, y_test), verbose=False)\n",
    "    \n",
    "    train_predictions = model.predict(X_train_subset)\n",
    "    train_accuracy = accuracy_score(y_train_subset, train_predictions)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    test_predictions = model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(training_sizes, train_accuracies, label='Training Accuracy')\n",
    "plt.plot(training_sizes, test_accuracies, label='Test Accuracy')\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Size of Training Data')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.yticks(np.arange(0.5, 1.05, 0.05))  # Setting y-ticks from 50% to 100% with 5% increments\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f7e435",
   "metadata": {},
   "source": [
    "<p><strong>Observations:</strong> Although it exhibits characteristics of a <strong>good fit model</strong>, where we observe the training accuracy slightly above the testing accuracy, converging as the training set size increases, we cannot consider it satisfactory due to the relatively low accuracy.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decfc54d",
   "metadata": {},
   "source": [
    "#### Learning curve with TruncatedSVD with <u>CatBoost with TruncatedSVD_v2</u> with high learning rate=0.1, iteration=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9975c9e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.20, random_state=42, stratify=y)\n",
    "\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "training_sizes = []\n",
    "\n",
    "training_proportions = np.arange(0.1, 1.1, 0.1)  # From 10% to 100% in 10% increments\n",
    "\n",
    "for proportion in training_proportions:\n",
    "    num_samples = int(proportion * len(X_train))\n",
    "    training_sizes.append(num_samples)\n",
    "    \n",
    "    X_train_subset = X_train[:num_samples]\n",
    "    y_train_subset = y_train[:num_samples]\n",
    "    \n",
    "    model = CatBoostClassifier(\n",
    "        iterations=500, \n",
    "        learning_rate=0.1, \n",
    "        depth=6,\n",
    "        l2_leaf_reg=3,\n",
    "        loss_function='MultiClass',  # Specifying the loss function for multi-class classification\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train_subset, y_train_subset, eval_set=(X_test, y_test), verbose=False)\n",
    "    \n",
    "    train_predictions = model.predict(X_train_subset)\n",
    "    train_accuracy = accuracy_score(y_train_subset, train_predictions)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    test_predictions = model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(training_sizes, train_accuracies, label='Training Accuracy')\n",
    "plt.plot(training_sizes, test_accuracies, label='Test Accuracy')\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Size of Training Data')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.yticks(np.arange(0.5, 1.05, 0.05))  # Setting y-ticks from 50% to 100% with 5% increments\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940006a0",
   "metadata": {},
   "source": [
    "<p><strong>Observations:</strong></p>\n",
    "<p>The learning curve reveals insightful trends about the model's performance as more data is introduced during training. Initially, the model displays a perfect training accuracy of 100%. This could be indicative of the model memorizing the training data, a phenomenon commonly known as overfitting.</p>\n",
    "<p>As the size of the training data increases, the training accuracy begins to decline slightly, falling to 99% at 10,000 data points and further dipping to 98% with 17,000 data points. This gradual decrease in training accuracy is actually a <strong>positive sign</strong>. It suggests that the model is starting to generalize better rather than just memorizing the training data. The slight reduction in training accuracy implies that the model is learning to accommodate a wider variety of data patterns, which is <strong>desirable in a well-generalizing model</strong>.</p>\n",
    "<p>Conversely, the testing accuracy, which reflects the model's ability to apply what it has learned to new, unseen data, increases to 91% at 17,000 data points. This increase in testing accuracy as more data is provided suggests that the model is benefiting from the additional information and becoming more adept at predicting unseen samples.</p>\n",
    "<p>In a well-functioning learning curve, we expect to see the training accuracy decrease slightly and the testing accuracy increase as more data is added. Ideally, both curves should converge to a point where increasing the amount of training data does not significantly change the accuracy. This model seems to be following this trend, which <strong>indicates a good fit</strong>: the model is neither overfitting (where the training accuracy is unrealistically high compared to the testing accuracy) nor underfitting (where the model fails to learn from the training data and performs poorly on both training and testing data).</p>\n",
    "<p>However, the fact that there is still a noticeable gap between training and testing accuracy at 17,000 data points suggests that there might be room for further improvement. We may consider experimenting by adding more data if possible.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba38b307",
   "metadata": {},
   "source": [
    "#### Learning curve with TruncatedSVD with <u>CatBoost with Truancated SVD_version1</u> with high learning rate=0.1, iteration=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5d200b44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmcAAAFNCAYAAABFbcjcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABC2UlEQVR4nO3deXxV9Z3/8deHJCwhLLIFBRRQBEEISoqKtgSpSquIVlRwqUuV6lRt9WfrNh1rO3Yc60xHq5WhDlotFRVFsOIuAeuCgKLsghAlILssYU/y+f1xTsglJHDh5ubcJO/n45FH7jnn+z33cz4qfPx+z/kec3dEREREJDU0iDoAERERESmn4kxEREQkhag4ExEREUkhKs5EREREUoiKMxEREZEUouJMREREJIWoOBOResfMvmtmi6OOQ0SkMirORKRGmVmBmX0/yhjc/T13756s85vZOWY23cy2mtk6M5tmZucn6/tEpG5RcSYidY6ZpUX43cOBF4CngY5ANvBvwNDDOJeZmf6cFqln9B+9iKQEM2tgZnea2ZdmtsHMnjezVjHHXzCz1Wa2ORyV6hVz7Ckze9zMppjZNmBQOEJ3u5l9HvZ5zswah+3zzKwwpn+VbcPjvzKzb8xslZldZ2ZuZsdVcg0G/DfwO3d/wt03u3upu09z9+vDNr8xs7/F9Okcni893M43s/vN7H1gO3C3mc2q8D23mtnk8HMjM3vIzL42szVmNtrMmiT4j0NEIqTiTERSxS3ABcBA4CjgW+CxmOOvAd2AdsAnwLgK/S8D7geaAf8M910CDAG6AH2Aqw/w/ZW2NbMhwG3A94Hjwviq0h3oBEw4QJt4XAmMIriWPwHdzaxbzPHLgL+Hn/8TOB7oG8bXgWCkTkRqKRVnIpIqfgrc4+6F7r4L+A0wvGxEyd3HuvvWmGM5ZtYipv8kd38/HKnaGe57xN1XuftG4BWCAqYqVbW9BHjS3ee7+3bgvgOco3X4+5s4r7kqT4XfV+zum4FJwEiAsEjrAUwOR+quB251943uvhX4PTAiwe8XkQipOBORVHEMMNHMNpnZJmAhUAJkm1mamT0QTnluAQrCPm1i+q+o5JyrYz5vB7IO8P1VtT2qwrkr+54yG8LfRx6gTTwqfsffCYszglGzl8NCsS2QCcyOydvr4X4RqaVUnIlIqlgB/MDdW8b8NHb3lQQFyTCCqcUWQOewj8X09yTF9Q3Bjf1lOh2g7WKC67joAG22ERRUZdpX0qbitbwJtDGzvgRFWtmU5npgB9ArJmct3P1ARaiIpDgVZyIShQwzaxzzkw6MBu43s2MAzKytmQ0L2zcDdhGMTGUSTN3VlOeBa8zsBDPL5AD3c7m7E9yf9mszu8bMmocPOpxhZmPCZnOA75nZ0eG07F0HC8DdiwnuY/sD0Ap4K9xfCvwF+KOZtQMwsw5mds7hXqyIRE/FmYhEYQrBiE/Zz2+Ah4HJwJtmthX4CDglbP808BWwElgQHqsR7v4a8AgwFVgKfBge2lVF+wnApcC1wCpgDfDvBPeN4e5vAc8BnwOzgX/EGcrfCUYOXwiLtTJ3hHF9FE75vk3wYIKI1FIW/I+eiIjEw8xOAOYBjSoUSSIi1UIjZyIiB2FmF5pZQzM7gmDpildUmIlIsqg4ExE5uJ8C64AvCZ4gvTHacESkLtO0poiIiEgK0ciZiIiISApRcSYiIiKSQtKjDqA6tWnTxjt37hx1GAnZtm0bTZs2jTqMWkv5S4zylxjlLzHKX2KUv8REkb/Zs2evd/f93uhRp4qzzp07M2vWrKjDSEh+fj55eXlRh1FrKX+JUf4So/wlRvlLjPKXmCjyZ2ZfVbZf05oiIiIiKUTFmYiIiEgKUXEmIiIikkJUnImIiIikkKQVZ2Y21szWmtm8Ko6bmT1iZkvN7HMzOznm2BAzWxweuzNZMYqIiIikmmSOnD0FDDnA8R8A3cKfUcDjAGaWBjwWHu8JjDSznkmMU0RERCRlJK04c/fpwMYDNBkGPO2Bj4CWZnYk0B9Y6u7L3H03MD5sKyIiIlLnRbnOWQdgRcx2Ybivsv2n1GBcVbrvlfksWLUlqd+xadMOHl/8YVK/oy5T/hKj/CVG+UuM8pcY5S8xsfnreVRz7h3aK7JYoizOrJJ9foD9lZ/EbBTBtCjZ2dnk5+dXS3CVKSzcxaYtpUk7P0BJSQmbNm1K6nfUZcpfYpS/xCh/iVH+EqP8JSY2f4WlW8jPXxdZLFEWZ4VAp5jtjsAqoGEV+yvl7mOAMQC5ubmezNV9a2LhYK3wnBjlLzHKX2KUv8Qof4lR/hKTSvmLcimNycCPw6c2TwU2u/s3wEygm5l1MbOGwIiwrYiIiEidl7SRMzN7FsgD2phZIXAvkAHg7qOBKcAPgaXAduCa8Fixmd0EvAGkAWPdfX6y4hQRERFJJUkrztx95EGOO/CzKo5NISjeREREROoVvSFAREREJIWoOBMRERFJISrORERERFKIijMRERGRFKLiTERERCSFqDgTERERSSEqzkRERERSiIozERERkRSi4kxEREQkhag4ExEREUkhKs5EREREUoiKMxEREZEUouJMREREJIWoOBMRERFJISrORERERFKIijMRERGRFKLiTERERCSFJLU4M7MhZrbYzJaa2Z2VHD/CzCaa2edm9rGZnRhzrMDM5prZHDOblcw4RURERFJFerJObGZpwGPAWUAhMNPMJrv7gphmdwNz3P1CM+sRth8cc3yQu69PVowiIiIiqSaZI2f9gaXuvszddwPjgWEV2vQE3gFw90VAZzPLTmJMIiIiIiktmcVZB2BFzHZhuC/WZ8CPAMysP3AM0DE85sCbZjbbzEYlMU4RERGRlGHunpwTm10MnOPu14XbVwL93f3mmDbNgYeBk4C5QA/gOnf/zMyOcvdVZtYOeAu42d2nV/I9o4BRANnZ2f3Gjx+flOupKUVFRWRlZUUdRq2l/CVG+UuM8pcY5S8xyl9iosjfoEGDZrt7bsX9SbvnjGCkrFPMdkdgVWwDd98CXANgZgYsD39w91Xh77VmNpFgmnS/4szdxwBjAHJzcz0vL6+6r6NG5efnU9uvIUrKX2KUv8Qof4lR/hKj/CUmlfKXzGnNmUA3M+tiZg2BEcDk2AZm1jI8BnAdMN3dt5hZUzNrFrZpCpwNzEtirCIiIiIpIWkjZ+5ebGY3AW8AacBYd59vZjeEx0cDJwBPm1kJsAD4Sdg9G5gYDKaRDvzd3V9PVqwiIiIiqSKZ05q4+xRgSoV9o2M+fwh0q6TfMiAnmbGJiIiIpCK9IUBEREQkhag4ExEREUkhSZ3WFBEREUlZ7rBrK2xbR4tNC2DBZihaC3u2w+k/jywsFWciIiJSd7jDjm9h27qg0Nq2ForWBb+3rSv/XPa7eCcQLLjKnPAcaY1gwC0QPJhY41SciYiISGorLYUdG/cvtorWVlKErYPSPfufw9KgaRto2g6y2kLrbsHvpu0gqx2fffkNOQO+H2xnto6sMAMVZyIiIhKFkmLYvr68wKpspGvviNd68JL9z9EgA7LaQdO2kJUN2b33Kbho2jY81g6atIIGVd9q/+23+dC+d/Ku9xCoOBMREZHqUbw7LLQONrq1FrZvJHiNdgXpjctHt1p0hA4n7VtsZbUrP964ZaQjXMmi4kxEREQObPd22LISNq+AzSuhaPX+o1tFa2Hnpsr7ZzQtH9FqfSwcfWolxVa43ahZnSy4DoWKMxERkfqstASK1sDmwrD4KgwKsLLtLSth+4b9+zVqUV5wtTsBugysfHSraVto2LTmr6sWU3EmIiJSV7nDzs3hqFdlxVchbF0FpcX79mvUPJhSbNEROuZC8w7QolO4rwNktYeMxtFcUz2g4kxERKS2Kt69t/DKXj0Vps8sL7rKCrDdW/ft0yA9LLY6wjGnlRdhzTuWF1+NW0RzPQKoOBMREUlN7sFTimWjXZWNfhWtoeym+hMAFgGZbYIiq/Wx0HXg/sVXVjtokBbhhcnBqDgTERGJwu5t4ejWivLRri0r9y2+Snbt2ye9SXmx1a1n+ecWHZmxaBWnnHUhZDSJ5nqk2qg4ExERqW4lxcETjQcqvnZ8u28fawDNjgymHI/sCz3OC+/zCqcgW3SCJkdU+STjjq/zVZjVESrORESkbnMPbngv3gnFuyr8jvm8Z2cVbSr7vaPqc+3eBltX779oauMWQYHVvAN07F9ecJUVX82OhLSMaHIkKUXFmYiI1JySPcF9VJUUPa02zIIFW/YvmqoqkPZUViBVUUh5aWJxN8gIFkdNb1T+O6NJ+XajZsGSEemNICMzKLT2Fl/hTfaNmlVPDqXOU3EmIiLVb/d22LAE1i0OftYvhnVfwMYv91+2IdQHYG5VJ7R9i6HKfjduHn6u7HjM54yq2sT+rvBduoFealBSizMzGwI8DKQBT7j7AxWOHwGMBY4FdgLXuvu8ePqKiEgK2PFtUHStWwTrvygvxDatYO+reSwNWnWBNt2hx7nBSNJ+hVYTZn8+n379B1ReMKVl1PtV46X+SFpxZmZpwGPAWUAhMNPMJrv7gphmdwNz3P1CM+sRth8cZ18REakJ7sE9VGWjX7GF2La15e3SG0PrbtDxO3DSldDmeGjbHVp1DYqsg9hasAfan5jECxGpHZI5ctYfWOruywDMbDwwDIgtsHoC/wHg7ovMrLOZZQNd4+grIiLVqbQENn0VFGD7FGJLYNfm8naNWkDb46Hb2cHvtj2CQqzl0Zr+E6kGySzOOgArYrYLgVMqtPkM+BHwTzPrDxwDdIyzr4iIHI7iXbDhy/ICrOz3hiXBTfRlsrKDoqvPxcGUZNvwJytbU4wiSZTM4qyy/3K9wvYDwMNmNofgNtBPgeI4+wZfYjYKGAWQnZ1Nfn7+YYabGoqKimr9NURJ+UuM8peYVMtfWvF2MrevJHP7CjK3F9J0WyGZ21fQZMdqjODpRcfY2bgd2zM7sr39ELY17Rh8zuxEcUZW+cl2AF87fL2IYBn66pdq+attlL/EpFL+klmcFQKdYrY7AqtiG7j7FuAaADMzYHn4k3mwvjHnGAOMAcjNzfW8vLzqiT4i+fn51PZriJLylxjlLzGR5W/bhnD0a9G+I2FbCsvbNEiH1sdB59xg9KtNd2h7PNa6G00aZtIEaF3zke9D//4lRvlLTCrlL5nF2Uygm5l1AVYCI4DLYhuYWUtgu7vvBq4Dprv7FjM7aF8RkXrFPVhdft3i8Gb8mEJs+4bydhmZwVRk59PLb8hv0z14WlILnIrUCkkrzty92MxuAt4gWA5jrLvPN7MbwuOjCd7T+rSZlRDc7P+TA/VNVqwiIimjpBi+LQhHv2IKsfVLYHdRebsmR4RLU5y3z0gYzTtCgwaRhS8iiUvqOmfuPgWYUmHf6JjPHwLd4u0rIlKn7NwCa+bD6rmw+vPg99qF+77sutlRQdHV9/LyG/LbdIembXRTvkgdpTcEiIgkmztsWRUWYTGF2LfLy9tktob2vaH/9dCuZ7g8Rbdg1XsRqVdUnImIVKeSPcEUZGwRtnou7NhY3qZVVziyD5x0ObTvExRlzY7USJiIACrOREQOX4VpyX5LPoD3CsunJdMbB6NgJwwNCrD2vSG7l16ALSIHpOJMRORg4pyW3NOoE/T5afloWOvjIE1/zIrIodGfGiIiseKaljwWjsyBk66ImZZsz+fTpqXMOkkiUnupOBOR+utgT0vuNy3ZB7J7alpSRJJKxZmI1H1xPy3ZB07RtKSIREt/6ohI3ZLAtKSelhSRVKDiTERqL01LikgdpOJMRFJfyR7YuCx4ndG6ReWjYZqWFJE6SH9qiUjq2L0dNiwJi7DF4fslv4CNX0JpcXk7TUuKSB2m4kxEat6Ob4Oiq+zl3mWF2KYVgAdtLC1YSb9td+hxbvl7JVt3g0ZZkYYvIpJMKs5EJDncoWhNMA1ZsRDbtra8XXrjoODq2B9OuhLaHB8UYa2OhfSG0cUvIhIRFWcikpjSUtj0Faz/Yv/pyF2by9s1agFtj4duZwe/2/YICrGWR0ODtOjiFxFJMSrORCQ+xbuDe7/WLd63ENuwBIp3lrdr2i4Y+epzMbTpXl6IZWXrnjARkTioOBORfe3eVmEULPy8cRl4SXm7lkcHxVfXgUExVlaINTkiuthFROoAFWci9dX2jftOQa5bxKmFn0P+uvI2DdKDe7/a9YCew/a9Kb9hZnSxi4jUYSrOROoyd9j6zf73gq1fDNtiirD0JtCmG5tbnEDjE38ajoJ1D56WTMuILn4RkXooqcWZmQ0BHgbSgCfc/YEKx1sAfwOODmN5yN2fDI8VAFuBEqDY3XOTGatIrVdaCitmBD/rg5Ew1i+BXVvK2zRuEdz/dfyQmKnI7tCiEzRowML8fLK/lxfZJYiISBKLMzNLAx4DzgIKgZlmNtndF8Q0+xmwwN2HmllbYLGZjXP33eHxQe6+PlkxitR67vDNZzBvAsybCFsKg/1Z7YP7v3JGlC9N0aY7ZLXTTfkiIikumSNn/YGl7r4MwMzGA8OA2OLMgWZmZkAWsBEorngiEalg/dKgIJs7IXhaskE6HDsYvn8vdDtLN+WLiNRi5u7JObHZcGCIu18Xbl8JnOLuN8W0aQZMBnoAzYBL3f3V8Nhy4FuCAu5/3X1MFd8zChgFkJ2d3W/8+PFJuZ6aUlRURFaWVj8/XHU5f412rqPd2n/Sbu17NCv6EsfY1LIXa9t9j3VtT6M4o3nC31GX81cTlL/EKH+JUf4SE0X+Bg0aNLuy27aSOXJW2dxJxUrwHGAOcCZwLPCWmb3n7luA0919lZm1C/cvcvfp+50wKNrGAOTm5npeXl41XkLNy8/Pp7ZfQ5TqXP62bYAFLwcjZF9/EOw76mQ4/fdYrws5ovlRHAF0r6avq3P5q2HKX2KUv8Qof4lJpfwlszgrBDrFbHcEVlVocw3wgAfDd0vD0bIewMfuvgrA3dea2USCadL9ijOROmfXVlj0alCQLZsavPC7TXcYdA+ceBG0PjbqCEVEJImSWZzNBLqZWRdgJTACuKxCm6+BwcB7ZpZNMACwzMyaAg3cfWv4+Wzgt0mMVSRae3bC0rdg7gvwxRvBivstjobTboLewyH7RN3ILyJSTyStOHP3YjO7CXiDYCmNse4+38xuCI+PBn4HPGVmcwmmQe9w9/Vm1hWYGDwnQDrwd3d/PVmxikSipBiWT4N5L8LCV4IlLzLbBC//7n0xdPwONGgQdZQiIlLDkrrOmbtPAaZU2Dc65vMqglGxiv2WATnJjE0kEu6w4uNghGzBy8FCsI2awwlDgynLLgMhTWtDi4jUZ/pbQCTZ3GHNvOAesnkvweavIb1xsBBs7+Fw3FmQ0TjqKEVEJEWoOBNJlg1fBlOWcycEr0uyNDj2TDjzHuhxLjRqFnWEIiKSglSciVSnLd/A/JeCactVnwb7jjkdTvkp9LwAmraONDwREUl9Ks5EErV9IyyYFIySFfwTcDgyB87+d+j1I2jRIeoIRUSkFlFxJnI4dhXB4teCEbIv3wnWImvdDfLuhBOHQ5vjoo5QRERqKRVnIvEq3gVL3w7uIVv8GhTvgOYd4dR/CW7sb99Ha5GJiEjCVJyJHEhpCRS8F4yQLXwFdm6GzNbQ97KgIOt0qtYiExGRaqXiTKQidyicBfMmwPyJULQGGjaDE84Lpiy7DoS0jKijFBGROkrFmUiZNQuCEbJ5L8KmryCtERx/drBaf7ezIaNJ1BGKiEg9oOJM6reNy4NibN6LsHZBsBZZ17zgxv4e50LjFlFHKCIi9YyKM6l/tq4OpivnToCVs4J9R58GP3woWIssq22k4YmISP2m4kzqj68+gGkPBi8b91Jo3xu+f1/wTsuWnaKOTkREBFBxJvXB6rnwzm9hyZuQ1R6+98vgxv62x0cdmYiIyH5UnEndtXEZTP19cJN/4xbBKFn/UdAwM+rIREREqqTiTOqerauD6ctP/goNMuCM2+D0W6DJEVFHJiIiclAqzqTu2LGJLsuegX++CqV74OSrYOCvoFn7qCMTERGJm4ozqf12b4ePx8A//8gxOzcF65INuhtadY06MhERkUOW1PfOmNkQM1tsZkvN7M5Kjrcws1fM7DMzm29m18TbV4SSPTDrSfjTyfD2vdCpP7P6/REuekKFmYiI1FpJGzkzszTgMeAsoBCYaWaT3X1BTLOfAQvcfaiZtQUWm9k4oCSOvlJflZbCgonw7r8HN/13OgUu+j/ofDpF+flRRyciIpKQZE5r9geWuvsyADMbDwwDYgssB5qZmQFZwEagGDgljr5S37jDl+/A2/fB6s+hXU8YOR6OHwJmUUcnIiJSLZJZnHUAVsRsFxIUXbEeBSYDq4BmwKXuXmpm8fSV+mTFTHjnPih4D1oeDReOgd7DoUFa1JGJiIhUq4MWZ2Z2HjDF3UsP8dyVDWV4he1zgDnAmcCxwFtm9l6cfcviGwWMAsjOzia/lk9rFRUV1fprqE6Z276m67K/0WbDDHZntOCr40ax6qiz8W8zYPp7+7VX/hKj/CVG+UuM8pcY5S8xqZS/eEbORgAPm9mLwJPuvjDOcxcCse/E6UgwQhbrGuABd3dgqZktB3rE2RcAdx8DjAHIzc31vLy8OMNLTfn5+dT2a6gWm76Gqf8Bnz0LjZrBmf9Kw1NupFujLLodoJvylxjlLzHKX2KUv8Qof4lJpfwdtDhz9yvMrDkwEnjSzBx4EnjW3bceoOtMoJuZdQFWEhR5l1Vo8zUwGHjPzLKB7sAyYFMcfaUuKloH7/0XzPo/wGDATcEispmtoo5MRESkRsR1z5m7bwlHzpoAvwAuBH5pZo+4+5+q6FNsZjcBbwBpwFh3n29mN4THRwO/A54ys7kEU5l3uPt6gMr6JnCdkup2boEPH4MPH4U92+GkK2DgHdCiY9SRiYiI1Kh47jkbClxLcE/YM0B/d19rZpnAQqDS4gzA3acAUyrsGx3zeRVwdrx9pQ7aszMYJZv+EOzYCD2HwaB/1UvJRUSk3opn5Oxi4I/uPj12p7tvN7NrkxOW1HklxfD5+OC+si2F0HUQDP436HBy1JGJiIhEKp7i7F7gm7INM2sCZLt7gbu/k7TIpG5yh0X/gHd+B+sXw1EnwwWPQde8qCMTERFJCfEUZy8AA2K2S8J930lKRFJ3LZsWrFW2cja0OR4ueQZOGKoFZEVERGLEU5ylu/vusg13321mDZMYk9Q1qz4NVvVfNhWad4Rhj0GfEZCWzDWQRUREaqd4/nZcZ2bnu/tkADMbBqxPblhSJ6xfErz/csHL0KQVnPN7yP0JZDSOOjIREZGUFU9xdgMwzsweJVjuYgXw46RGJbXb5pUw7QH4dBykNw6WxDjtJmjcPOrIREREUl48i9B+CZxqZlmAHWThWanPtm+Ef/43zBgDXgr9r4fv3g5ZbaOOTEREpNaI66YfMzsX6AU0tvDmbXf/bRLjktpkVxHMeBzefwR2bYWckZB3JxxxTNSRiYiI1DrxLEI7GsgEBgFPAMOBj5Mcl9QGxbvhk7/CtAdh21rofi6c+a+Q3TPqyERERGqteEbOBrh7HzP73N3vM7P/Al5KdmCSwkpLYO4EmHo/bPoKjjkDRoyDTv2jjkxERKTWi6c42xn+3m5mRwEbgC7JC0lSljt88Qa881tYOx/a94ErXoRjB2utMhERkWoST3H2ipm1BP4AfAI48JdkBiUp6KsPgrXKVnwErbrC8LHQ80Jo0CDqyEREROqUAxZnZtYAeMfdNwEvmtk/gMbuvrkmgpMUsHpuMFK25E3Iag/n/RFOuhLSMqKOTEREpE46YHHm7qXhPWanhdu7gF01EZhEbOMymPr74N6yxs3h+/dB/1HQMDPqyEREROq0eKY13zSzi4CX3N2THZBEbOtqmP4HmP0UNMiAM26F02+BJkdEHZmIiEi9EE9xdhvQFCg2s50Ebwlwd9dy73XNgkkw8QYo2Q0nXwUDfwXN2kcdlYiISL0SzxsCmtVEIBKxtQuDwqxdT/jRGGh9bNQRiYiI1EvxLEL7vcr2u/v0OPoOAR4G0oAn3P2BCsd/CVweE8sJQFt332hmBcBWoAQodvfcg32fHKadW+C5K6BRs2C9Mo2WiYiIRCaeac1fxnxuDPQHZgNnHqiTmaUBjwFnAYXATDOb7O4Lytq4+x8IlujAzIYCt7r7xpjTDHL39fFciBwmd3j5Rti4HK7+hwozERGRiMUzrTk0dtvMOgEPxnHu/sBSd18W9hsPDAMWVNF+JPBsHOeV6vT+w7DoH3DO7+GYAVFHIyIiUu8dzgqihcCJcbTrAKyo0K9DZQ3NLBMYArwYs9sJnhSdbWajDiNOOZhl0+Cd+6DXhXDqv0QdjYiIiBDfPWd/IiiUICjm+gKfxXHuyt7nU9VSHEOB9ytMaZ7u7qvMrB3wlpktquw+t7BwGwWQnZ1Nfn5+HKGlrqKiohq5hkY719Nv9m3saXIUnxxxCSXTpiX9O2tCTeWvrlL+EqP8JUb5S4zyl5hUyl8895zNivlcDDzr7u/H0a8Q6BSz3RFYVUXbEVSY0nT3VeHvtWY2kWCadL/izN3HAGMAcnNzPS8vL47QUld+fj5Jv4bi3fDUD8FKaHjNRL7b9vjkfl8NqpH81WHKX2KUv8Qof4lR/hKTSvmLpzibAOx09xIIbvQ3s0x3336QfjOBbmbWBVhJUIBdVrGRmbUABgJXxOxrCjRw963h57OB38ZzQRKHN+6Gwplw8V+hDhVmIiIidUE895y9AzSJ2W4CvH2wTu5eDNwEvAEsBJ539/lmdoOZ3RDT9ELgTXffFrMvG/inmX0GfAy86u6vxxGrHMxnz8HMv8CAm6HXBVFHIyIiIhXEM3LW2N2LyjbcvSi8gf+g3H0KMKXCvtEVtp8CnqqwbxmQE893yCFYPQ9e+TkccwYM/k3U0YiIiEgl4hk522ZmJ5dtmFk/YEfyQpKk2LEpWGi2SUu4+ElIi6cuFxERkZoWz9/QvwBeMLOym/mPBC5NWkRS/UpLg1czbV4BV0+BrHZRRyQiIiJViGcR2plm1gPoTrA8xiJ335P0yKT6/PO/4YvX4AcPwtGnRB2NiIiIHMBBpzXN7GdAU3ef5+5zgSwz04qltcWX78LU++HE4dBfa/mKiIikunjuObve3TeVbbj7t8D1SYtIqs+mFTDhJ9C2B5z/CFhl6wKLiIhIKomnOGtgVv63evhC84bJC0mqRfEueP7HUFoMlzwDDZtGHZGIiIjEIZ4HAt4Anjez0QSvX7oBeC2pUUniXrsDVn0Cl/4N2hwXdTQiIiISp3iKszsI3l15I8EDAZ8SPLEpqerTcTD7STj9F3DC0KijERERkUNw0GlNdy8FPgKWAbnAYIIV/yUVffMZvHobdPkenPnrqKMRERGRQ1TlyJmZHU/wPsyRwAbgOQB3H1Qzockh2/EtPHclZLaGi8ZqoVkREZFa6EB/ey8C3gOGuvtSADO7tUaikkNXWgovjYItq+Da1yGrbdQRiYiIyGE40LTmRcBqYKqZ/cXMBhPccyapaPofYMmb8IMHoGNu1NGIiIjIYaqyOHP3ie5+KdADyAduBbLN7HEzO7uG4pN4LHkb8v8D+oyA3J9EHY2IiIgkIJ4HAra5+zh3Pw/oCMwB7kx2YBKnb7+CF38C2b3gvD9qoVkREZFaLp5FaPdy943u/r/ufmayApJDsGcnPH8luMOlz0DDzKgjEhERkQTpcb7abMrtwdIZI8dDq65RRyMiIiLV4JBGziSFzP4rfPoMfPd26P6DqKMRERGRapLU4szMhpjZYjNbamb73admZr80sznhzzwzKzGzVvH0rddWfgJTfgldB8Ggu6OORkRERKpR0oqz8AXpjwE/AHoCI82sZ2wbd/+Du/d1977AXcA0d98YT996a/tGeP4qyGoHF/0fNEiLOiIRERGpRskcOesPLHX3Ze6+GxgPDDtA+5HAs4fZt34oLYEXr4Oi1XDJX6Fp66gjEhERkWqWzOKsA7AiZrsw3LcfM8sEhgAvHmrfeiX/AfjyHfjBg9ChX9TRiIiISBIk82nNyhbc8iraDgXed/eNh9rXzEYBowCys7PJz88/xDBTS1FRUaXX0Hr9THrPe5Bv2g9m8dbOUMuvM1mqyp/ER/lLjPKXGOUvMcpfYlIpf8kszgqBTjHbHYFVVbQdQfmU5iH1dfcxwBiA3Nxcz8vLO8xwU0N+fj77XcPG5TDmx9C+D0f+ZBxHZjSJJLbaoNL8SdyUv8Qof4lR/hKj/CUmlfKXzGnNmUA3M+tiZg0JCrDJFRuZWQtgIDDpUPvWC7u3w3NXAhYsNKvCTEREpE5L2siZuxeb2U3AG0AaMNbd55vZDeHx0WHTC4E33X3bwfomK9aU5Q6v/j9YMw8ufwGO6Bx1RCIiIpJkSX1DgLtPAaZU2De6wvZTwFPx9K13Zj8Jn/0dBt4J3c6KOhoRERGpAXpDQKoqnA2v3QHHnQUD74g6GhEREakhKs5S0bb18PyPoVl7+NEYaKB/TCIiIvWFXnyearwEJlwL29bBT96EzFZRRyQiIiI1SMVZiumy/O/w9TQ4/1E4qm/U4YiIiEgN03xZKln0Ksd8PQFOvgpOvjLqaERERCQCKs5SxYYvYeINbM06Nng9k4iIiNRLKs5Swe5twUKzDdKYd+KdkNE46ohEREQkIrrnLGru8MovYO0CuOJFdhWmRR2RiIiIREgjZ1Gb+QTMfR4G3QPHDY46GhEREYmYirMorfgYXr8Ljh8C3/1/UUcjIiIiKUDFWVSK1gYLzbboABf+rxaaFREREUD3nEWjpDhYaHbHt3Dd29CkZdQRiYiISIpQcRaFd38LBe/BBaOhfe+ooxEREZEUorm0mrZgErz/MOT+BPqOjDoaERERSTEqzmrS+iXw8s+gQz8Y8h9RRyMiIiIpSMVZTdlVBM9dAekN4ZKnIb1R1BGJiIhICtI9ZzXBHSbfDOu/gCsnQouOUUckIiIiKSqpI2dmNsTMFpvZUjO7s4o2eWY2x8zmm9m0mP0FZjY3PDYrmXEm3YzRMP8lOPPX0DUv6mhEREQkhSVt5MzM0oDHgLOAQmCmmU129wUxbVoCfwaGuPvXZtauwmkGufv6ZMVYI776EN78V+h+Lpxxa9TRiIiISIpL5shZf2Cpuy9z993AeGBYhTaXAS+5+9cA7r42ifHUvK2r4YWroOXRcOHjYBZ1RCIiIpLiklmcdQBWxGwXhvtiHQ8cYWb5ZjbbzH4cc8yBN8P9o5IYZ3KU7IEXroGdW+DSv0HjFlFHJCIiIrWAuXtyTmx2MXCOu18Xbl8J9Hf3m2PaPArkAoOBJsCHwLnu/oWZHeXuq8KpzreAm919eiXfMwoYBZCdnd1v/PjxSbmeQ3Xs0rF0KpzEghNuZW12Xtz9ioqKyMrKSl5gdZzylxjlLzHKX2KUv8Qof4mJIn+DBg2a7e65Ffcn82nNQqBTzHZHYFUlbda7+zZgm5lNB3KAL9x9FQRTnWY2kWCadL/izN3HAGMAcnNzPS8vr7qv49DNewnyJ0H/n9Lzh7+h5yF0zc/PJyWuoZZS/hKj/CVG+UuM8pcY5S8xqZS/ZE5rzgS6mVkXM2sIjAAmV2gzCfiumaWbWSZwCrDQzJqaWTMAM2sKnA3MS2Ks1WfdYph0E3TsD2f/e9TRiIiISC2TtJEzdy82s5uAN4A0YKy7zzezG8Ljo919oZm9DnwOlAJPuPs8M+sKTLTgBvp04O/u/nqyYq02u7YGC802zIRL/hosOCsiIiJyCJK6CK27TwGmVNg3usL2H4A/VNi3jGB6s/Zwh0k/gw1fwo8nQfOjoo5IREREaiG9IaC6fPho8FLzs34HXb4bdTQiIiJSS+ndmtWh4J/w1r1wwvkw4OaDtxcRERGpgoqzRG1ZBS9cDa26wrDHtNCsiIiIJETTmoko3h0UZru3w1X/gMbNo45IREREajkVZ4l469ewYgYMHwvtekQdjYiIiNQBmtY8XHMnwIzRcOq/wIkXRR2NiIiI1BEqzg7HmgUw+WY4+jQ467dRRyMiIiJ1iIqzQ7Vzc7DQbKNmcPFTkJYRdUQiIiJSh+ies0PhDi//C3xbAFf/A5q1jzoiERERqWNUnB2K9/8HFv0DzvkPOGZA1NGIiIhIHaRpzXiVFMOiKdDrR3DqjVFHIyIiInWURs7ilZYeTGWWFmuhWREREUkaFWeHIr0R0CjqKERERKQO07SmiIiISApRcSYiIiKSQlSciYiIiKQQFWciIiIiKUTFmYiIiEgKSWpxZmZDzGyxmS01szuraJNnZnPMbL6ZTTuUviIiIiJ1TdKW0jCzNOAx4CygEJhpZpPdfUFMm5bAn4Eh7v61mbWLt6+IiIhIXZTMkbP+wFJ3X+buu4HxwLAKbS4DXnL3rwHcfe0h9BURERGpc5K5CG0HYEXMdiFwSoU2xwMZZpYPNAMedven4+wLgJmNAkYBZGdnk5+fXx2xR6aoqKjWX0OUlL/EKH+JUf4So/wlRvlLTCrlL5nFWWXvOPJKvr8fMBhoAnxoZh/F2TfY6T4GGAOQm5vreXl5hxtvSsjPz6e2X0OUlL/EKH+JUf4So/wlRvlLTCrlL5nFWSHQKWa7I7Cqkjbr3X0bsM3MpgM5cfYVERERqXOSec/ZTKCbmXUxs4bACGByhTaTgO+aWbqZZRJMXS6Ms6+IiIhInZO0kTN3Lzazm4A3gDRgrLvPN7MbwuOj3X2hmb0OfA6UAk+4+zyAyvomK1YRERGRVJHMaU3cfQowpcK+0RW2/wD8IZ6+IiIiInWd3hAgIiIikkJUnImIiIikEBVnIiIiIilExZmIiIhIClFxJiIiIpJCVJyJiIiIpBAVZyIiIiIpRMWZiIiISApRcSYiIiKSQpL6hoBUsGfPHgoLC9m5c2fUocSlRYsWLFy4MOowaqXGjRtjZlGHISIikpA6X5wVFhbSrFkzOnfuXCv+4t66dSvNmjWLOoxax93ZsGEDTZs2jToUERGRhNT5ac2dO3fSunXrWlGYyeEzM1q3bk1aWlrUoYiIiCSkzhdngAqzekL/nEVEpC6oF8VZVDZs2EDfvn3p27cv7du3p0OHDnu3d+/efcC+s2bN4pZbbjnodwwYMKC6wgXg5z//OR06dKC0tLRazysiIiLxqfP3nEWpdevWzJkzB4Df/OY3ZGVlcfvtt+89XlxcTHp65f8IcnNzyc3NPeh3fPDBB9USK0BpaSkTJ06kU6dOTJ8+nby8vGo7d6ySkhJNP4qIiFRBI2c17Oqrr+a2225j0KBB3HHHHXz88ccMGDCAk046iQEDBrBkyRIA8vPzOe+884CgsLv22mvJy8uja9euPPLII3vPl5WVtbd9Xl4ew4cPp0ePHlx++eW4OwBTpkyhR48enHHGGdxyyy17z1vR1KlTOfHEE7nxxht59tln9+5fs2YNF154ITk5OeTk5OwtCJ9++mn69OlDTk4OV1555d7rmzBhQqXxDRo0iMsuu4zevXsDcMEFF9CvXz969erFmDFj9vZ5/fXXOfnkk8nJyWHw4MGUlpbSrVs31q1bBwRF5HHHHcf69esP9x+DiIhIyqpXI2f3vTKfBau2VOs5ex7VnHuH9jqkPl988QVvv/02aWlpbNmyhenTp5Oens7bb7/Nfffdx6RJk/brs2jRIqZOncrWrVvp3r07N954IxkZGfu0+fTTT5k/fz5HHXUUp59+Ou+//z65ubn89Kc/Zfr06XTp0oWRI0dWGdezzz7LyJEjGTZsGHfffTd79uwhIyODW265hYEDBzJx4kRKSkooKipi/vz53H///bz//vu0adOGjRs3HvS6P/74Y+bNm0eXLl0AGDt2LK1atWLHjh185zvf4aKLLqK0tJTrr79+b7wbN26kQYMGXHHFFYwbN45f/OIXvP322+Tk5NCmTZtDyruIiEhtkNSRMzMbYmaLzWypmd1ZyfE8M9tsZnPCn3+LOVZgZnPD/bOSGWdNu/jii/dO623evJmLL76YE088kVtvvbXKNc7OPfdcGjVqRJs2bWjXrh1r1qzZr03//v3p2LEjDRo0oG/fvhQUFLBo0SK6du26tyCqqjjbvXs3U6ZM4YILLqB58+accsopvPnmmwC8++673HjjjQCkpaXRokUL3n33XYYPH763QGrVqtVBr7t///574wB45JFHyMnJ4dRTT2XFihUsWbKEjz76iO9973t725Wd99prr+Xpp58GgqLummuuOej3iYiI1EZJGzkzszTgMeAsoBCYaWaT3X1BhabvuXvl82wwyN2rbe7qUEe4kiV2La5f//rXDBo0iIkTJ1JQUMDAgQMr7dOoUaO9n9PS0iguLo6rTdnU5sG8/vrrbN68ee+U4/bt28nMzOTcc8+ttL27V/p0ZHp6+t6HCdx9nwcfYq87Pz+ft99+mw8//JDMzEzy8vLYuXNnleft1KkT2dnZvPvuu8yYMYNx48bFdV0iIiK1TTJHzvoDS919mbvvBsYDw5L4fbXS5s2b6dChAwBPPfVUtZ+/R48eLFu2jIKCAgCee+65Sts9++yzPPHEExQUFFBQUMDy5ct588032b59O4MHD+bxxx8Hgpv5t2zZwuDBg3n++efZsGEDwN5pzc6dOzN79mwAJk2axJ49eyr9vs2bN3PEEUeQmZnJokWL+OijjwA47bTTmDZtGsuXL9/nvADXXXcdV1xxBZdccokeKBARkTormcVZB2BFzHZhuK+i08zsMzN7zcxih7YceNPMZpvZqCTGGalf/epX3HXXXZx++umUlJRU+/mbNGnCn//8Z4YMGcIZZ5xBdnY2LVq02KfN9u3beeONN/YZJWvatClnnHEGr7zyCg8//DBTp06ld+/e9OvXj/nz59OrVy/uueceBg4cSE5ODrfddhsA119/PdOmTaN///7MmDGjyhX7hwwZQnFxMX369OHXv/41p556KgBt27ZlzJgx/OhHPyInJ4dLL710b5/zzz+foqIiTWmKiEidZvFOex3yic0uBs5x9+vC7SuB/u5+c0yb5kCpuxeZ2Q+Bh929W3jsKHdfZWbtgLeAm919eiXfMwoYBZCdnd1v/Pjx+xxv0aIFxx13XFKuMRmSscxEUVERWVlZuDu33XYbxx57LDfddFO1fkdN+OSTT7jrrrt44403qmyzZMkStmyp3oc+6pOyf1fk8Ch/iVH+EqP8JSaK/A0aNGi2u++3blYyn9YsBDrFbHcEVsU2cPctMZ+nmNmfzayNu69391Xh/rVmNpFgmnS/4szdxwBjAHJzc73i2lwLFy6sVe+qTMa7NZ944gn++te/snv3bk466SR+/vOfk5mZWa3fkWwPPPAAjz/+OOPGjTtgfswsaeuz1QdlS7LI4VH+EqP8JUb5S0wq5S+ZxdlMoJuZdQFWAiOAy2IbmFl7YI27u5n1J5hm3WBmTYEG7r41/Hw28Nskxlqn3Xrrrdx6661Rh5GQO++8kzvv3O+BXxERkTonacWZuxeb2U3AG0AaMNbd55vZDeHx0cBw4EYzKwZ2ACPCQi0bmBg+tZcO/N3dX09WrCIiIiKpIqmL0Lr7FGBKhX2jYz4/CjxaSb9lQE4yYxMRERFJRXp9k4iIiEgKUXEmIiIikkLq1bs1a9qGDRsYPHgwAKtXryYtLY22bdsCwXsmGzZseMD++fn5NGzYkAEDBlTZZtiwYaxdu5YPP/yw+gIXERGRyKg4S6LWrVszZ84cAH7zm9+QlZXF7bffHnf//Px8srKyqizONm3axCeffEJWVhbLly/f572V1am4uJj0dP2rIiIiUhM0rVnDZs+ezcCBA+nXrx/nnHMO33zzDRC8BLxnz56cdtppjBgxgoKCAkaPHs0f//hH+vbty3vvvbffuV588UWGDh3KiBEjiF18d+nSpXz/+98nJyeHk08+mS+//BKABx98kN69e5OTk7N3WYq8vDxmzQreK79+/Xo6d+4MBK+Suvjiixk6dChnn302RUVFDB48mJNPPpnevXszadKkvd/39NNP06dPH3JycrjyyivZunUrXbp02fvqpi1bttC5c+cqX+UkIiIi5erXcMhrd8LqudV7zva94QcPxNXU3bn55puZNGkSbdu25bnnnuOee+5h7NixPPDAAyxfvpzdu3dTUlJCy5YtueGGGw442vbss89y7733kp2dzfDhw7nrrrsAuPzyy7nzzju58MIL2blzJ6Wlpbz22mu8/PLLzJgxg8zMzH3eWVmVDz/8kM8//5xWrVpRXFzMxIkTad68OevXr+fUU0/l/PPPZ8GCBdx///28//77tGnTho0bN9KsWTPy8vJ49dVXueCCCxg/fjwXXXQRGRkZ8edVRESknqpfxVnEdu3axbx58zjrrLOA4FVNRx55JAB9+vTh8ssv55xzzmHkyJEHPdeaNWtYunQpZ5xxBmZGeno68+bN45hjjmHlypVceOGFADRu3BiAt99+m2uuuWbvmwFatWp10O8466yz9rZzd+6++26mT59OgwYNWLlyJWvWrOHdd99l+PDhtGnTZp/zXnfddTz44INccMEFPPnkk/zlL385lFSJiIjUW/WrOItzhCtZ3J1evXpVevP+q6++yvTp05kwYQIPPfQQ8+fPP+C5nnvuOb799tu995lt2bKF8ePH86tf/arK7w4X9d1Heno6paWlAOzcuXOfY7EvLR83bhzr1q1j9uzZZGRk0LlzZ3bu3FnleU8//XQKCgqYNm0aJSUlnHjiiQe8HhEREQnonrMa1KhRI9atW7e3ONuzZw/z58+ntLSUFStWMGjQIH73u9+xadMmioqKaNasGVu3bq30XM8++yyvv/46BQUFFBQUMHv2bMaPH0/z5s3p2LEjL7/8MhCM1m3fvp2zzz6bsWPHsn37doC905qdO3dm9uzZAEyYMKHK2Ddv3ky7du3IyMhg6tSpfPXVVwAMHjyY559/ng0bNuxzXoAf//jHjBw5kmuuuSaBrImIiNQvKs5qUIMGDZgwYQJ33HEHOTk59O3blw8++ICSkhKuuOIKevfuzRlnnMGtt95Ky5YtGTp0KBMnTtzvgYCCggK+/vprTj311L37unTpQvPmzZkxYwbPPPMMjzzyCH369GHAgAGsXr2aIUOGcP7555Obm0vfvn156KGHALj99tt5/PHHGTBgAOvXr68y9ssvv5xZs2aRm5vLuHHj6NGjBwC9evXinnvuYeDAgeTk5HDbbbft0+fbb7+Na5pWREREAubuUcdQbXJzc73sycMyCxcu5IQTTogookO3detWmjVrFnUY1WLChAlMmjSJZ555psa+89NPP+Wkk06qse+ra/Lz88nLy4s6jFpL+UuM8pcY5S8xUeTPzGa7e27F/fXrnjOpMTfffDOvvfYaU6ZMOXhjERER2UvFmSTFn/70p6hDEBERqZV0z5mIiIhICqkXxVlduq9OqqZ/ziIiUhfU+eKscePGbNiwQX9x13HuzoYNGygpKYk6FBERkYTU+XvOOnbsSGFhIevWrYs6lLjs3Llz76r+cmgaN27Mtm3bog5DREQkIUktzsxsCPAwkAY84e4PVDieB0wCloe7XnL338bTN14ZGRl7V9GvDfLz87UURALKFscVERGprZJWnJlZGvAYcBZQCMw0s8nuvqBC0/fc/bzD7CsiIiJSpyTznrP+wFJ3X+buu4HxwLAa6CsiIiJSayWzOOsArIjZLgz3VXSamX1mZq+ZWa9D7CsiIiJSpyTznjOrZF/FRyY/AY5x9yIz+yHwMtAtzr7Bl5iNAkaFm0Vmtvjwwk0ZbYCqX3IpB6P8JUb5S4zylxjlLzHKX2KiyN8xle1MZnFWCHSK2e4IrIpt4O5bYj5PMbM/m1mbePrG9BsDjKmuoKNmZrMqe8+WxEf5S4zylxjlLzHKX2KUv8SkUv6SOa05E+hmZl3MrCEwApgc28DM2puZhZ/7h/FsiKeviIiISF2UtJEzdy82s5uANwiWwxjr7vPN7Ibw+GhgOHCjmRUDO4ARHqwWW2nfZMUqIiIikiqSus6Zu08BplTYNzrm86PAo/H2rSfqzBRtRJS/xCh/iVH+EqP8JUb5S0zK5M/0WiMRERGR1FHn360pIiIiUpuoOEsyM+tkZlPNbKGZzTezn4f7W5nZW2a2JPx9REyfu8xsqZktNrNzYvb3M7O54bFHyh6mqOvMLM3MPjWzf4Tbyt0hMLOWZjbBzBaF/x6ephzGz8xuDf/bnWdmz5pZY+WvamY21szWmtm8mH3Vli8za2Rmz4X7Z5hZ5xq9wCSrIn9/CP/7/dzMJppZy5hjyl+MyvIXc+x2M3MLVoUo25ea+XN3/STxBzgSODn83Az4AugJPAjcGe6/E/jP8HNP4DOgEdAF+BJIC499DJxGsA7ca8APor6+GsrhbcDfgX+E28rdoeXvr8B14eeGQEvlMO7cdSB492+TcPt54Grl74A5+x5wMjAvZl+15Qv4F2B0+HkE8FzU11wD+TsbSA8//6fyd2j5C/d3InjI8CugTarnTyNnSebu37j7J+HnrcBCgj/whxH8pUn4+4Lw8zBgvLvvcvflwFKgv5kdCTR39w89+Lfi6Zg+dZaZdQTOBZ6I2a3cxcnMmhP8YfV/AO6+2903oRweinSgiZmlA5kEay4qf1Vw9+nAxgq7qzNfseeaAAyuS6OQleXP3d909+Jw8yOCtT9B+dtPFf/+AfwR+BX7LmifsvlTcVaDwuHPk4AZQLa7fwNBAQe0C5tV9eqqDuHnivvruv8h+A+qNGafche/rsA64EkLpoafMLOmKIdxcfeVwEPA18A3wGZ3fxPl71BVZ7729gkLls1A66RFnnquJRjJAeUvLmZ2PrDS3T+rcChl86firIaYWRbwIvALj3kzQmVNK9nnB9hfZ5nZecBad58db5dK9tXL3MVIJxjif9zdTwK2EUwrVUU5jBHeGzWMYMrjKKCpmV1xoC6V7Ku3+YvD4eSr3ubSzO4BioFxZbsqaab8xTCzTOAe4N8qO1zJvpTIn4qzGmBmGQSF2Th3fyncvSYcOiX8vTbcX9WrqwopH8qO3V+XnQ6cb2YFwHjgTDP7G8rdoSgECt19Rrg9gaBYUw7j831gubuvc/c9wEvAAJS/Q1Wd+drbJ5xqbkHl01h1ipldBZwHXB5OtYHyF49jCf7n6rPw75KOwCdm1p4Uzp+KsyQL56L/D1jo7v8dc2gycFX4+SpgUsz+EeETIV0IXgT/cTgVsNXMTg3P+eOYPnWSu9/l7h3dvTPBjZfvuvsVKHdxc/fVwAoz6x7uGgwsQDmM19fAqWaWGV73YIL7RpW/Q1Od+Yo913CCPxfq7MgPgJkNAe4Aznf37TGHlL+DcPe57t7O3TuHf5cUEjykt5pUzl8ynjLQzz5PiJxBMOT5OTAn/PkhwRz1O8CS8HermD73EDw1spiYJ7qAXGBeeOxRwkWE68MPkEf505rK3aHlri8wK/x38GXgCOXwkPJ3H7AovPZnCJ7sUv6qztezBPfn7SH4i/An1ZkvoDHwAsHN2x8DXaO+5hrI31KC+5zK/g4ZrfzFn78KxwsIn9ZM5fzpDQEiIiIiKUTTmiIiIiIpRMWZiIiISApRcSYiIiKSQlSciYiIiKQQFWciIiIiKUTFmYgkhZndY2bzzexzM5tjZqeE+58ws55J+s62ZjYjfFXVd2P2TwxjWGpmm8PPc8xsQJzn/SCONtV2XWZWEsY338w+M7PbzOyAf16bWWczu6w6vl9EoqWlNESk2pnZacB/A3nuvsvM2gAN3T2pq+Kb2QiCtYququJ4HnC7u59XYX+6l79YOnJmVuTuWeHndsDfgffd/d4D9MmjkmsTkdpHI2cikgxHAuvdfReAu68vK8zMLN/Mcs3s/JgRrMVmtjw83s/MppnZbDN7o+y1P7HM7BgzeycclXvHzI42s77Ag8APw3M2OVCAZna1mb1gZq8Ab5pZVniuT8xsrpkNi2lbFP7OC+OfYGaLzGxcuIL43usqa29m94ejXh+ZWXa4/9hwe6aZ/bbsvAfi7muBUcBNFuhsZu+FcX4SM/r3APDd8NpvPUA7EUlxKs5EJBneBDqZ2Rdm9mczG1ixgbtPdve+7t4X+Ax4yIL30P4JGO7u/YCxwP2VnP9R4Gl370PwEuhH3H0OwcuNnwvPuyOOOE8DrnL3M4GdwIXufjIwCPivssKrgpOAXwA9ga4E74CtqCnwkbvnANOB68P9DwMPu/t3OIR3a7r7MoI/r9sRvJfyrDDOS4FHwmZ3Au+F1/7HA7QTkRSn4kxEqp27FwH9CEZ81gHPmdnVlbU1s18BO9z9MaA7cCLwlpnNAf6VfV9AXOY0gqk+CF6pdMZhhvqWu5e9tNiA35vZ58DbQAcgu5I+H7t7obuXErxKp3MlbXYD/wg/z45pcxrBq1+IiT9eZYViBvAXM5sbnquq+9zibSciKSY96gBEpG5y9xIgH8gPC4SrgKdi25jZYOBi4Htlu4D57n7aoX7dYYa5Lebz5UBboJ+77zGzAoL36FW0K+ZzCZX/ObrHy2/orapN3Mysa3ietcC9wBogh+B/sHdW0e3WONuJSIrRyJmIVDsz625m3WJ29QW+qtDmGODPwCUxU5CLgbbhAwWYWYaZ9arkKz4ARoSfLwf+WQ1htwDWhoXZIOCYajhnRR8BF4WfRxyoYRkzawuMBh4NC74WwDfhyN2VQFrYdCvQLKZrVe1EJMVp5ExEkiEL+JOZtQSKgaUEU5yxrgZaAxPDW7tWufsPzWw48IiZtSD4M+p/gPkV+t4CjDWzXxJMm15TDTGPA14xs1kE05WLquGcFf0C+JuZ/T/gVWBzFe2ahNO6GQT5e4bg6VcICtoXzexiYCrlo3+fA8Vm9hnBCGVV7UQkxWkpDRGRGmJmmQT313m47MdIdx92sH4iUr9o5ExEpOb0Ax4NnwLdBFwbbTgikoo0ciYiIiKSQvRAgIiIiEgKUXEmIiIikkJUnImIiIikEBVnIiIiIilExZmIiIhIClFxJiIiIpJC/j//X/yi48AJJQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.20, random_state=42, stratify=y)\n",
    "\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "training_sizes = []\n",
    "\n",
    "training_proportions = np.arange(0.1, 1.1, 0.1)  # From 10% to 100% in 10% increments\n",
    "\n",
    "for proportion in training_proportions:\n",
    "    num_samples = int(proportion * len(X_train))\n",
    "    training_sizes.append(num_samples)\n",
    "    \n",
    "    X_train_subset = X_train[:num_samples]\n",
    "    y_train_subset = y_train[:num_samples]\n",
    "    \n",
    "    model = CatBoostClassifier(\n",
    "        iterations=1000, \n",
    "        learning_rate=0.1, \n",
    "        depth=6,\n",
    "        l2_leaf_reg=3,\n",
    "        loss_function='MultiClass',  # Specifying the loss function for multi-class classification\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train_subset, y_train_subset, eval_set=(X_test, y_test), verbose=False)\n",
    "    \n",
    "    train_predictions = model.predict(X_train_subset)\n",
    "    train_accuracy = accuracy_score(y_train_subset, train_predictions)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    test_predictions = model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(training_sizes, train_accuracies, label='Training Accuracy')\n",
    "plt.plot(training_sizes, test_accuracies, label='Test Accuracy')\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Size of Training Data')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.yticks(np.arange(0.5, 1.05, 0.05))  # Setting y-ticks from 50% to 100% with 5% increments\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e65283",
   "metadata": {},
   "source": [
    "<p><strong>Observations:</strong> The testing accuracy rising gradually to 93% is a good sign, indicating that the model is improving its ability to generalize to new data. However, as training accuracy is at 100% and does not converge with the testing accuracy, this discrepancy can still be indicative of overfitting.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feeb6c7",
   "metadata": {},
   "source": [
    "### Catboost best Model:\n",
    "Based on the analysis of CatBoost models, we have identified the best-performing model to be <u>CatBoost with Truncated SVD_version2</u> This model, with a high learning rate of 0.1 and 500 iterations, achieved an precision of 90% and also showing good fit trend as per learning curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f14458",
   "metadata": {},
   "source": [
    "### Confidence threshold in CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0010349c",
   "metadata": {},
   "source": [
    "<p>Up to this point, CatBoost with a high learning rate of 0.1 and 500 iterations has produced a good model, achieving a precision of 91%. Now, let's delve into another valuable aspect of this models: the use of a confidence threshold.</p>\n",
    "<p>Here's how the confidence_threshold works in CatBoost:</p>\n",
    "<ul>\n",
    "    <li><strong>Probability Prediction:</strong> After training a CatBoostClassifier model, we can use it to predict the probabilities of each class for a given input data point. These probabilities indicate how confident the model is about the data point belonging to each class.</li>\n",
    "    <li><strong>Setting the Threshold:</strong> We can set a confidence_threshold value between 0 and 1.</li>\n",
    "    <li><strong>Classification Decision:</strong> When making predictions, if the highest probability for a data point is less than the confidence_threshold, the prediction is flagged as unreliable.</li>\n",
    "    <li><strong>Manual Review:</strong> Data points for which the highest predicted probability is below the confidence_threshold are typically flagged for manual review. In practice, a human reviewer can assess these cases and make informed decisions about their classification. For example, in a legal document categorization application, the confidence_threshold can be used to identify documents that require further scrutiny.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "90f1c1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total flagged items out of 3521 items: 805\n",
      "Flagged items for manual review:\n",
      "Item 975 flagged for manual review. Probability: 0.19\n",
      "Item 1255 flagged for manual review. Probability: 0.21\n",
      "Item 1504 flagged for manual review. Probability: 0.21\n",
      "Item 2321 flagged for manual review. Probability: 0.22\n",
      "Item 1177 flagged for manual review. Probability: 0.23\n",
      "Item 1500 flagged for manual review. Probability: 0.23\n",
      "Item 1047 flagged for manual review. Probability: 0.25\n",
      "Item 3498 flagged for manual review. Probability: 0.25\n",
      "Item 1079 flagged for manual review. Probability: 0.26\n",
      "Item 2589 flagged for manual review. Probability: 0.26\n",
      "Item 1226 flagged for manual review. Probability: 0.26\n",
      "Item 114 flagged for manual review. Probability: 0.26\n",
      "Item 1353 flagged for manual review. Probability: 0.26\n",
      "Item 1723 flagged for manual review. Probability: 0.26\n",
      "Item 1267 flagged for manual review. Probability: 0.27\n",
      "Item 1994 flagged for manual review. Probability: 0.27\n",
      "Item 1200 flagged for manual review. Probability: 0.27\n",
      "Item 1023 flagged for manual review. Probability: 0.27\n",
      "Item 1232 flagged for manual review. Probability: 0.27\n",
      "Item 665 flagged for manual review. Probability: 0.29\n",
      "Item 2954 flagged for manual review. Probability: 0.29\n",
      "Item 1419 flagged for manual review. Probability: 0.29\n",
      "Item 2138 flagged for manual review. Probability: 0.30\n",
      "Item 3232 flagged for manual review. Probability: 0.30\n",
      "Item 1834 flagged for manual review. Probability: 0.30\n",
      "Item 2291 flagged for manual review. Probability: 0.30\n",
      "Item 1728 flagged for manual review. Probability: 0.30\n",
      "Item 1254 flagged for manual review. Probability: 0.30\n",
      "Item 843 flagged for manual review. Probability: 0.31\n",
      "Item 2810 flagged for manual review. Probability: 0.31\n",
      "Item 3370 flagged for manual review. Probability: 0.31\n",
      "Item 1710 flagged for manual review. Probability: 0.31\n",
      "Item 261 flagged for manual review. Probability: 0.31\n",
      "Item 3327 flagged for manual review. Probability: 0.31\n",
      "Item 168 flagged for manual review. Probability: 0.31\n",
      "Item 2993 flagged for manual review. Probability: 0.32\n",
      "Item 1984 flagged for manual review. Probability: 0.32\n",
      "Item 2116 flagged for manual review. Probability: 0.32\n",
      "Item 2495 flagged for manual review. Probability: 0.32\n",
      "Item 1816 flagged for manual review. Probability: 0.32\n",
      "Item 3017 flagged for manual review. Probability: 0.32\n",
      "Item 451 flagged for manual review. Probability: 0.32\n",
      "Item 3348 flagged for manual review. Probability: 0.32\n",
      "Item 3068 flagged for manual review. Probability: 0.32\n",
      "Item 2877 flagged for manual review. Probability: 0.32\n",
      "Item 1350 flagged for manual review. Probability: 0.33\n",
      "Item 2655 flagged for manual review. Probability: 0.33\n",
      "Item 2301 flagged for manual review. Probability: 0.33\n",
      "Item 3204 flagged for manual review. Probability: 0.33\n",
      "Item 1986 flagged for manual review. Probability: 0.33\n",
      "Item 2621 flagged for manual review. Probability: 0.33\n",
      "Item 1826 flagged for manual review. Probability: 0.34\n",
      "Item 2380 flagged for manual review. Probability: 0.34\n",
      "Item 2384 flagged for manual review. Probability: 0.34\n",
      "Item 38 flagged for manual review. Probability: 0.34\n",
      "Item 2519 flagged for manual review. Probability: 0.34\n",
      "Item 1498 flagged for manual review. Probability: 0.35\n",
      "Item 1712 flagged for manual review. Probability: 0.35\n",
      "Item 1024 flagged for manual review. Probability: 0.35\n",
      "Item 1265 flagged for manual review. Probability: 0.35\n",
      "Item 1422 flagged for manual review. Probability: 0.35\n",
      "Item 58 flagged for manual review. Probability: 0.35\n",
      "Item 1027 flagged for manual review. Probability: 0.35\n",
      "Item 412 flagged for manual review. Probability: 0.35\n",
      "Item 1224 flagged for manual review. Probability: 0.35\n",
      "Item 2369 flagged for manual review. Probability: 0.36\n",
      "Item 2657 flagged for manual review. Probability: 0.36\n",
      "Item 2978 flagged for manual review. Probability: 0.36\n",
      "Item 2969 flagged for manual review. Probability: 0.36\n",
      "Item 198 flagged for manual review. Probability: 0.36\n",
      "Item 126 flagged for manual review. Probability: 0.36\n",
      "Item 303 flagged for manual review. Probability: 0.36\n",
      "Item 1392 flagged for manual review. Probability: 0.36\n",
      "Item 3472 flagged for manual review. Probability: 0.36\n",
      "Item 729 flagged for manual review. Probability: 0.37\n",
      "Item 1379 flagged for manual review. Probability: 0.37\n",
      "Item 2019 flagged for manual review. Probability: 0.37\n",
      "Item 1286 flagged for manual review. Probability: 0.37\n",
      "Item 892 flagged for manual review. Probability: 0.37\n",
      "Item 2563 flagged for manual review. Probability: 0.37\n",
      "Item 1482 flagged for manual review. Probability: 0.37\n",
      "Item 2037 flagged for manual review. Probability: 0.37\n",
      "Item 3249 flagged for manual review. Probability: 0.38\n",
      "Item 2789 flagged for manual review. Probability: 0.38\n",
      "Item 1781 flagged for manual review. Probability: 0.38\n",
      "Item 2718 flagged for manual review. Probability: 0.38\n",
      "Item 1476 flagged for manual review. Probability: 0.38\n",
      "Item 2457 flagged for manual review. Probability: 0.38\n",
      "Item 1403 flagged for manual review. Probability: 0.38\n",
      "Item 1885 flagged for manual review. Probability: 0.38\n",
      "Item 2355 flagged for manual review. Probability: 0.38\n",
      "Item 1709 flagged for manual review. Probability: 0.38\n",
      "Item 648 flagged for manual review. Probability: 0.39\n",
      "Item 403 flagged for manual review. Probability: 0.39\n",
      "Item 2879 flagged for manual review. Probability: 0.39\n",
      "Item 1015 flagged for manual review. Probability: 0.39\n",
      "Item 2967 flagged for manual review. Probability: 0.39\n",
      "Item 2686 flagged for manual review. Probability: 0.39\n",
      "Item 1151 flagged for manual review. Probability: 0.39\n",
      "Item 3494 flagged for manual review. Probability: 0.39\n",
      "Item 2300 flagged for manual review. Probability: 0.39\n",
      "Item 2909 flagged for manual review. Probability: 0.39\n",
      "Item 1954 flagged for manual review. Probability: 0.39\n",
      "Item 1899 flagged for manual review. Probability: 0.39\n",
      "Item 1175 flagged for manual review. Probability: 0.39\n",
      "Item 2440 flagged for manual review. Probability: 0.39\n",
      "Item 1097 flagged for manual review. Probability: 0.39\n",
      "Item 401 flagged for manual review. Probability: 0.39\n",
      "Item 2798 flagged for manual review. Probability: 0.39\n",
      "Item 468 flagged for manual review. Probability: 0.40\n",
      "Item 553 flagged for manual review. Probability: 0.40\n",
      "Item 967 flagged for manual review. Probability: 0.40\n",
      "Item 2587 flagged for manual review. Probability: 0.40\n",
      "Item 753 flagged for manual review. Probability: 0.40\n",
      "Item 1336 flagged for manual review. Probability: 0.40\n",
      "Item 2614 flagged for manual review. Probability: 0.40\n",
      "Item 385 flagged for manual review. Probability: 0.40\n",
      "Item 2035 flagged for manual review. Probability: 0.40\n",
      "Item 1137 flagged for manual review. Probability: 0.40\n",
      "Item 2873 flagged for manual review. Probability: 0.40\n",
      "Item 2869 flagged for manual review. Probability: 0.41\n",
      "Item 968 flagged for manual review. Probability: 0.41\n",
      "Item 1846 flagged for manual review. Probability: 0.41\n",
      "Item 1855 flagged for manual review. Probability: 0.41\n",
      "Item 2238 flagged for manual review. Probability: 0.41\n",
      "Item 1753 flagged for manual review. Probability: 0.41\n",
      "Item 252 flagged for manual review. Probability: 0.41\n",
      "Item 1570 flagged for manual review. Probability: 0.41\n",
      "Item 2700 flagged for manual review. Probability: 0.41\n",
      "Item 1243 flagged for manual review. Probability: 0.41\n",
      "Item 212 flagged for manual review. Probability: 0.41\n",
      "Item 715 flagged for manual review. Probability: 0.41\n",
      "Item 2728 flagged for manual review. Probability: 0.41\n",
      "Item 847 flagged for manual review. Probability: 0.41\n",
      "Item 2696 flagged for manual review. Probability: 0.41\n",
      "Item 2977 flagged for manual review. Probability: 0.41\n",
      "Item 2135 flagged for manual review. Probability: 0.41\n",
      "Item 1531 flagged for manual review. Probability: 0.41\n",
      "Item 2896 flagged for manual review. Probability: 0.41\n",
      "Item 3405 flagged for manual review. Probability: 0.42\n",
      "Item 3094 flagged for manual review. Probability: 0.42\n",
      "Item 2487 flagged for manual review. Probability: 0.42\n",
      "Item 3233 flagged for manual review. Probability: 0.42\n",
      "Item 2843 flagged for manual review. Probability: 0.42\n",
      "Item 2994 flagged for manual review. Probability: 0.42\n",
      "Item 2592 flagged for manual review. Probability: 0.42\n",
      "Item 2092 flagged for manual review. Probability: 0.43\n",
      "Item 250 flagged for manual review. Probability: 0.43\n",
      "Item 3288 flagged for manual review. Probability: 0.43\n",
      "Item 1334 flagged for manual review. Probability: 0.43\n",
      "Item 3217 flagged for manual review. Probability: 0.43\n",
      "Item 1431 flagged for manual review. Probability: 0.43\n",
      "Item 1695 flagged for manual review. Probability: 0.43\n",
      "Item 3090 flagged for manual review. Probability: 0.43\n",
      "Item 2119 flagged for manual review. Probability: 0.43\n",
      "Item 2821 flagged for manual review. Probability: 0.43\n",
      "Item 504 flagged for manual review. Probability: 0.43\n",
      "Item 3111 flagged for manual review. Probability: 0.43\n",
      "Item 1184 flagged for manual review. Probability: 0.43\n",
      "Item 1831 flagged for manual review. Probability: 0.43\n",
      "Item 2526 flagged for manual review. Probability: 0.44\n",
      "Item 328 flagged for manual review. Probability: 0.44\n",
      "Item 125 flagged for manual review. Probability: 0.44\n",
      "Item 113 flagged for manual review. Probability: 0.44\n",
      "Item 795 flagged for manual review. Probability: 0.44\n",
      "Item 2249 flagged for manual review. Probability: 0.44\n",
      "Item 2884 flagged for manual review. Probability: 0.44\n",
      "Item 270 flagged for manual review. Probability: 0.44\n",
      "Item 1271 flagged for manual review. Probability: 0.44\n",
      "Item 2557 flagged for manual review. Probability: 0.44\n",
      "Item 1110 flagged for manual review. Probability: 0.44\n",
      "Item 2362 flagged for manual review. Probability: 0.44\n",
      "Item 2521 flagged for manual review. Probability: 0.44\n",
      "Item 3506 flagged for manual review. Probability: 0.44\n",
      "Item 1309 flagged for manual review. Probability: 0.44\n",
      "Item 683 flagged for manual review. Probability: 0.44\n",
      "Item 1776 flagged for manual review. Probability: 0.45\n",
      "Item 1481 flagged for manual review. Probability: 0.45\n",
      "Item 3171 flagged for manual review. Probability: 0.45\n",
      "Item 2817 flagged for manual review. Probability: 0.45\n",
      "Item 2429 flagged for manual review. Probability: 0.45\n",
      "Item 2771 flagged for manual review. Probability: 0.45\n",
      "Item 548 flagged for manual review. Probability: 0.45\n",
      "Item 316 flagged for manual review. Probability: 0.45\n",
      "Item 1667 flagged for manual review. Probability: 0.45\n",
      "Item 1571 flagged for manual review. Probability: 0.45\n",
      "Item 2981 flagged for manual review. Probability: 0.45\n",
      "Item 1126 flagged for manual review. Probability: 0.45\n",
      "Item 2607 flagged for manual review. Probability: 0.45\n",
      "Item 623 flagged for manual review. Probability: 0.45\n",
      "Item 10 flagged for manual review. Probability: 0.45\n",
      "Item 2608 flagged for manual review. Probability: 0.45\n",
      "Item 1579 flagged for manual review. Probability: 0.45\n",
      "Item 2478 flagged for manual review. Probability: 0.45\n",
      "Item 1760 flagged for manual review. Probability: 0.45\n",
      "Item 140 flagged for manual review. Probability: 0.45\n",
      "Item 2377 flagged for manual review. Probability: 0.45\n",
      "Item 1505 flagged for manual review. Probability: 0.45\n",
      "Item 103 flagged for manual review. Probability: 0.46\n",
      "Item 1279 flagged for manual review. Probability: 0.46\n",
      "Item 1143 flagged for manual review. Probability: 0.46\n",
      "Item 2666 flagged for manual review. Probability: 0.46\n",
      "Item 3359 flagged for manual review. Probability: 0.47\n",
      "Item 1031 flagged for manual review. Probability: 0.47\n",
      "Item 1077 flagged for manual review. Probability: 0.47\n",
      "Item 1393 flagged for manual review. Probability: 0.47\n",
      "Item 2484 flagged for manual review. Probability: 0.47\n",
      "Item 414 flagged for manual review. Probability: 0.47\n",
      "Item 3234 flagged for manual review. Probability: 0.47\n",
      "Item 1000 flagged for manual review. Probability: 0.47\n",
      "Item 821 flagged for manual review. Probability: 0.47\n",
      "Item 732 flagged for manual review. Probability: 0.47\n",
      "Item 2180 flagged for manual review. Probability: 0.47\n",
      "Item 1567 flagged for manual review. Probability: 0.47\n",
      "Item 163 flagged for manual review. Probability: 0.47\n",
      "Item 1802 flagged for manual review. Probability: 0.47\n",
      "Item 3007 flagged for manual review. Probability: 0.47\n",
      "Item 2581 flagged for manual review. Probability: 0.47\n",
      "Item 573 flagged for manual review. Probability: 0.47\n",
      "Item 3213 flagged for manual review. Probability: 0.48\n",
      "Item 2511 flagged for manual review. Probability: 0.48\n",
      "Item 2391 flagged for manual review. Probability: 0.48\n",
      "Item 1991 flagged for manual review. Probability: 0.48\n",
      "Item 2448 flagged for manual review. Probability: 0.48\n",
      "Item 3123 flagged for manual review. Probability: 0.48\n",
      "Item 1805 flagged for manual review. Probability: 0.48\n",
      "Item 397 flagged for manual review. Probability: 0.48\n",
      "Item 202 flagged for manual review. Probability: 0.48\n",
      "Item 1643 flagged for manual review. Probability: 0.48\n",
      "Item 2761 flagged for manual review. Probability: 0.48\n",
      "Item 162 flagged for manual review. Probability: 0.48\n",
      "Item 1733 flagged for manual review. Probability: 0.48\n",
      "Item 1149 flagged for manual review. Probability: 0.48\n",
      "Item 2080 flagged for manual review. Probability: 0.48\n",
      "Item 1299 flagged for manual review. Probability: 0.48\n",
      "Item 1559 flagged for manual review. Probability: 0.48\n",
      "Item 3374 flagged for manual review. Probability: 0.48\n",
      "Item 2363 flagged for manual review. Probability: 0.48\n",
      "Item 494 flagged for manual review. Probability: 0.48\n",
      "Item 3125 flagged for manual review. Probability: 0.48\n",
      "Item 819 flagged for manual review. Probability: 0.48\n",
      "Item 569 flagged for manual review. Probability: 0.48\n",
      "Item 3152 flagged for manual review. Probability: 0.48\n",
      "Item 2428 flagged for manual review. Probability: 0.48\n",
      "Item 3492 flagged for manual review. Probability: 0.48\n",
      "Item 3434 flagged for manual review. Probability: 0.48\n",
      "Item 2251 flagged for manual review. Probability: 0.49\n",
      "Item 988 flagged for manual review. Probability: 0.49\n",
      "Item 3009 flagged for manual review. Probability: 0.49\n",
      "Item 394 flagged for manual review. Probability: 0.49\n",
      "Item 3032 flagged for manual review. Probability: 0.49\n",
      "Item 3023 flagged for manual review. Probability: 0.49\n",
      "Item 895 flagged for manual review. Probability: 0.49\n",
      "Item 1772 flagged for manual review. Probability: 0.49\n",
      "Item 2616 flagged for manual review. Probability: 0.49\n",
      "Item 2359 flagged for manual review. Probability: 0.49\n",
      "Item 3435 flagged for manual review. Probability: 0.49\n",
      "Item 2667 flagged for manual review. Probability: 0.49\n",
      "Item 12 flagged for manual review. Probability: 0.49\n",
      "Item 2987 flagged for manual review. Probability: 0.49\n",
      "Item 601 flagged for manual review. Probability: 0.49\n",
      "Item 1038 flagged for manual review. Probability: 0.49\n",
      "Item 917 flagged for manual review. Probability: 0.49\n",
      "Item 1186 flagged for manual review. Probability: 0.49\n",
      "Item 697 flagged for manual review. Probability: 0.49\n",
      "Item 576 flagged for manual review. Probability: 0.49\n",
      "Item 218 flagged for manual review. Probability: 0.49\n",
      "Item 3211 flagged for manual review. Probability: 0.50\n",
      "Item 1356 flagged for manual review. Probability: 0.50\n",
      "Item 2627 flagged for manual review. Probability: 0.50\n",
      "Item 1745 flagged for manual review. Probability: 0.50\n",
      "Item 1022 flagged for manual review. Probability: 0.50\n",
      "Item 39 flagged for manual review. Probability: 0.50\n",
      "Item 1943 flagged for manual review. Probability: 0.50\n",
      "Item 514 flagged for manual review. Probability: 0.50\n",
      "Item 3256 flagged for manual review. Probability: 0.50\n",
      "Item 1599 flagged for manual review. Probability: 0.50\n",
      "Item 1227 flagged for manual review. Probability: 0.50\n",
      "Item 1300 flagged for manual review. Probability: 0.50\n",
      "Item 432 flagged for manual review. Probability: 0.50\n",
      "Item 1164 flagged for manual review. Probability: 0.50\n",
      "Item 3509 flagged for manual review. Probability: 0.50\n",
      "Item 1502 flagged for manual review. Probability: 0.50\n",
      "Item 1008 flagged for manual review. Probability: 0.50\n",
      "Item 2489 flagged for manual review. Probability: 0.50\n",
      "Item 2992 flagged for manual review. Probability: 0.50\n",
      "Item 3020 flagged for manual review. Probability: 0.50\n",
      "Item 3431 flagged for manual review. Probability: 0.50\n",
      "Item 1880 flagged for manual review. Probability: 0.51\n",
      "Item 3290 flagged for manual review. Probability: 0.51\n",
      "Item 887 flagged for manual review. Probability: 0.51\n",
      "Item 324 flagged for manual review. Probability: 0.51\n",
      "Item 371 flagged for manual review. Probability: 0.51\n",
      "Item 1520 flagged for manual review. Probability: 0.51\n",
      "Item 2805 flagged for manual review. Probability: 0.51\n",
      "Item 600 flagged for manual review. Probability: 0.51\n",
      "Item 235 flagged for manual review. Probability: 0.51\n",
      "Item 505 flagged for manual review. Probability: 0.51\n",
      "Item 3187 flagged for manual review. Probability: 0.51\n",
      "Item 755 flagged for manual review. Probability: 0.51\n",
      "Item 1618 flagged for manual review. Probability: 0.51\n",
      "Item 1461 flagged for manual review. Probability: 0.51\n",
      "Item 2166 flagged for manual review. Probability: 0.51\n",
      "Item 1078 flagged for manual review. Probability: 0.51\n",
      "Item 40 flagged for manual review. Probability: 0.51\n",
      "Item 852 flagged for manual review. Probability: 0.51\n",
      "Item 1064 flagged for manual review. Probability: 0.51\n",
      "Item 2611 flagged for manual review. Probability: 0.52\n",
      "Item 1727 flagged for manual review. Probability: 0.52\n",
      "Item 2740 flagged for manual review. Probability: 0.52\n",
      "Item 289 flagged for manual review. Probability: 0.52\n",
      "Item 1174 flagged for manual review. Probability: 0.52\n",
      "Item 906 flagged for manual review. Probability: 0.52\n",
      "Item 70 flagged for manual review. Probability: 0.52\n",
      "Item 2609 flagged for manual review. Probability: 0.52\n",
      "Item 1055 flagged for manual review. Probability: 0.52\n",
      "Item 1382 flagged for manual review. Probability: 0.52\n",
      "Item 862 flagged for manual review. Probability: 0.52\n",
      "Item 3379 flagged for manual review. Probability: 0.52\n",
      "Item 2491 flagged for manual review. Probability: 0.52\n",
      "Item 2126 flagged for manual review. Probability: 0.52\n",
      "Item 1269 flagged for manual review. Probability: 0.52\n",
      "Item 266 flagged for manual review. Probability: 0.52\n",
      "Item 781 flagged for manual review. Probability: 0.52\n",
      "Item 1803 flagged for manual review. Probability: 0.52\n",
      "Item 2274 flagged for manual review. Probability: 0.52\n",
      "Item 2888 flagged for manual review. Probability: 0.53\n",
      "Item 1266 flagged for manual review. Probability: 0.53\n",
      "Item 540 flagged for manual review. Probability: 0.53\n",
      "Item 1383 flagged for manual review. Probability: 0.53\n",
      "Item 456 flagged for manual review. Probability: 0.53\n",
      "Item 1672 flagged for manual review. Probability: 0.53\n",
      "Item 77 flagged for manual review. Probability: 0.53\n",
      "Item 51 flagged for manual review. Probability: 0.53\n",
      "Item 194 flagged for manual review. Probability: 0.53\n",
      "Item 3502 flagged for manual review. Probability: 0.53\n",
      "Item 2117 flagged for manual review. Probability: 0.53\n",
      "Item 2024 flagged for manual review. Probability: 0.53\n",
      "Item 1767 flagged for manual review. Probability: 0.53\n",
      "Item 2181 flagged for manual review. Probability: 0.53\n",
      "Item 3338 flagged for manual review. Probability: 0.53\n",
      "Item 1769 flagged for manual review. Probability: 0.53\n",
      "Item 3214 flagged for manual review. Probability: 0.53\n",
      "Item 3302 flagged for manual review. Probability: 0.53\n",
      "Item 1455 flagged for manual review. Probability: 0.53\n",
      "Item 2995 flagged for manual review. Probability: 0.53\n",
      "Item 2141 flagged for manual review. Probability: 0.53\n",
      "Item 2788 flagged for manual review. Probability: 0.53\n",
      "Item 3384 flagged for manual review. Probability: 0.53\n",
      "Item 2561 flagged for manual review. Probability: 0.53\n",
      "Item 1025 flagged for manual review. Probability: 0.53\n",
      "Item 1220 flagged for manual review. Probability: 0.53\n",
      "Item 2868 flagged for manual review. Probability: 0.53\n",
      "Item 1624 flagged for manual review. Probability: 0.54\n",
      "Item 2204 flagged for manual review. Probability: 0.54\n",
      "Item 701 flagged for manual review. Probability: 0.54\n",
      "Item 1655 flagged for manual review. Probability: 0.54\n",
      "Item 497 flagged for manual review. Probability: 0.54\n",
      "Item 1788 flagged for manual review. Probability: 0.54\n",
      "Item 1828 flagged for manual review. Probability: 0.54\n",
      "Item 1640 flagged for manual review. Probability: 0.54\n",
      "Item 2077 flagged for manual review. Probability: 0.54\n",
      "Item 1276 flagged for manual review. Probability: 0.54\n",
      "Item 1670 flagged for manual review. Probability: 0.54\n",
      "Item 2900 flagged for manual review. Probability: 0.54\n",
      "Item 1148 flagged for manual review. Probability: 0.54\n",
      "Item 990 flagged for manual review. Probability: 0.54\n",
      "Item 1046 flagged for manual review. Probability: 0.54\n",
      "Item 2435 flagged for manual review. Probability: 0.54\n",
      "Item 2488 flagged for manual review. Probability: 0.54\n",
      "Item 605 flagged for manual review. Probability: 0.54\n",
      "Item 1990 flagged for manual review. Probability: 0.54\n",
      "Item 2172 flagged for manual review. Probability: 0.54\n",
      "Item 3417 flagged for manual review. Probability: 0.55\n",
      "Item 1843 flagged for manual review. Probability: 0.55\n",
      "Item 900 flagged for manual review. Probability: 0.55\n",
      "Item 3389 flagged for manual review. Probability: 0.55\n",
      "Item 1608 flagged for manual review. Probability: 0.55\n",
      "Item 2822 flagged for manual review. Probability: 0.55\n",
      "Item 174 flagged for manual review. Probability: 0.55\n",
      "Item 1895 flagged for manual review. Probability: 0.55\n",
      "Item 2315 flagged for manual review. Probability: 0.55\n",
      "Item 1400 flagged for manual review. Probability: 0.55\n",
      "Item 2058 flagged for manual review. Probability: 0.55\n",
      "Item 3226 flagged for manual review. Probability: 0.55\n",
      "Item 2845 flagged for manual review. Probability: 0.55\n",
      "Item 3154 flagged for manual review. Probability: 0.55\n",
      "Item 2013 flagged for manual review. Probability: 0.55\n",
      "Item 3316 flagged for manual review. Probability: 0.55\n",
      "Item 2562 flagged for manual review. Probability: 0.55\n",
      "Item 1305 flagged for manual review. Probability: 0.55\n",
      "Item 2074 flagged for manual review. Probability: 0.55\n",
      "Item 2887 flagged for manual review. Probability: 0.55\n",
      "Item 123 flagged for manual review. Probability: 0.55\n",
      "Item 2081 flagged for manual review. Probability: 0.56\n",
      "Item 3158 flagged for manual review. Probability: 0.56\n",
      "Item 3320 flagged for manual review. Probability: 0.56\n",
      "Item 2856 flagged for manual review. Probability: 0.56\n",
      "Item 1385 flagged for manual review. Probability: 0.56\n",
      "Item 1368 flagged for manual review. Probability: 0.56\n",
      "Item 3089 flagged for manual review. Probability: 0.56\n",
      "Item 1673 flagged for manual review. Probability: 0.56\n",
      "Item 2088 flagged for manual review. Probability: 0.56\n",
      "Item 1304 flagged for manual review. Probability: 0.56\n",
      "Item 816 flagged for manual review. Probability: 0.56\n",
      "Item 719 flagged for manual review. Probability: 0.56\n",
      "Item 2018 flagged for manual review. Probability: 0.56\n",
      "Item 1702 flagged for manual review. Probability: 0.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item 205 flagged for manual review. Probability: 0.56\n",
      "Item 3006 flagged for manual review. Probability: 0.56\n",
      "Item 2306 flagged for manual review. Probability: 0.56\n",
      "Item 1372 flagged for manual review. Probability: 0.56\n",
      "Item 725 flagged for manual review. Probability: 0.56\n",
      "Item 2693 flagged for manual review. Probability: 0.56\n",
      "Item 1508 flagged for manual review. Probability: 0.56\n",
      "Item 3402 flagged for manual review. Probability: 0.56\n",
      "Item 475 flagged for manual review. Probability: 0.56\n",
      "Item 216 flagged for manual review. Probability: 0.56\n",
      "Item 1036 flagged for manual review. Probability: 0.57\n",
      "Item 2622 flagged for manual review. Probability: 0.57\n",
      "Item 2602 flagged for manual review. Probability: 0.57\n",
      "Item 2009 flagged for manual review. Probability: 0.57\n",
      "Item 2654 flagged for manual review. Probability: 0.57\n",
      "Item 273 flagged for manual review. Probability: 0.57\n",
      "Item 2299 flagged for manual review. Probability: 0.57\n",
      "Item 2313 flagged for manual review. Probability: 0.57\n",
      "Item 269 flagged for manual review. Probability: 0.57\n",
      "Item 1467 flagged for manual review. Probability: 0.57\n",
      "Item 3108 flagged for manual review. Probability: 0.57\n",
      "Item 886 flagged for manual review. Probability: 0.57\n",
      "Item 1113 flagged for manual review. Probability: 0.57\n",
      "Item 2075 flagged for manual review. Probability: 0.57\n",
      "Item 1338 flagged for manual review. Probability: 0.57\n",
      "Item 1653 flagged for manual review. Probability: 0.57\n",
      "Item 1989 flagged for manual review. Probability: 0.57\n",
      "Item 107 flagged for manual review. Probability: 0.57\n",
      "Item 1923 flagged for manual review. Probability: 0.57\n",
      "Item 3248 flagged for manual review. Probability: 0.57\n",
      "Item 1231 flagged for manual review. Probability: 0.57\n",
      "Item 1942 flagged for manual review. Probability: 0.57\n",
      "Item 2835 flagged for manual review. Probability: 0.57\n",
      "Item 2715 flagged for manual review. Probability: 0.57\n",
      "Item 106 flagged for manual review. Probability: 0.57\n",
      "Item 1883 flagged for manual review. Probability: 0.57\n",
      "Item 2407 flagged for manual review. Probability: 0.57\n",
      "Item 2825 flagged for manual review. Probability: 0.57\n",
      "Item 3351 flagged for manual review. Probability: 0.57\n",
      "Item 2769 flagged for manual review. Probability: 0.57\n",
      "Item 1035 flagged for manual review. Probability: 0.57\n",
      "Item 86 flagged for manual review. Probability: 0.57\n",
      "Item 2424 flagged for manual review. Probability: 0.58\n",
      "Item 3081 flagged for manual review. Probability: 0.58\n",
      "Item 1317 flagged for manual review. Probability: 0.58\n",
      "Item 2329 flagged for manual review. Probability: 0.58\n",
      "Item 2040 flagged for manual review. Probability: 0.58\n",
      "Item 461 flagged for manual review. Probability: 0.58\n",
      "Item 526 flagged for manual review. Probability: 0.58\n",
      "Item 241 flagged for manual review. Probability: 0.58\n",
      "Item 674 flagged for manual review. Probability: 0.58\n",
      "Item 2261 flagged for manual review. Probability: 0.58\n",
      "Item 1741 flagged for manual review. Probability: 0.58\n",
      "Item 994 flagged for manual review. Probability: 0.58\n",
      "Item 2703 flagged for manual review. Probability: 0.58\n",
      "Item 621 flagged for manual review. Probability: 0.58\n",
      "Item 136 flagged for manual review. Probability: 0.58\n",
      "Item 1216 flagged for manual review. Probability: 0.59\n",
      "Item 687 flagged for manual review. Probability: 0.59\n",
      "Item 3293 flagged for manual review. Probability: 0.59\n",
      "Item 230 flagged for manual review. Probability: 0.59\n",
      "Item 1080 flagged for manual review. Probability: 0.59\n",
      "Item 2235 flagged for manual review. Probability: 0.59\n",
      "Item 387 flagged for manual review. Probability: 0.59\n",
      "Item 2437 flagged for manual review. Probability: 0.59\n",
      "Item 1682 flagged for manual review. Probability: 0.59\n",
      "Item 293 flagged for manual review. Probability: 0.59\n",
      "Item 292 flagged for manual review. Probability: 0.59\n",
      "Item 1962 flagged for manual review. Probability: 0.59\n",
      "Item 3257 flagged for manual review. Probability: 0.59\n",
      "Item 1410 flagged for manual review. Probability: 0.59\n",
      "Item 170 flagged for manual review. Probability: 0.59\n",
      "Item 1793 flagged for manual review. Probability: 0.59\n",
      "Item 603 flagged for manual review. Probability: 0.59\n",
      "Item 1067 flagged for manual review. Probability: 0.59\n",
      "Item 1850 flagged for manual review. Probability: 0.59\n",
      "Item 1463 flagged for manual review. Probability: 0.59\n",
      "Item 1893 flagged for manual review. Probability: 0.59\n",
      "Item 1141 flagged for manual review. Probability: 0.59\n",
      "Item 1522 flagged for manual review. Probability: 0.59\n",
      "Item 3322 flagged for manual review. Probability: 0.59\n",
      "Item 1262 flagged for manual review. Probability: 0.59\n",
      "Item 195 flagged for manual review. Probability: 0.59\n",
      "Item 870 flagged for manual review. Probability: 0.59\n",
      "Item 1166 flagged for manual review. Probability: 0.59\n",
      "Item 965 flagged for manual review. Probability: 0.59\n",
      "Item 1352 flagged for manual review. Probability: 0.59\n",
      "Item 1540 flagged for manual review. Probability: 0.60\n",
      "Item 1096 flagged for manual review. Probability: 0.60\n",
      "Item 2916 flagged for manual review. Probability: 0.60\n",
      "Item 2340 flagged for manual review. Probability: 0.60\n",
      "Item 1277 flagged for manual review. Probability: 0.60\n",
      "Item 3099 flagged for manual review. Probability: 0.60\n",
      "Item 2595 flagged for manual review. Probability: 0.60\n",
      "Item 1644 flagged for manual review. Probability: 0.60\n",
      "Item 2837 flagged for manual review. Probability: 0.60\n",
      "Item 1150 flagged for manual review. Probability: 0.60\n",
      "Item 534 flagged for manual review. Probability: 0.60\n",
      "Item 1905 flagged for manual review. Probability: 0.60\n",
      "Item 2103 flagged for manual review. Probability: 0.60\n",
      "Item 223 flagged for manual review. Probability: 0.60\n",
      "Item 1494 flagged for manual review. Probability: 0.60\n",
      "Item 1283 flagged for manual review. Probability: 0.60\n",
      "Item 709 flagged for manual review. Probability: 0.60\n",
      "Item 3000 flagged for manual review. Probability: 0.60\n",
      "Item 1056 flagged for manual review. Probability: 0.60\n",
      "Item 2493 flagged for manual review. Probability: 0.60\n",
      "Item 3371 flagged for manual review. Probability: 0.60\n",
      "Item 1222 flagged for manual review. Probability: 0.60\n",
      "Item 1319 flagged for manual review. Probability: 0.60\n",
      "Item 1867 flagged for manual review. Probability: 0.60\n",
      "Item 2914 flagged for manual review. Probability: 0.60\n",
      "Item 347 flagged for manual review. Probability: 0.60\n",
      "Item 2179 flagged for manual review. Probability: 0.61\n",
      "Item 829 flagged for manual review. Probability: 0.61\n",
      "Item 1894 flagged for manual review. Probability: 0.61\n",
      "Item 1557 flagged for manual review. Probability: 0.61\n",
      "Item 3378 flagged for manual review. Probability: 0.61\n",
      "Item 1396 flagged for manual review. Probability: 0.61\n",
      "Item 1539 flagged for manual review. Probability: 0.61\n",
      "Item 2517 flagged for manual review. Probability: 0.61\n",
      "Item 2936 flagged for manual review. Probability: 0.61\n",
      "Item 1357 flagged for manual review. Probability: 0.61\n",
      "Item 1157 flagged for manual review. Probability: 0.61\n",
      "Item 2746 flagged for manual review. Probability: 0.61\n",
      "Item 3076 flagged for manual review. Probability: 0.61\n",
      "Item 1285 flagged for manual review. Probability: 0.61\n",
      "Item 1103 flagged for manual review. Probability: 0.61\n",
      "Item 1678 flagged for manual review. Probability: 0.61\n",
      "Item 1354 flagged for manual review. Probability: 0.61\n",
      "Item 1412 flagged for manual review. Probability: 0.61\n",
      "Item 2685 flagged for manual review. Probability: 0.61\n",
      "Item 14 flagged for manual review. Probability: 0.61\n",
      "Item 2544 flagged for manual review. Probability: 0.61\n",
      "Item 904 flagged for manual review. Probability: 0.61\n",
      "Item 166 flagged for manual review. Probability: 0.61\n",
      "Item 1451 flagged for manual review. Probability: 0.61\n",
      "Item 3250 flagged for manual review. Probability: 0.61\n",
      "Item 2096 flagged for manual review. Probability: 0.61\n",
      "Item 283 flagged for manual review. Probability: 0.61\n",
      "Item 318 flagged for manual review. Probability: 0.61\n",
      "Item 2462 flagged for manual review. Probability: 0.61\n",
      "Item 2551 flagged for manual review. Probability: 0.62\n",
      "Item 2333 flagged for manual review. Probability: 0.62\n",
      "Item 933 flagged for manual review. Probability: 0.62\n",
      "Item 2849 flagged for manual review. Probability: 0.62\n",
      "Item 2188 flagged for manual review. Probability: 0.62\n",
      "Item 1965 flagged for manual review. Probability: 0.62\n",
      "Item 2910 flagged for manual review. Probability: 0.62\n",
      "Item 3137 flagged for manual review. Probability: 0.62\n",
      "Item 88 flagged for manual review. Probability: 0.62\n",
      "Item 2420 flagged for manual review. Probability: 0.62\n",
      "Item 3407 flagged for manual review. Probability: 0.62\n",
      "Item 1906 flagged for manual review. Probability: 0.62\n",
      "Item 1687 flagged for manual review. Probability: 0.62\n",
      "Item 911 flagged for manual review. Probability: 0.62\n",
      "Item 3336 flagged for manual review. Probability: 0.62\n",
      "Item 2633 flagged for manual review. Probability: 0.62\n",
      "Item 2980 flagged for manual review. Probability: 0.62\n",
      "Item 2121 flagged for manual review. Probability: 0.62\n",
      "Item 2073 flagged for manual review. Probability: 0.62\n",
      "Item 2130 flagged for manual review. Probability: 0.62\n",
      "Item 1866 flagged for manual review. Probability: 0.62\n",
      "Item 1318 flagged for manual review. Probability: 0.62\n",
      "Item 1700 flagged for manual review. Probability: 0.62\n",
      "Item 1761 flagged for manual review. Probability: 0.62\n",
      "Item 1856 flagged for manual review. Probability: 0.62\n",
      "Item 932 flagged for manual review. Probability: 0.62\n",
      "Item 63 flagged for manual review. Probability: 0.62\n",
      "Item 1706 flagged for manual review. Probability: 0.62\n",
      "Item 2144 flagged for manual review. Probability: 0.62\n",
      "Item 1659 flagged for manual review. Probability: 0.62\n",
      "Item 2091 flagged for manual review. Probability: 0.62\n",
      "Item 3059 flagged for manual review. Probability: 0.62\n",
      "Item 3229 flagged for manual review. Probability: 0.62\n",
      "Item 3266 flagged for manual review. Probability: 0.63\n",
      "Item 255 flagged for manual review. Probability: 0.63\n",
      "Item 64 flagged for manual review. Probability: 0.63\n",
      "Item 731 flagged for manual review. Probability: 0.63\n",
      "Item 590 flagged for manual review. Probability: 0.63\n",
      "Item 2648 flagged for manual review. Probability: 0.63\n",
      "Item 1009 flagged for manual review. Probability: 0.63\n",
      "Item 1526 flagged for manual review. Probability: 0.63\n",
      "Item 151 flagged for manual review. Probability: 0.63\n",
      "Item 3298 flagged for manual review. Probability: 0.63\n",
      "Item 3511 flagged for manual review. Probability: 0.63\n",
      "Item 3497 flagged for manual review. Probability: 0.63\n",
      "Item 785 flagged for manual review. Probability: 0.63\n",
      "Item 519 flagged for manual review. Probability: 0.63\n",
      "Item 82 flagged for manual review. Probability: 0.63\n",
      "Item 389 flagged for manual review. Probability: 0.63\n",
      "Item 577 flagged for manual review. Probability: 0.63\n",
      "Item 406 flagged for manual review. Probability: 0.63\n",
      "Item 2047 flagged for manual review. Probability: 0.63\n",
      "Item 807 flagged for manual review. Probability: 0.63\n",
      "Item 1882 flagged for manual review. Probability: 0.63\n",
      "Item 2426 flagged for manual review. Probability: 0.63\n",
      "Item 2497 flagged for manual review. Probability: 0.63\n",
      "Item 28 flagged for manual review. Probability: 0.63\n",
      "Item 3457 flagged for manual review. Probability: 0.63\n",
      "Item 956 flagged for manual review. Probability: 0.63\n",
      "Item 68 flagged for manual review. Probability: 0.63\n",
      "Item 37 flagged for manual review. Probability: 0.63\n",
      "Item 1204 flagged for manual review. Probability: 0.63\n",
      "Item 1235 flagged for manual review. Probability: 0.63\n",
      "Item 2225 flagged for manual review. Probability: 0.63\n",
      "Item 2618 flagged for manual review. Probability: 0.64\n",
      "Item 467 flagged for manual review. Probability: 0.64\n",
      "Item 3403 flagged for manual review. Probability: 0.64\n",
      "Item 2529 flagged for manual review. Probability: 0.64\n",
      "Item 2793 flagged for manual review. Probability: 0.64\n",
      "Item 1275 flagged for manual review. Probability: 0.64\n",
      "Item 3026 flagged for manual review. Probability: 0.64\n",
      "Item 2533 flagged for manual review. Probability: 0.64\n",
      "Item 3423 flagged for manual review. Probability: 0.64\n",
      "Item 558 flagged for manual review. Probability: 0.64\n",
      "Item 3354 flagged for manual review. Probability: 0.64\n",
      "Item 428 flagged for manual review. Probability: 0.64\n",
      "Item 818 flagged for manual review. Probability: 0.64\n",
      "Item 157 flagged for manual review. Probability: 0.64\n",
      "Item 3146 flagged for manual review. Probability: 0.64\n",
      "Item 2754 flagged for manual review. Probability: 0.64\n",
      "Item 513 flagged for manual review. Probability: 0.64\n",
      "Item 2186 flagged for manual review. Probability: 0.64\n",
      "Item 998 flagged for manual review. Probability: 0.64\n",
      "Item 1316 flagged for manual review. Probability: 0.64\n",
      "Item 2041 flagged for manual review. Probability: 0.64\n",
      "Item 1161 flagged for manual review. Probability: 0.64\n",
      "Item 392 flagged for manual review. Probability: 0.64\n",
      "Item 3381 flagged for manual review. Probability: 0.64\n",
      "Item 1964 flagged for manual review. Probability: 0.64\n",
      "Item 2102 flagged for manual review. Probability: 0.64\n",
      "Item 3067 flagged for manual review. Probability: 0.64\n",
      "Item 190 flagged for manual review. Probability: 0.65\n",
      "Item 1829 flagged for manual review. Probability: 0.65\n",
      "Item 1649 flagged for manual review. Probability: 0.65\n",
      "Item 2573 flagged for manual review. Probability: 0.65\n",
      "Item 3047 flagged for manual review. Probability: 0.65\n",
      "Item 672 flagged for manual review. Probability: 0.65\n",
      "Item 404 flagged for manual review. Probability: 0.65\n",
      "Item 1202 flagged for manual review. Probability: 0.65\n",
      "Item 1747 flagged for manual review. Probability: 0.65\n",
      "Item 1450 flagged for manual review. Probability: 0.65\n",
      "Item 1307 flagged for manual review. Probability: 0.65\n",
      "Item 2441 flagged for manual review. Probability: 0.65\n",
      "Item 309 flagged for manual review. Probability: 0.65\n",
      "Item 690 flagged for manual review. Probability: 0.65\n",
      "Item 2542 flagged for manual review. Probability: 0.65\n",
      "Item 2215 flagged for manual review. Probability: 0.65\n",
      "Item 1992 flagged for manual review. Probability: 0.65\n",
      "Item 295 flagged for manual review. Probability: 0.65\n",
      "Item 3397 flagged for manual review. Probability: 0.65\n",
      "Item 2317 flagged for manual review. Probability: 0.65\n",
      "Item 1518 flagged for manual review. Probability: 0.65\n",
      "Item 3004 flagged for manual review. Probability: 0.65\n",
      "Item 450 flagged for manual review. Probability: 0.65\n",
      "Item 1084 flagged for manual review. Probability: 0.65\n",
      "Item 2026 flagged for manual review. Probability: 0.65\n",
      "Item 1057 flagged for manual review. Probability: 0.65\n",
      "Item 2105 flagged for manual review. Probability: 0.66\n",
      "Item 2460 flagged for manual review. Probability: 0.66\n",
      "Item 2937 flagged for manual review. Probability: 0.66\n",
      "Item 3419 flagged for manual review. Probability: 0.66\n",
      "Item 2147 flagged for manual review. Probability: 0.66\n",
      "Item 2345 flagged for manual review. Probability: 0.66\n",
      "Item 2804 flagged for manual review. Probability: 0.66\n",
      "Item 871 flagged for manual review. Probability: 0.66\n",
      "Item 640 flagged for manual review. Probability: 0.66\n",
      "Item 1657 flagged for manual review. Probability: 0.66\n",
      "Item 1995 flagged for manual review. Probability: 0.66\n",
      "Item 1108 flagged for manual review. Probability: 0.66\n",
      "Item 2260 flagged for manual review. Probability: 0.66\n",
      "Item 3294 flagged for manual review. Probability: 0.66\n",
      "Item 1629 flagged for manual review. Probability: 0.66\n",
      "Item 2292 flagged for manual review. Probability: 0.66\n",
      "Item 2128 flagged for manual review. Probability: 0.66\n",
      "Item 2765 flagged for manual review. Probability: 0.66\n",
      "Item 1598 flagged for manual review. Probability: 0.66\n",
      "Item 3149 flagged for manual review. Probability: 0.66\n",
      "Item 2281 flagged for manual review. Probability: 0.66\n",
      "Item 2082 flagged for manual review. Probability: 0.66\n",
      "Item 500 flagged for manual review. Probability: 0.66\n",
      "Item 3231 flagged for manual review. Probability: 0.66\n",
      "Item 1580 flagged for manual review. Probability: 0.66\n",
      "Item 470 flagged for manual review. Probability: 0.66\n",
      "Item 2957 flagged for manual review. Probability: 0.66\n",
      "Item 2094 flagged for manual review. Probability: 0.66\n",
      "Item 2847 flagged for manual review. Probability: 0.66\n",
      "Item 2597 flagged for manual review. Probability: 0.66\n",
      "Item 2192 flagged for manual review. Probability: 0.67\n",
      "Item 2412 flagged for manual review. Probability: 0.67\n",
      "Item 2421 flagged for manual review. Probability: 0.67\n",
      "Item 186 flagged for manual review. Probability: 0.67\n",
      "Item 1578 flagged for manual review. Probability: 0.67\n",
      "Item 1774 flagged for manual review. Probability: 0.67\n",
      "Item 372 flagged for manual review. Probability: 0.67\n",
      "Item 1156 flagged for manual review. Probability: 0.67\n",
      "Item 2503 flagged for manual review. Probability: 0.67\n",
      "Item 2886 flagged for manual review. Probability: 0.67\n",
      "Item 2524 flagged for manual review. Probability: 0.67\n",
      "Item 1210 flagged for manual review. Probability: 0.67\n",
      "Item 454 flagged for manual review. Probability: 0.67\n",
      "Item 1617 flagged for manual review. Probability: 0.67\n",
      "Item 2662 flagged for manual review. Probability: 0.67\n",
      "Item 1759 flagged for manual review. Probability: 0.67\n",
      "Item 2293 flagged for manual review. Probability: 0.67\n",
      "Item 2153 flagged for manual review. Probability: 0.67\n",
      "Item 2513 flagged for manual review. Probability: 0.67\n",
      "Item 582 flagged for manual review. Probability: 0.67\n",
      "Item 566 flagged for manual review. Probability: 0.67\n",
      "Item 3373 flagged for manual review. Probability: 0.67\n",
      "Item 2744 flagged for manual review. Probability: 0.67\n",
      "Item 1676 flagged for manual review. Probability: 0.67\n",
      "Item 1533 flagged for manual review. Probability: 0.67\n",
      "Item 947 flagged for manual review. Probability: 0.67\n",
      "Item 1115 flagged for manual review. Probability: 0.67\n",
      "Item 61 flagged for manual review. Probability: 0.67\n",
      "Item 3418 flagged for manual review. Probability: 0.67\n",
      "Item 2574 flagged for manual review. Probability: 0.67\n",
      "Item 2838 flagged for manual review. Probability: 0.67\n",
      "Item 2637 flagged for manual review. Probability: 0.67\n",
      "Item 1351 flagged for manual review. Probability: 0.67\n",
      "Item 87 flagged for manual review. Probability: 0.67\n",
      "Item 2922 flagged for manual review. Probability: 0.67\n",
      "Item 2625 flagged for manual review. Probability: 0.67\n",
      "Item 915 flagged for manual review. Probability: 0.67\n",
      "Item 574 flagged for manual review. Probability: 0.68\n",
      "Item 1754 flagged for manual review. Probability: 0.68\n",
      "Item 208 flagged for manual review. Probability: 0.68\n",
      "Item 1048 flagged for manual review. Probability: 0.68\n",
      "Item 845 flagged for manual review. Probability: 0.68\n",
      "Item 1912 flagged for manual review. Probability: 0.68\n",
      "Item 867 flagged for manual review. Probability: 0.68\n",
      "Item 1693 flagged for manual review. Probability: 0.68\n",
      "Item 2777 flagged for manual review. Probability: 0.68\n",
      "Item 2201 flagged for manual review. Probability: 0.68\n",
      "Item 1719 flagged for manual review. Probability: 0.68\n",
      "Item 767 flagged for manual review. Probability: 0.68\n",
      "Item 692 flagged for manual review. Probability: 0.68\n",
      "Item 2709 flagged for manual review. Probability: 0.68\n",
      "Item 99 flagged for manual review. Probability: 0.68\n",
      "Item 1614 flagged for manual review. Probability: 0.68\n",
      "Item 2809 flagged for manual review. Probability: 0.68\n",
      "Item 2870 flagged for manual review. Probability: 0.68\n",
      "Item 3084 flagged for manual review. Probability: 0.68\n",
      "Item 1563 flagged for manual review. Probability: 0.68\n",
      "Item 2514 flagged for manual review. Probability: 0.68\n",
      "Item 720 flagged for manual review. Probability: 0.68\n",
      "Item 196 flagged for manual review. Probability: 0.68\n",
      "Item 2108 flagged for manual review. Probability: 0.68\n",
      "Item 2781 flagged for manual review. Probability: 0.68\n",
      "Item 16 flagged for manual review. Probability: 0.68\n",
      "Item 3480 flagged for manual review. Probability: 0.68\n",
      "Item 3191 flagged for manual review. Probability: 0.69\n",
      "Item 2688 flagged for manual review. Probability: 0.69\n",
      "Item 954 flagged for manual review. Probability: 0.69\n",
      "Item 1688 flagged for manual review. Probability: 0.69\n",
      "Item 1248 flagged for manual review. Probability: 0.69\n",
      "Item 2930 flagged for manual review. Probability: 0.69\n",
      "Item 1289 flagged for manual review. Probability: 0.69\n",
      "Item 3501 flagged for manual review. Probability: 0.69\n",
      "Item 2585 flagged for manual review. Probability: 0.69\n",
      "Item 2504 flagged for manual review. Probability: 0.69\n",
      "Item 3041 flagged for manual review. Probability: 0.69\n",
      "Item 1541 flagged for manual review. Probability: 0.69\n",
      "Item 2945 flagged for manual review. Probability: 0.69\n",
      "Item 31 flagged for manual review. Probability: 0.69\n",
      "Item 182 flagged for manual review. Probability: 0.69\n",
      "Item 2749 flagged for manual review. Probability: 0.69\n",
      "Item 3199 flagged for manual review. Probability: 0.69\n",
      "Item 2089 flagged for manual review. Probability: 0.69\n",
      "Item 541 flagged for manual review. Probability: 0.69\n",
      "Item 1488 flagged for manual review. Probability: 0.69\n",
      "Item 2342 flagged for manual review. Probability: 0.69\n",
      "Item 2016 flagged for manual review. Probability: 0.69\n",
      "Item 1654 flagged for manual review. Probability: 0.69\n",
      "Item 656 flagged for manual review. Probability: 0.69\n",
      "Item 1736 flagged for manual review. Probability: 0.69\n",
      "Item 2189 flagged for manual review. Probability: 0.69\n",
      "Item 3030 flagged for manual review. Probability: 0.69\n",
      "Item 2219 flagged for manual review. Probability: 0.69\n",
      "Item 2646 flagged for manual review. Probability: 0.69\n",
      "Item 2748 flagged for manual review. Probability: 0.70\n",
      "Item 2371 flagged for manual review. Probability: 0.70\n",
      "Item 3481 flagged for manual review. Probability: 0.70\n",
      "Item 1601 flagged for manual review. Probability: 0.70\n",
      "Item 1211 flagged for manual review. Probability: 0.70\n",
      "Item 3448 flagged for manual review. Probability: 0.70\n",
      "Item 2948 flagged for manual review. Probability: 0.70\n",
      "Item 1052 flagged for manual review. Probability: 0.70\n",
      "Item 2183 flagged for manual review. Probability: 0.70\n",
      "Item 1582 flagged for manual review. Probability: 0.70\n",
      "Item 1315 flagged for manual review. Probability: 0.70\n",
      "Item 3241 flagged for manual review. Probability: 0.70\n",
      "Item 2989 flagged for manual review. Probability: 0.70\n",
      "Item 1875 flagged for manual review. Probability: 0.70\n",
      "Item 112 flagged for manual review. Probability: 0.70\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.20, random_state=42, stratify=y)\n",
    "\n",
    "# Initialize and train the CatBoost classifier\n",
    "model = CatBoostClassifier(\n",
    "    iterations=500, \n",
    "    learning_rate=0.1, \n",
    "    depth=6,\n",
    "    l2_leaf_reg=3,\n",
    "    loss_function='MultiClass',  # Specifying the loss function for multi-class classification\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Define a confidence threshold for manual review\n",
    "confidence_threshold = 0.7\n",
    "\n",
    "# Predict class probabilities and flag items for review\n",
    "probabilities = model.predict_proba(X_test)\n",
    "max_probs = np.max(probabilities, axis=1)\n",
    "flags = max_probs < confidence_threshold\n",
    "\n",
    "# Create a list to store flagged items and their probabilities\n",
    "flagged_items = []\n",
    "\n",
    "for i, (flag, prob) in enumerate(zip(flags, max_probs)):\n",
    "    if flag:\n",
    "        flagged_items.append((i, prob))\n",
    "\n",
    "# Sort flagged items by probability (from lowest to highest)\n",
    "flagged_items.sort(key=lambda x: x[1])\n",
    "\n",
    "# Output the flagged items\n",
    "print(f\"\\nTotal flagged items out of 3521 items: {len(flagged_items)}\")\n",
    "\n",
    "print(\"Flagged items for manual review:\")\n",
    "for item in flagged_items:\n",
    "    print(f\"Item {item[0]} flagged for manual review. Probability: {item[1]:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baab236",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "<p>While achieving a high accuracy with CatBoost is positive, exploring other techniques like Bagging Classifier is a good practice in machine learning to ensure that we have considered various approaches. \n",
    "    \n",
    "A bagging classifier combines multiple classifiers to make better predictions. It creates subsets of the training data, trains individual classifiers on each subset, and then combines their predictions for a more accurate final result. We will construct a Bagging Classifier model employing DecisionTreeClassifier as the base estimator. It performs hyperparameter tuning using a randomized search strategy (RandomizedSearchCV). The coarse_search_bagging variable represents the hyperparameter tuning process.</p>\n",
    "\n",
    "### Coarse search\n",
    "<p>The <code>coarse_param_grid_bagging</code> defines a grid of hyperparameters that will be explored during the hyperparameter tuning process for the Bagging Classifier:</p>\n",
    "<ul>\n",
    "    <li><code>'n_estimators'</code>: This hyperparameter determines the number of base estimators (Decision Trees in this case) that will be used in the Bagging Classifier. It specifies different values to test, including 10, 50, and 100 base estimators.</li>\n",
    "    <li><code>'max_samples'</code>: This hyperparameter controls the maximum number of samples to draw from the training data when constructing each subset for training individual base estimators. It can take values of 0.5, 0.7, and 1.0, representing the fraction of the training data to be used.</li>\n",
    "    <li><code>'max_features'</code>: Similar to <code>'max_samples'</code>, this hyperparameter controls the maximum number of features (attributes) to consider when fitting each individual base estimator. Again, it can take values of 0.5, 0.7, and 1.0, representing the fraction of features to be considered.</li>\n",
    "    <li><code>'base_estimator__max_depth'</code>: This hyperparameter is specific to the base estimator used within the Bagging Classifier, which is a DecisionTreeClassifier in this case. It controls the maximum depth of the decision trees. The values specified are 5, 10, and None, where None means that the decision trees are allowed to expand until they reach their purest form (potentially leading to overfitting).</li>\n",
    "</ul>\n",
    "<p>These hyperparameters collectively define the configuration space that will be explored to find the optimal settings for the Bagging Classifier. Note that, we used Truncated SVD for dimension reduction.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9254de6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sheik\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters from Bagging Coarse Search: {'n_estimators': 50, 'max_samples': 1.0, 'max_features': 0.7, 'base_estimator__max_depth': None}\n",
      "Total time taken: 937.125803232193 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.20, random_state=42,stratify=y)\n",
    "\n",
    "# Coarse Hyperparameter Grid\n",
    "coarse_param_grid_bagging = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'max_samples': [0.5, 0.7, 1.0],\n",
    "    'max_features': [0.5, 0.7, 1.0],\n",
    "    'base_estimator__max_depth': [5, 10, None]\n",
    "}\n",
    "\n",
    "# Initialize Bagging Classifier with a base estimator\n",
    "base_estimator = DecisionTreeClassifier()\n",
    "bagging_clf = BaggingClassifier(base_estimator=base_estimator)\n",
    "\n",
    "# Randomized Search\n",
    "coarse_search_bagging = RandomizedSearchCV(\n",
    "    bagging_clf, coarse_param_grid_bagging, n_iter=10,\n",
    "    scoring='accuracy', cv=3, n_jobs=-1, random_state=42, verbose=1\n",
    ")\n",
    "coarse_search_bagging.fit(X_train, y_train)\n",
    "\n",
    "# Best Parameters from Coarse Search\n",
    "print(\"Best Parameters from Bagging Coarse Search:\", coarse_search_bagging.best_params_)\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "print(f\"Total time taken: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ec3caf",
   "metadata": {},
   "source": [
    "#### We proceeded by utilizing the best parameters obtained from the Bagging Coarse Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "769433fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.78      0.92      0.84       144\n",
      "           2       0.84      0.89      0.86        72\n",
      "           3       0.76      0.82      0.79       144\n",
      "           4       0.79      0.62      0.70       128\n",
      "           5       0.86      0.82      0.84       144\n",
      "           6       0.81      0.66      0.73       144\n",
      "           7       0.83      0.74      0.78        72\n",
      "           8       0.87      0.92      0.89       144\n",
      "           9       0.77      0.88      0.82       144\n",
      "\n",
      "    accuracy                           0.81      1136\n",
      "   macro avg       0.81      0.81      0.81      1136\n",
      "weighted avg       0.81      0.81      0.81      1136\n",
      "\n",
      "Total time taken: 250.9223346710205 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize the Decision Tree Classifier for bagging\n",
    "estimator = DecisionTreeClassifier(max_depth=None)  # None indicates no maximum depth\n",
    "\n",
    "# Initialize the Bagging Classifier with the best parameters and updated parameter name\n",
    "bagging_best = BaggingClassifier(\n",
    "    estimator=estimator,\n",
    "    n_estimators=50,                # Best number of estimators\n",
    "    max_samples=1.0,                # Best max_samples\n",
    "    max_features=0.7,               # Best max_features\n",
    "    random_state=42                 # For reproducibility\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "bagging_best.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = bagging_best.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "print(f\"Total time taken: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece1e2b4",
   "metadata": {},
   "source": [
    "<P><strong>Observations:</strong>The Bagging Classifier yielded an accuracy of 81%, which falls below our expectations. As a result, we have chosen to discontinue the use of this classifier.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558f6749",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716365d0",
   "metadata": {},
   "source": [
    "#### PCA with  95% (~1770 features from 10,000)\n",
    "| Model Description         | Average Precision | Average Recall | Average F1-Score | Time taken to run |\n",
    "|---------------------------|-------------------|----------------|-------------------|--------------------|\n",
    "| LogisticRegression        | 0.60              | 0.60           | 0.60              | 40 seconds         |\n",
    "| Randomized Search SVM     | 0.70              | 0.65           | 0.66              | 2208 seconds       |\n",
    "| <span style=\"background-color: orange;\">Grid Search SVM           | <span style=\"background-color: orange;\">0.68              | 0.66           | 0.67              | <span style=\"background-color: orange;\">11402 seconds      |\n",
    "| Refined Grid Search SVM   | 0.68              | 0.66           | 0.66              | 859 seconds        |\n",
    "\n",
    "#### PCA with 80% (~700 features from 10,000)\n",
    "| Model Description         | Average Precision | Average Recall | Average F1-Score | Time to run |\n",
    "|---------------------------|-------------------|----------------|-------------------|--------------|\n",
    "| Randomized Search SVM     | 0.79              | 0.79           | 0.79              | 571 seconds   |\n",
    "| Grid Search SVM           | 0.81              | 0.79           | 0.79              | 2943 seconds  |\n",
    "| Refined Grid Search SVM   | 0.81              | 0.79           | 0.79              | 271 seconds   |\n",
    "| <span style=\"background-color: lightblue;\">CatBoost with PCA 80%     | <span style=\"background-color: lightblue;\">0.93              | 0.92           | <span style=\"background-color: lightblue;\">0.92              | <span style=\"background-color: lightblue;\">784 seconds   |\n",
    "| XGBoost with PCA 80%      | 0.86              | 0.86           | 0.86              | 189 seconds   |\n",
    "\n",
    "#### Truncated SVD (400 features from 10,000)\n",
    "| Model Description          | Average Precision | Average Recall | Average F1-Score | Time taken to run |\n",
    "|----------------------------|-------------------|----------------|-------------------|--------------------|\n",
    "| XGBoost with Truncated SVD | 0.89              | 0.89           | 0.89              | 437 seconds         |\n",
    "| <span style=\"background-color: lightblue;\">CatBoost with Truncated SVD_v1 | <span style=\"background-color: lightblue;\">0.93           | 0.93           | <span style=\"background-color: lightblue;\">0.93              | <span style=\"background-color: lightblue;\">787 seconds         |\n",
    "| <span style=\"background-color: yellow;\">CatBoost with Truncated SVD_v2    | <span style=\"background-color: yellow;\">0.90              | 0.90           | 0.90              | <span style=\"background-color: yellow;\">300 seconds         |</span>        |\n",
    "| CatBoost with Truncated SVD_v3 | 0.76           | 0.76           | 0.76              | 120 seconds         |\n",
    "| Bagging Classifier         | 0.81              | 0.81           | 0.81              | 250 seconds         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6335d7",
   "metadata": {},
   "source": [
    "<p>In the quest for the most effective model for our signature verification system, we closely analyzed several contenders. While <strong>CatBoost with Truncated SVD_v1</strong> achieved the highest precision of 93%, further inspection of the learning curves hinted at potential overfitting, which could hinder its practical application.</p>\n",
    "<p>After an extensive review of model performances and their corresponding learning curves, the model that emerged as the most promising was <strong>CatBoost with Truncated SVD_v2</strong>. This particular model, with a robust learning rate of 0.1 and an iteration count of 500, managed to secure a notable <strong>precision score of 90%</strong>.</p>\n",
    "<p>Equally important to its precision was the model's efficiency in training; it took a feasible 5-7 minutes to train on a dataset size of 17,000. This blend of exceptional accuracy, a demonstrable avoidance of overfitting, and a practical training timeframe solidifies <strong>CatBoost with Truncated SVD_v2</strong> as the superior choice for our signature verification task.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c6c888",
   "metadata": {},
   "source": [
    "## Further Improvement:\n",
    "\n",
    "We have made good progress in configuring our CatBoost model, but there are still several avenues to explore for further improvement. From the learning curve, we can see a noticeable gap between training (98%) and testing accuracy (90%) at 17,000 data points. Here are some recommendations:\n",
    "\n",
    "- **Data Augmentation:** We can apply additional data augmentation techniques to generate more training samples. This can help the model generalize better.\n",
    "\n",
    "- **Fine-Tune Learning Rate:** The learning rate controls the step size during training. Experiment with different learning rates to find the one that works best for our dataset. We can try values like 0.01, 0.05, 0.1, and 0.2.\n",
    "\n",
    "- **Depth of Trees:** The depth of the trees in our ensemble can impact model complexity. Deeper trees can capture more complex patterns but may lead to overfitting. Experiment with different depths to find the right balance.\n",
    "\n",
    "- **Feature Engineering:** Analyze our features and consider creating new features or engineering existing ones. Feature engineering can significantly impact model performance. Domain-specific features can be particularly valuable.\n",
    "\n",
    "- **Cross-Validation:** Implement k-fold cross-validation to assess our model's performance more robustly. This can help us detect overfitting and provide a better estimate of generalization performance.\n",
    "\n",
    "- **Ensemble Methods:** Consider building an ensemble of multiple CatBoost models with different hyperparameters. We can combine their predictions to improve overall performance. We can also combine CatBoost and XGBoost into an ensemble.\n",
    "\n",
    "- **Grid Search or Random Search:** Use grid search or random search techniques to systematically explore hyperparameter combinations. This can be computationally expensive but may lead to better hyperparameter choices.\n",
    "\n",
    "- **Regularization Techniques:** Experiment with other regularization techniques such as dropout or early stopping with different criteria to prevent overfitting.\n",
    "\n",
    "- **Evaluate Feature Importance:** Use CatBoost's feature importance functionality to identify the most informative features. We can then focus on these features or consider eliminating less important ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135306eb",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dad1c3d",
   "metadata": {},
   "source": [
    "<p>Our ultimate model is the CatBoost with Truancated SVD_v2, which has been configured with the following parameters:</p>\n",
    "<ul>\n",
    "  <li>Iterations: 500 iterations</li>\n",
    "  <li>Learning Rate: 0.1</li>\n",
    "  <li>Depth: 6</li>\n",
    "  <li>L2 Leaf Regularization: 3</li>\n",
    "  <li>Loss Function: Specified as 'MultiClass' to handle multi-class classification</li>\n",
    "  <li>Early Stopping Rounds: 50</li>\n",
    "</ul>\n",
    "<p>These settings define the key characteristics of our final CatBoostClassifier model.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977ae3bc",
   "metadata": {},
   "source": [
    "### Train Ful model with entire Dataset: catboost_full_model\n",
    "We will now train the full model using the entire dataset and save it as a .pckl file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "16e7f098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "import pickle\n",
    "\n",
    "# Initialize CatBoost classifier\n",
    "catboost_full_model = CatBoostClassifier(\n",
    "        iterations=500, \n",
    "        learning_rate=0.1, \n",
    "        depth=6,\n",
    "        l2_leaf_reg=3,\n",
    "        loss_function='MultiClass',  # Specifying the loss function for multi-class classification\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model on the full dataset\n",
    "catboost_full_model.fit(X_pca, y)\n",
    "\n",
    "# Save the fully trained CatBoost model to a file\n",
    "model_filename = 'catboost_full_model.pkl'\n",
    "with open(model_filename, 'wb') as file:\n",
    "    pickle.dump(catboost_full_model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0287feb7",
   "metadata": {},
   "source": [
    "### Final Deployment: Prepare for Testing\n",
    "To test the model on real unseen data, we need to implement all the necessary functions to read the images, preprocess them into the desired format, and then utilize the model for making predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74ae68ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image_1.jpg: Predicted Class - [6]\n",
      "Image_10.jpg: Predicted Class - [6]\n",
      "Image_2.jpg: Predicted Class - [3]\n",
      "Image_3.jpg: Predicted Class - [6]\n",
      "Image_4.jpg: Predicted Class - [3]\n",
      "Image_5.jpg: Predicted Class - [3]\n",
      "Image_6.jpg: Predicted Class - [5]\n",
      "Image_7.jpg: Predicted Class - [5]\n",
      "Image_8.jpg: Predicted Class - [9]\n",
      "Image_9.jpg: Predicted Class - [6]\n",
      "Image processing and prediction completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFilter\n",
    "import pickle\n",
    "\n",
    "# Function to load, sharpen, and process an image\n",
    "def load_and_process_image(image_path):\n",
    "    # Load the image\n",
    "    img = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "\n",
    "    # Apply the filter for sharpening\n",
    "    img = img.filter(ImageFilter.CONTOUR)\n",
    "\n",
    "\n",
    "    # Check if image is already 100x100\n",
    "    if img.size != (100, 100):\n",
    "        img = img.resize((100, 100))  # Resize to 100x100 only if necessary\n",
    "    \n",
    "    img_array = np.array(img).flatten()  # Flatten the image\n",
    "    return img_array\n",
    "\n",
    "# Function to make a prediction\n",
    "def predict_signature(image_path, model, scaler, pca):\n",
    "    # Process the image\n",
    "    img_array = load_and_process_image(image_path)\n",
    "    \n",
    "    # Standardize and apply PCA transformation\n",
    "    img_array_scaled = scaler.transform([img_array])\n",
    "    img_array_pca = pca.transform(img_array_scaled)\n",
    "\n",
    "    # Make a prediction\n",
    "    prediction = model.predict(img_array_pca)\n",
    "    return prediction[0] # Add 1 to adjust the class label\n",
    "\n",
    "# Load the trained model, PCA, and scaler\n",
    "with open('catboost_full_model.pkl', 'rb') as file: #catboost_full_model\n",
    "    model = pickle.load(file)\n",
    "with open('scaler.pkl', 'rb') as file:\n",
    "    scaler = pickle.load(file)\n",
    "with open('tsvd.pkl', 'rb') as file:\n",
    "    pca = pickle.load(file)\n",
    "\n",
    "# Directory containing the test images\n",
    "test_image_directory = './Test/'\n",
    "\n",
    "# Predict on new images in the specified directory\n",
    "for filename in os.listdir(test_image_directory):\n",
    "    if filename.lower().endswith(('.jpg', '.jpeg')):\n",
    "        image_path = os.path.join(test_image_directory, filename)\n",
    "        prediction = predict_signature(image_path, model, scaler, pca)\n",
    "        print(f\"{filename}: Predicted Class - {prediction}\")\n",
    "\n",
    "print(\"Image processing and prediction completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083fe9d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
